{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train as t\n",
    "import tensorflow as tf\n",
    "\n",
    "import importlib\n",
    "#importlib.reload(t)\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ======================= Loading Data =======================\n",
      "dumped\n",
      "dumped\n",
      "\n",
      " ======================= Data Loaded =======================\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary\n",
    "token_vocabulary = [\"**end**\", \"**start**\", \"**unknown**\"]\n",
    "\n",
    "token_vocabulary.extend(t.get_vocabulary(\"train\"))\n",
    "\n",
    "target_tokens = token_vocabulary  # TODO: Refactor this. Currently duplicate naming\n",
    "\n",
    "token_vocab_size = len(target_tokens)\n",
    "# todo: document what was lifted from tutorials and what we wrote ourselves\n",
    "target_token_index = dict(\n",
    "    [(token, i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "reverse_target_token_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ======================= Loading Data =======================\")\n",
    "#new cell\n",
    "\n",
    "train_dataset = t.get_data_somehow('train', True, t.hparams['mini_batch_size'], t.hparams['max_token_length'], t.hparams['max_train_num_samples'] , target_token_index)\n",
    "val_dataset = t.get_data_somehow('val', True, t.hparams['mini_batch_size'], t.hparams['max_token_length'], t.hparams['max_val_num_samples'], target_token_index)\n",
    "\n",
    "train_encoder_input_data_batches = train_dataset[0]\n",
    "train_target_texts_batches = train_dataset[1]\n",
    "train_sequence_lengths_batches = train_dataset[2]\n",
    "train_decoder_input_data_batches = train_dataset[3]\n",
    "train_decoder_target_data_batches = train_dataset[4]\n",
    "\n",
    "val_encoder_input_data_batches = val_dataset[0]\n",
    "val_target_texts_batches = val_dataset[1]\n",
    "val_sequence_lengths_batches = val_dataset[2]\n",
    "val_decoder_input_data_batches = val_dataset[3]\n",
    "val_decoder_target_data_batches = val_dataset[4]\n",
    "print(\"\\n ======================= Data Loaded =======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv2d/kernel:0/gradient is illegal; using conv2d/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0/gradient is illegal; using conv2d/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/kernel:0/gradient is illegal; using conv2d_1/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/bias:0/gradient is illegal; using conv2d_1/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0/gradient is illegal; using conv2d_2/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0/gradient is illegal; using conv2d_2/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/kernel:0/gradient is illegal; using conv2d_3/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/bias:0/gradient is illegal; using conv2d_3/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/kernel:0/gradient is illegal; using conv2d_4/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/bias:0/gradient is illegal; using conv2d_4/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/kernel:0/gradient is illegal; using last_conv_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/bias:0/gradient is illegal; using last_conv_layer/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name memory_layer/kernel:0/gradient is illegal; using memory_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name embedding_encoder:0/gradient is illegal; using embedding_encoder_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/kernel:0/gradient is illegal; using decoder/attention_wrapper/basic_lstm_cell/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/bias:0/gradient is illegal; using decoder/attention_wrapper/basic_lstm_cell/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/luong_attention/attention_g:0/gradient is illegal; using decoder/attention_wrapper/luong_attention/attention_g_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/attention_layer/kernel:0/gradient is illegal; using decoder/attention_wrapper/attention_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/output_projection/kernel:0/gradient is illegal; using decoder/output_projection/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d/kernel:0/weight is illegal; using conv2d/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0/weight is illegal; using conv2d/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/kernel:0/weight is illegal; using conv2d_1/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/bias:0/weight is illegal; using conv2d_1/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0/weight is illegal; using conv2d_2/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0/weight is illegal; using conv2d_2/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/kernel:0/weight is illegal; using conv2d_3/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/bias:0/weight is illegal; using conv2d_3/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/kernel:0/weight is illegal; using conv2d_4/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/bias:0/weight is illegal; using conv2d_4/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/kernel:0/weight is illegal; using last_conv_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/bias:0/weight is illegal; using last_conv_layer/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name memory_layer/kernel:0/weight is illegal; using memory_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name embedding_encoder:0/weight is illegal; using embedding_encoder_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/kernel:0/weight is illegal; using decoder/attention_wrapper/basic_lstm_cell/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/bias:0/weight is illegal; using decoder/attention_wrapper/basic_lstm_cell/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/luong_attention/attention_g:0/weight is illegal; using decoder/attention_wrapper/luong_attention/attention_g_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/attention_layer/kernel:0/weight is illegal; using decoder/attention_wrapper/attention_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/output_projection/kernel:0/weight is illegal; using decoder/output_projection/kernel_0/weight instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "embedding_decoder, decoder_cell, decoder_initial_state, projection_layer, img \\\n",
    "= t.create_graph(token_vocab_size, t.hparams['num_units'], t.hparams['use_attention'], t.hparams['use_encoding_average_as_initial_state'], False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinitializing\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "t.initialize_variables(sess, restore=True, path=\"../output/checkpoints\" + '/model_127.ckpt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[364,   5,   5,   5,   5,   5, 382, 382, 382, 382, 382, 382, 382,\n",
       "        382, 382, 382, 382, 382, 382, 382, 382, 382, 382, 382, 382, 382,\n",
       "        382, 382, 382, 382, 382, 382, 382, 382, 382, 382, 382, 382, 382,\n",
       "        382, 382, 382, 382, 382, 382, 382, 382, 382, 382, 382],\n",
       "       [364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364],\n",
       "       [364, 364, 364,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "          5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "          5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "          5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5],\n",
       "       [364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364],\n",
       "       [364,   5,   5, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364],\n",
       "       [364,   5,   5,   5,   5, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364],\n",
       "       [364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364],\n",
       "       [364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364],\n",
       "       [364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364],\n",
       "       [364,   5, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364,\n",
       "        364, 364, 364, 364, 364, 364, 364, 364, 364, 364, 364]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = train_encoder_input_data_batches[0]\n",
    "\n",
    "little_variable, logits = t.predict_batch(sess,\n",
    "                  batch,\n",
    "                  target_token_index,\n",
    "                  embedding_decoder,\n",
    "                  decoder_cell,\n",
    "                  decoder_initial_state,\n",
    "                  projection_layer,\n",
    "                  img)\n",
    "\n",
    "little_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "^ & & & & & n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n \n",
      "\n",
      "Ground truth: \n",
      "**start** \\, ^ { * } d \\, ^ { * } H = \\kappa \\, ^ { * } d \\phi = J _ { B } .\n",
      "\n",
      "Output: \n",
      "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      "\n",
      "Ground truth: \n",
      "**start** \\partial _ { \\mu } ( F ^ { \\mu \\nu } - e j ^ { \\mu } x ^ { \\nu } ) = 0 .\n",
      "\n",
      "Output: \n",
      "^ ^ ^ & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \n",
      "\n",
      "Ground truth: \n",
      "**start** L _ { 0 } = \\Phi ( w ) = \\bigtriangleup \\Phi ( w ) ,\n",
      "\n",
      "Output: \n",
      "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      "\n",
      "Ground truth: \n",
      "**start** ( { \\cal L } _ { a } g ) _ { i j } = 0 , \\ \\ \\ \\ ( { \\cal L } _ { a } H ) _ { i j k } = 0 ,\n",
      "\n",
      "Output: \n",
      "^ & & ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      "\n",
      "Ground truth: \n",
      "**start** S _ { s t a t } = 2 \\pi \\sqrt { N _ { 5 } ^ { ( 1 ) } N _ { 5 } ^ { ( 2 ) } N _ { 5 } ^ { ( 3 ) } } \\left( \\sqrt { n } + \\sqrt { \\bar { n } } \\right)\n",
      "\n",
      "Output: \n",
      "^ & & & & ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      "\n",
      "Ground truth: \n",
      "**start** g _ { i j } ( x ) = { \\frac { 1 } { a ^ { 2 } } } \\, \\delta _ { i j } , ~ ~ \\phi ^ { a } ( x ) = \\phi ^ { a } , \\quad ( a , \\phi ^ { a } \\! : ~ \\mathrm { c o n s t . } )\n",
      "\n",
      "Output: \n",
      "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      "\n",
      "Ground truth: \n",
      "**start** \\widetilde \\gamma _ { \\mathrm { h o p f } } \\simeq \\sum _ { n > 0 } \\widetilde { G } _ { n } { \\frac { ( - a ) ^ { n } } { 2 ^ { 2 n - 1 } } }\n",
      "\n",
      "Output: \n",
      "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      "\n",
      "Ground truth: \n",
      "**start** \\rho _ { L } ( q ) = \\sum _ { m = 1 } ^ { L } \\ P _ { L } ( m ) \\ { \\frac { 1 } { q ^ { m - 1 } } } \\ \\ .\n",
      "\n",
      "Output: \n",
      "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      "\n",
      "Ground truth: \n",
      "**start** e x p \\left( - \\frac { \\partial } { \\partial \\alpha _ { j } } \\theta ^ { j k } \\frac { \\partial } { \\partial \\alpha _ { k } } \\right)\n",
      "\n",
      "Output: \n",
      "^ & ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
      "\n",
      "Ground truth: \n",
      "**start** \\hat { N } _ { 3 } = \\sum \\sp f _ { j = 1 } a _ { j } \\sp { \\dagger } a _ { j } \\, .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_token_seq(int_sequence):\n",
    "    \n",
    "    output_string = \"\"\n",
    "    for value in int_sequence:\n",
    "        output_string += reverse_target_token_index[value] + \" \"\n",
    "        if reverse_target_token_index[value] == \"**end**\":\n",
    "            break\n",
    "    return output_string\n",
    "\n",
    "for idx, seq in enumerate(train_target_texts_batches[0]):\n",
    "    \n",
    "    print(\"Output: \")\n",
    "    print(get_token_seq(little_variable[idx]))\n",
    "    print(\"\")\n",
    "    print(\"Ground truth: \")\n",
    "    print(train_target_texts_batches[0][idx])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Enum):\n",
    "    ENCODER_INPUTS = 0\n",
    "    TARGET_TEXTS = 1\n",
    "    LENGTHS = 2\n",
    "    DECODER_INPUTS = 3\n",
    "    DECODER_TARGETS = 4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess=sess, save_path=\"../output/checkpoints/inner.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = ????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not done\n",
    "def predict(set_, loss_tensor, logits_tensor, sess, labels=None):\n",
    "    n_batches = len(set_[DS.LENGTHS])\n",
    "    \n",
    "    #returns the logits for a batch\n",
    "    def predict_batch_tensor(set_, idx):\n",
    "        input_ = {img: set_[Batch.ENCODER_INPUTS][idx],\n",
    "                  decoder_lengths: set_[DS.TARGET_TEXT],\n",
    "                  decoder_inputs: set_[DS.DECODER_INPUT],\n",
    "                  decoder_outputs: set_[DS.DECODER_TARGET]\n",
    "                 }\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = tf.equal(tf.argmax(logits_tensor, axis=1), tf.argmax(labels, axis=1))\n",
    "        \n",
    "    \n",
    "    total = decoder_lengths.shape[0]\n",
    "\n",
    "    loss, correct, total = sess.run([loss_tensor, accuracy_tensor, total], feed_dict=input_)\n",
    "    return loss, correct, total\n",
    "\n",
    "    def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_loss():\n",
    "    #num_val_batches = len(val_sequence_lengths_batches)\n",
    "    val_loss = 0    \n",
    "    for i in range(num_val_batches):\n",
    "        input_data = {img: val_encoder_input_data_batches[i],\n",
    "                                        decoder_lengths: val_sequence_lengths_batches[i],\n",
    "                                         decoder_inputs: val_decoder_input_data_batches[i],\n",
    "                                          decoder_outputs: val_decoder_target_data_batches[i],\n",
    "                                         }\n",
    "   \n",
    "        output_tensors = [train_loss]\n",
    "        loss = sess.run(output_tensors, \n",
    "                               feed_dict=input_data)\n",
    "        \n",
    "        print(loss)\n",
    "        val_loss = val_loss + loss[0]\n",
    "        \n",
    "    val_loss = val_loss / len(val_decoder_input_data_batches[i])\n",
    "    return val_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
