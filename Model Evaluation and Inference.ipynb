{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train as t\n",
    "import importlib\n",
    "importlib.reload(t)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI MOM!  ../data/\n",
      "\n",
      " ======================= Loading Data =======================\n",
      "fresh data coming up\n",
      "dumped: train\n",
      "fresh data coming up\n",
      "dumped: val\n",
      "\n",
      " ======================= Data Loaded =======================\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary\n",
    "target_tokens = [\"**end**\", \"**start**\", \"**unknown**\"]\n",
    "\n",
    "target_tokens.extend(t.get_vocabulary(\"train\"))\n",
    "\n",
    "token_vocab_size = len(target_tokens)\n",
    "# todo: document what was lifted from tutorials and what we wrote ourselves\n",
    "target_token_index = dict(\n",
    "    [(token, i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "reverse_target_token_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ======================= Loading Data =======================\")\n",
    "#new cell\n",
    "\n",
    "train_dataset = t.get_data_somehow('train', True, t.hparams['mini_batch_size'], t.hparams['max_token_length'], t.hparams['max_train_num_samples'] , target_token_index)\n",
    "val_dataset = t.get_data_somehow('val', True, t.hparams['mini_batch_size'], t.hparams['max_token_length'], t.hparams['max_val_num_samples'], target_token_index)\n",
    "\n",
    "train_encoder_input_data_batches = train_dataset[0]\n",
    "train_target_texts_batches = train_dataset[1]\n",
    "train_sequence_lengths_batches = train_dataset[2]\n",
    "train_decoder_input_data_batches = train_dataset[3]\n",
    "train_decoder_target_data_batches = train_dataset[4]\n",
    "\n",
    "val_encoder_input_data_batches = val_dataset[0]\n",
    "val_target_texts_batches = val_dataset[1]\n",
    "val_sequence_lengths_batches = val_dataset[2]\n",
    "val_decoder_input_data_batches = val_dataset[3]\n",
    "val_decoder_target_data_batches = val_dataset[4]\n",
    "print(\"\\n ======================= Data Loaded =======================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv1/kernel:0/gradient is illegal; using conv1/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv1/bias:0/gradient is illegal; using conv1/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2/kernel:0/gradient is illegal; using conv2/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2/bias:0/gradient is illegal; using conv2/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv3/kernel:0/gradient is illegal; using conv3/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv3/bias:0/gradient is illegal; using conv3/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv4/kernel:0/gradient is illegal; using conv4/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv4/bias:0/gradient is illegal; using conv4/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv5/kernel:0/gradient is illegal; using conv5/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv5/bias:0/gradient is illegal; using conv5/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d/kernel:0/gradient is illegal; using conv2d/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0/gradient is illegal; using conv2d/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name memory_layer/kernel:0/gradient is illegal; using memory_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name embedding_encoder:0/gradient is illegal; using embedding_encoder_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/kernel:0/gradient is illegal; using decoder/attention_wrapper/basic_lstm_cell/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/bias:0/gradient is illegal; using decoder/attention_wrapper/basic_lstm_cell/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/luong_attention/attention_g:0/gradient is illegal; using decoder/attention_wrapper/luong_attention/attention_g_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/attention_layer/kernel:0/gradient is illegal; using decoder/attention_wrapper/attention_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/output_projection/kernel:0/gradient is illegal; using decoder/output_projection/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv1/kernel:0/weight is illegal; using conv1/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv1/bias:0/weight is illegal; using conv1/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2/kernel:0/weight is illegal; using conv2/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2/bias:0/weight is illegal; using conv2/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv3/kernel:0/weight is illegal; using conv3/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv3/bias:0/weight is illegal; using conv3/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv4/kernel:0/weight is illegal; using conv4/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv4/bias:0/weight is illegal; using conv4/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv5/kernel:0/weight is illegal; using conv5/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv5/bias:0/weight is illegal; using conv5/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d/kernel:0/weight is illegal; using conv2d/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0/weight is illegal; using conv2d/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name memory_layer/kernel:0/weight is illegal; using memory_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name embedding_encoder:0/weight is illegal; using embedding_encoder_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/kernel:0/weight is illegal; using decoder/attention_wrapper/basic_lstm_cell/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/bias:0/weight is illegal; using decoder/attention_wrapper/basic_lstm_cell/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/luong_attention/attention_g:0/weight is illegal; using decoder/attention_wrapper/luong_attention/attention_g_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/attention_layer/kernel:0/weight is illegal; using decoder/attention_wrapper/attention_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/output_projection/kernel:0/weight is illegal; using decoder/output_projection/kernel_0/weight instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "g = t.create_graph(token_vocab_size, t.hparams['num_units'], t.hparams['use_attention'], t.hparams['use_encoding_average_as_initial_state'], False)\n",
    "embedding_decoder = g['embedding_decoder']\n",
    "decoder_cell = g['decoder_cell']\n",
    "decoder_initial_state = g['decoder_initial_state']\n",
    "projection_layer = g['projection_layer']\n",
    "img = g['img']\n",
    "decoder_lengths = g['decoder_lengths']\n",
    "decoder_inputs = g['decoder_inputs']\n",
    "decoder_outputs = g['decoder_outputs']\n",
    "train_loss = g['train_loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encoder_input_data_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring\n",
      "INFO:tensorflow:Restoring parameters from /Users/adamjensen/project-environments/handwriting-to-latex-env/output/checkpoints/model_a_16.ckpt-17\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "tf_saver = tf.train.Saver(allow_empty=False)\n",
    "p = '/Users/adamjensen/project-environments/handwriting-to-latex-env/output/checkpoints/model_a_16.ckpt-17'\n",
    "t.initialize_variables(sess, tf_saver, restore=True, path=p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158.42501831054688"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_loss = t.get_loss(img,\n",
    "                  train_encoder_input_data_batches,\n",
    "                  decoder_lengths,\n",
    "                  train_sequence_lengths_batches,\n",
    "                  decoder_inputs,\n",
    "                  train_decoder_input_data_batches,\n",
    "                  decoder_outputs,\n",
    "                  train_decoder_target_data_batches,\n",
    "                  train_loss,\n",
    "                  sess)\n",
    "\n",
    "t_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_loss = t.get_loss(img,\n",
    "                  val_encoder_input_data_batches,\n",
    "                  decoder_lengths,\n",
    "                  val_sequence_lengths_batches,\n",
    "                  decoder_inputs,\n",
    "                  val_decoder_input_data_batches,\n",
    "                  decoder_outputs,\n",
    "                  val_decoder_target_data_batches,\n",
    "                  train_loss,\n",
    "                  sess)\n",
    "\n",
    "v_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val predictions\n",
    "batch = val_encoder_input_data_batches[15]\n",
    "\n",
    "little_variable, logits = t.predict_batch(sess,\n",
    "                  batch,\n",
    "                  target_token_index,\n",
    "                  embedding_decoder,\n",
    "                  decoder_cell,\n",
    "                  decoder_initial_state,\n",
    "                  projection_layer,\n",
    "                  img)\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train predictions\n",
    "batch = train_encoder_input_data_batches[0]\n",
    "e\n",
    "little_variable, logits = t.predict_batch(sess,\n",
    "                  batch,\n",
    "                  target_token_index,\n",
    "                  embedding_decoder,\n",
    "                  decoder_cell,\n",
    "                  decoder_initial_state,\n",
    "                  projection_layer,\n",
    "                  img)\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_seq(int_sequence):\n",
    "    \n",
    "    output_string = \"\"\n",
    "    for value in int_sequence:\n",
    "        output_string += reverse_target_token_index[value] + \" \"\n",
    "        if reverse_target_token_index[value] == \"**end**\":\n",
    "            break\n",
    "    return output_string\n",
    "\n",
    "for idx, seq in enumerate(train_target_texts_batches[0]):\n",
    "    \n",
    "    print(\"Output: \")\n",
    "    print(get_token_seq(little_variable[idx]))\n",
    "    print(\"\")\n",
    "    print(\"Ground truth: \")\n",
    "    print(train_target_texts_batches[0][idx])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
