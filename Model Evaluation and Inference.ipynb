{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train as t\n",
    "import importlib\n",
    "importlib.reload(t)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Enum):\n",
    "    ENCODER_INPUTS = 0\n",
    "    TARGET_TEXTS = 1\n",
    "    LENGTHS = 2\n",
    "    DECODER_INPUTS = 3\n",
    "    DECODER_TARGETS = 4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ======================= Loading Data =======================\n",
      "yesterday's data half price\n",
      "loading:  train(40, 160, 1)\n",
      "yesterday's data half price\n",
      "loading:  val(40, 160, 1)\n",
      "\n",
      " ======================= Data Loaded =======================\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary\n",
    "token_vocabulary = [\"**end**\", \"**start**\", \"**unknown**\"]\n",
    "\n",
    "token_vocabulary.extend(t.get_vocabulary(\"train\"))\n",
    "\n",
    "target_tokens = token_vocabulary  # TODO: Refactor this. Currently duplicate naming\n",
    "\n",
    "token_vocab_size = len(target_tokens)\n",
    "# todo: document what was lifted from tutorials and what we wrote ourselves\n",
    "target_token_index = dict(\n",
    "    [(token, i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "reverse_target_token_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ======================= Loading Data =======================\")\n",
    "#new cell\n",
    "\n",
    "train_dataset = t.get_data_somehow('train(40, 160, 1)', False, t.hparams['mini_batch_size'], t.hparams['max_token_length'], t.hparams['max_train_num_samples'] , target_token_index)\n",
    "val_dataset = t.get_data_somehow('val(40, 160, 1)', False, t.hparams['mini_batch_size'], t.hparams['max_token_length'], t.hparams['max_val_num_samples'], target_token_index)\n",
    "\n",
    "train_encoder_input_data_batches = train_dataset[0]\n",
    "train_target_texts_batches = train_dataset[1]\n",
    "train_sequence_lengths_batches = train_dataset[2]\n",
    "train_decoder_input_data_batches = train_dataset[3]\n",
    "train_decoder_target_data_batches = train_dataset[4]\n",
    "\n",
    "val_encoder_input_data_batches = val_dataset[0]\n",
    "val_target_texts_batches = val_dataset[1]\n",
    "val_sequence_lengths_batches = val_dataset[2]\n",
    "val_decoder_input_data_batches = val_dataset[3]\n",
    "val_decoder_target_data_batches = val_dataset[4]\n",
    "print(\"\\n ======================= Data Loaded =======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv2d/kernel:0/gradient is illegal; using conv2d/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0/gradient is illegal; using conv2d/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/kernel:0/gradient is illegal; using conv2d_1/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/bias:0/gradient is illegal; using conv2d_1/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0/gradient is illegal; using conv2d_2/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0/gradient is illegal; using conv2d_2/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/kernel:0/gradient is illegal; using conv2d_3/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/bias:0/gradient is illegal; using conv2d_3/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/kernel:0/gradient is illegal; using conv2d_4/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/bias:0/gradient is illegal; using conv2d_4/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_5/kernel:0/gradient is illegal; using conv2d_5/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_5/bias:0/gradient is illegal; using conv2d_5/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name memory_layer/kernel:0/gradient is illegal; using memory_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name embedding_encoder:0/gradient is illegal; using embedding_encoder_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/kernel:0/gradient is illegal; using decoder/attention_wrapper/basic_lstm_cell/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/bias:0/gradient is illegal; using decoder/attention_wrapper/basic_lstm_cell/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/luong_attention/attention_g:0/gradient is illegal; using decoder/attention_wrapper/luong_attention/attention_g_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/attention_layer/kernel:0/gradient is illegal; using decoder/attention_wrapper/attention_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/output_projection/kernel:0/gradient is illegal; using decoder/output_projection/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d/kernel:0/weight is illegal; using conv2d/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0/weight is illegal; using conv2d/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/kernel:0/weight is illegal; using conv2d_1/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/bias:0/weight is illegal; using conv2d_1/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0/weight is illegal; using conv2d_2/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0/weight is illegal; using conv2d_2/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/kernel:0/weight is illegal; using conv2d_3/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/bias:0/weight is illegal; using conv2d_3/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/kernel:0/weight is illegal; using conv2d_4/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/bias:0/weight is illegal; using conv2d_4/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_5/kernel:0/weight is illegal; using conv2d_5/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_5/bias:0/weight is illegal; using conv2d_5/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name memory_layer/kernel:0/weight is illegal; using memory_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name embedding_encoder:0/weight is illegal; using embedding_encoder_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/kernel:0/weight is illegal; using decoder/attention_wrapper/basic_lstm_cell/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/bias:0/weight is illegal; using decoder/attention_wrapper/basic_lstm_cell/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/luong_attention/attention_g:0/weight is illegal; using decoder/attention_wrapper/luong_attention/attention_g_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/attention_layer/kernel:0/weight is illegal; using decoder/attention_wrapper/attention_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/output_projection/kernel:0/weight is illegal; using decoder/output_projection/kernel_0/weight instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "g = t.create_graph(token_vocab_size, t.hparams['num_units'], t.hparams['use_attention'], t.hparams['use_encoding_average_as_initial_state'], False)\n",
    "embedding_decoder = g['embedding_decoder']\n",
    "decoder_cell = g['decoder_cell']\n",
    "decoder_initial_state = g['decoder_initial_state']\n",
    "projection_layer = g['projection_layer']\n",
    "img = g['img']\n",
    "decoder_lengths = g['decoder_lengths']\n",
    "decoder_inputs = g['decoder_inputs']\n",
    "decoder_outputs = g['decoder_outputs']\n",
    "train_loss = g['train_loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "577"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encoder_input_data_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring\n",
      "INFO:tensorflow:Restoring parameters from /Users/adamjensen/project-environments/handwriting-to-latex-env/output/checkpoints/model_47.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "t.initialize_variables(sess, restore=True, path=\"/Users/adamjensen/project-environments/handwriting-to-latex-env/output/checkpoints/model_47.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.205822449622989"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_loss = t.get_loss(img,\n",
    "                  train_encoder_input_data_batches,\n",
    "                  decoder_lengths,\n",
    "                  train_sequence_lengths_batches,\n",
    "                  decoder_inputs,\n",
    "                  train_decoder_input_data_batches,\n",
    "                  decoder_outputs,\n",
    "                  train_decoder_target_data_batches,\n",
    "                  train_loss,\n",
    "                  sess)\n",
    "\n",
    "t_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.728467825687289"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_loss = t.get_loss(img,\n",
    "                  val_encoder_input_data_batches,\n",
    "                  decoder_lengths,\n",
    "                  val_sequence_lengths_batches,\n",
    "                  decoder_inputs,\n",
    "                  val_decoder_input_data_batches,\n",
    "                  decoder_outputs,\n",
    "                  val_decoder_target_data_batches,\n",
    "                  train_loss,\n",
    "                  sess)\n",
    "\n",
    "v_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 70, 400)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val predictions\n",
    "batch = val_encoder_input_data_batches[0]\n",
    "\n",
    "little_variable, logits = t.predict_batch(sess,\n",
    "                  batch,\n",
    "                  target_token_index,\n",
    "                  embedding_decoder,\n",
    "                  decoder_cell,\n",
    "                  decoder_initial_state,\n",
    "                  projection_layer,\n",
    "                  img)\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train predictions\n",
    "batch = train_encoder_input_data_batches[0]\n",
    "\n",
    "little_variable, logits = t.predict_batch(sess,\n",
    "                  batch,\n",
    "                  target_token_index,\n",
    "                  embedding_decoder,\n",
    "                  decoder_cell,\n",
    "                  decoder_initial_state,\n",
    "                  projection_layer,\n",
    "                  img)\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "E ( v ) = { \\frac { d } { d \\beta } } \\, { \\cal V } ( k ) **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** \\, ^ { * } d \\, ^ { * } H = \\kappa \\, ^ { * } d \\phi = J _ { B } .\n",
      "\n",
      "Output: \n",
      "( \\partial _ { i } , \\partial _ { j } ) _ { ( n ) } = \\sqrt { \\chi } e ^ { 2 U } **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** \\partial _ { \\mu } ( F ^ { \\mu \\nu } - e j ^ { \\mu } x ^ { \\nu } ) = 0 .\n",
      "\n",
      "Output: \n",
      "{ \\cal S } ^ { \\mu \\nu } \\equiv \\epsilon ^ { \\mu \\nu } \\equiv \\epsilon ^ { \\mu \\nu \\nu } F _ { \\mu } { } _ { \\nu } = 0 \\ \\ , **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** L _ { 0 } = \\Phi ( w ) = \\bigtriangleup \\Phi ( w ) ,\n",
      "\n",
      "Output: \n",
      "M _ { E } ^ { 2 } \\int d c x ^ { - 2 \\sigma ( x ) } **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** \\left( D ^ { * } D ^ { * } + m ^ { 2 } \\right) { \\cal H } = 0\n",
      "\n",
      "Output: \n",
      "t _ { i } = { \\frac { E } { 2 \\pi } } , , \\hspace { s c n } _ { 2 } = - \\frac { 2 } { 2 \\pi } , **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** \\langle T _ { z z } \\rangle = - 3 \\times \\frac { \\pi ^ { 2 } } { 1 4 4 0 a ^ { 4 } } .\n",
      "\n",
      "Output: \n",
      "\\chi _ { 2 } ^ { S S } ( \\partial ) = \\sum S _ { 0 } ^ { ( 2 ) } { } ^ { B } ( \\partial ) = 2 **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** \\beta ( g ) \\frac { \\partial } { \\partial g } = 2 g \\beta ( g ) \\frac { \\partial } { \\partial g ^ { 2 } }\n",
      "\n",
      "Output: \n",
      "S \\left( \\mathbf { 3 } + \\delta A \\right) = S \\left( 2 \\right) **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** \\hat { e } = e / \\varepsilon , \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat { G } _ { 4 } = G _ { 4 } ,\n",
      "\n",
      "Output: \n",
      "\\delta _ { i } \\Psi = \\mathrm { r \\star } \\star \\epsilon \\epsilon \\epsilon \\epsilon \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \n",
      "\n",
      "Ground truth: \n",
      "**start** | 0 ( t ) \\rangle _ { e , \\mu } \\equiv G _ { \\theta } ^ { - 1 } ( t ) | 0 \\rangle _ { 1 , 2 } \\, ,\n",
      "\n",
      "Output: \n",
      "\\alpha = - \\frac { 1 } { 6 ( 3 + 1 6 k _ { 0 } R ^ { 2 } } } . **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** { \\cal L } \\rightarrow { \\cal L } + \\frac { \\alpha N } { 1 6 \\pi ^ { 2 } } F \\tilde { F } .\n",
      "\n",
      "Output: \n",
      "{ \\cal F } = { 0 } , ~ { \\cal C } ^ { 2 } = Q { \\cal L } + { \\cal P } ^ { 2 } = 0 , **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** \\phi ( x , y ) = \\lambda ^ { 2 s } \\, \\phi ( \\lambda x , \\lambda y )\n",
      "\n",
      "Output: \n",
      "\\xi ^ { * ^ { 1 } ( x ) } { x } = { \\frac { \\xi } { 4 } ^ { 0 } } { x } _ { i } ( x ) = 0 **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** m _ { H } \\approx 2 7 2 \\, \\lambda ^ { 1 / 4 } \\ \\mathrm { G e V } .\n",
      "\n",
      "Output: \n",
      "\\Delta = - D ^ { 2 } - \\frac { 1 } { 2 } \\sigma _ { \\mu \\nu } F _ { \\mu \\nu } . **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** S _ { \\Omega ^ { \\prime } , \\Omega } x \\Omega = x ^ { * } \\Omega , \\, \\, x \\in M\n",
      "\n",
      "Output: \n",
      "m \\simeq a ~ \\sqrt { | e H | } \\, \\mathrm { e } { \\cal H } | \\, e ^ { - { \\cal { \\cal x } } _ { - 1 } } , **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** \\operatorname { e x p } \\left( i p _ { \\mu } X ^ { \\mu } \\right) \\rightarrow \\hat { v } _ { p } = \\hat { h } ^ { k _ { 2 } }\n",
      "\n",
      "Output: \n",
      "- \\frac { 1 } { 2 \\pi _ { 0 } ^ { 2 } } + V ^ { \\prime } ( x _ { 0 } ) = 0 . **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** T _ { [ \\mu \\nu ] } ^ { \\ a } = \\partial _ { \\mu } E _ { \\nu } ^ { \\underline { { a } } } - \\partial _ { \\nu } E _ { \\mu } ^ { \\underline { { a } } }\n",
      "\n",
      "Output: \n",
      "\\delta _ { 2 } \\bar { Q } \\sim \\bar { Q } { \\frac { 4 \\omega _ { 0 } } } { 4 } } \\mathrm { l o n } \\Bigl \\frac { u _ { 0 } } { 4 ^ { 2 \\nu \\nu } } \\mathrm { l n } \\Bigl \\frac { \\omega _ { 0 } } { a } \n",
      "\n",
      "Ground truth: \n",
      "**start** \\chi _ { a } ( A ) = \\varepsilon _ { a b i } \\, A _ { b i } ( x ) = 0 \\, ,\n",
      "\n",
      "Output: \n",
      "g _ { a b } = \\partial _ { \\mu } \\, \\partial _ { b } K ( x , \\bar { z } ) \\ . **end** \n",
      "\n",
      "Ground truth: \n",
      "**start** \\varphi \\sim A + B \\mathrm { s i g n } ( t ) | \\vec { k } t | ^ { \\epsilon } .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_token_seq(int_sequence):\n",
    "    \n",
    "    output_string = \"\"\n",
    "    for value in int_sequence:\n",
    "        output_string += reverse_target_token_index[value] + \" \"\n",
    "        if reverse_target_token_index[value] == \"**end**\":\n",
    "            break\n",
    "    return output_string\n",
    "\n",
    "for idx, seq in enumerate(train_target_texts_batches[0]):\n",
    "    \n",
    "    print(\"Output: \")\n",
    "    print(get_token_seq(little_variable[idx]))\n",
    "    print(\"\")\n",
    "    print(\"Ground truth: \")\n",
    "    print(train_target_texts_batches[0][idx])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
