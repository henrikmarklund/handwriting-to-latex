{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from prepare_data import get_decoder_data_int_sequences\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensor2tensor.layers.common_attention import add_timing_signal_nd #currently not in use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Outline\n",
    "\n",
    "# 1. Encoder\n",
    "# 2. Decoder\n",
    "# 3. Optimization and training\n",
    "\n",
    "\n",
    "# What is bucketing?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_dict = {(40, 160): 0,\n",
    " (40, 200): 1,\n",
    " (40, 240): 2,\n",
    " (40, 280): 3,\n",
    " (40, 320): 4,\n",
    " (40, 360): 5,\n",
    " (50, 120): 6,\n",
    " (50, 200): 7,\n",
    " (50, 240): 8,\n",
    " (50, 280): 9,\n",
    " (50, 320): 10,\n",
    " (50, 360): 11,\n",
    " (50, 400): 12,\n",
    " (60, 360): 13,\n",
    " (100, 360): 14,\n",
    " (100, 500): 15,\n",
    " (160, 400): 16,\n",
    " (200, 500): 17,\n",
    " (800, 800): 18}\n",
    "\n",
    "buckets = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "mini_batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "def load_raw_data(dataset_name, max_token_length = 400, max_num_samples = 5000):\n",
    "\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    if dataset_name == \"small\":\n",
    "        image_folder = 'data/tin/tiny/'\n",
    "        formula_file_path = \"data/tin/tiny.formulas.norm.txt\"\n",
    "    elif dataset_name == \"test\":\n",
    "        image_folder = 'data/images_test/'\n",
    "        formula_file_path = \"data/test.formulas.norm.txt\"\n",
    "    elif dataset_name == \"train\":\n",
    "        image_folder = 'data/images_train/'\n",
    "        formula_file_path = \"data/train.formulas.norm.txt\"\n",
    "    elif dataset_name == \"val\":\n",
    "        image_folder = 'data/images_val/'\n",
    "        formula_file_path = \"data/val.formulas.norm.txt\"\n",
    "    elif dataset_name == \"digital_numbers\":\n",
    "        image_folder = 'datasets/digital_numbers/images/'\n",
    "        formula_file_path = \"datasets/digital_numbers/number_sequences.txt\"\n",
    "\n",
    "        \n",
    "    included_counter = 0\n",
    "    examples_counter = 0\n",
    "    with open (formula_file_path, \"r\") as myfile:\n",
    "\n",
    "        for idx, token_sequence in enumerate(myfile):\n",
    "            examples_counter += 1\n",
    "            #Check token size:\n",
    "            token_sequence = token_sequence.rstrip('\\n')\n",
    "            tokens = token_sequence.split()\n",
    "\n",
    "            file_name = str(idx) + '.png'\n",
    "            image = cv2.imread(image_folder + file_name, 0)\n",
    "            \n",
    "            if image is None:\n",
    "                print(\"Id:\", idx)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            #print(tokens)\n",
    "            if len(tokens) <= max_token_length:\n",
    "                \n",
    "                token_sequence = '**start** ' + token_sequence\n",
    "                #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #Grey scale\n",
    "                #print(image)\n",
    "                \n",
    "                seq_length = len(token_sequence.split())\n",
    "                \n",
    "                relevant_bucket_id = buckets_dict[image.shape]\n",
    "                #print(relevant_bucket_id)\n",
    "                #print(buckets[relevant_bucket_id])\n",
    "                buckets[relevant_bucket_id].append([image, token_sequence, seq_length])\n",
    "                \n",
    "                \n",
    "                \n",
    "                #print(\"Bucket \" + str(buckets_dict[image.shape]) + \"size: \" + str(len(buckets[buckets_dict[image.shape]])))\n",
    "                if len(buckets[buckets_dict[image.shape]]) == mini_batch_size:\n",
    "                    data_batch = np.array(buckets[buckets_dict[image.shape]])\n",
    "                    dataset.append(data_batch)\n",
    "                    buckets[buckets_dict[image.shape]] = []\n",
    "\n",
    "                included_counter += 1\n",
    "                if included_counter == max_num_samples:\n",
    "                    break\n",
    "\n",
    "                #print(dataset)\n",
    "        for idx, bucket in enumerate(buckets):\n",
    "            if len(bucket) != 0:\n",
    "                data_batch = np.array(bucket)\n",
    "                dataset.append(data_batch)\n",
    "                buckets[idx] = []\n",
    "        \n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is list of all batches containing (image, target_text, sequence_length)\n",
    "# we split that up into three lists\n",
    "\n",
    "\n",
    "def split_dataset(dataset):\n",
    "\n",
    "    encoder_input_data_batches = []\n",
    "    target_texts_batches = []\n",
    "    sequence_lengths_batches = []\n",
    "\n",
    "    for batch in range(len(dataset)):\n",
    "        image_batch = dataset[batch][:,0]\n",
    "        image_batch = image_batch.tolist()\n",
    "        image_batch = np.array(image_batch)\n",
    "        # Add one dimension so that the conv net can take it (it expects four dimensions)\n",
    "        image_batch = np.reshape(image_batch, (image_batch.shape[0], image_batch.shape[1], image_batch.shape[2], 1))\n",
    "        image_batch = image_batch.astype('uint8')\n",
    "\n",
    "        \n",
    "        encoder_input_data_batches.append(image_batch)\n",
    "\n",
    "        target_text = dataset[batch][:,1]\n",
    "        target_texts_batches.append(target_text)\n",
    "\n",
    "        decoder_length = dataset[batch][:,2]\n",
    "        decoder_length = np.array(decoder_length)\n",
    "        sequence_lengths_batches.append(decoder_length)\n",
    "\n",
    "    # Make sure we have equal number of batches\n",
    "    assert (len(encoder_input_data_batches) == len(dataset))\n",
    "    assert (len(target_texts_batches) == len(dataset))\n",
    "    assert (len(sequence_lengths_batches) == len(dataset))\n",
    "    \n",
    "    return encoder_input_data_batches, target_texts_batches, sequence_lengths_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(dataset):\n",
    "    if dataset == \"small\":\n",
    "        vocab = [line for line in open('data/tin/tiny_vocab.txt')]\n",
    "    elif dataset == \"test\":\n",
    "        vocab = [line for line in open('data/vocab.txt')]\n",
    "    elif dataset == \"train\":\n",
    "        vocab = [line for line in open('data/vocab.txt')]\n",
    "\n",
    "    vocab = [x.strip('\\n') for x in vocab]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary\n",
    "\n",
    "token_vocabulary = []\n",
    "token_vocabulary.append(\"**end**\") # Want this to be the first token\n",
    "token_vocabulary.append(\"**start**\")\n",
    "token_vocabulary.append(\"**unknown**\")\n",
    "\n",
    "token_vocabulary.extend(get_vocabulary(\"train\"))\n",
    "\n",
    "target_tokens = token_vocabulary # TODO: Refactor this. Currently duplicate naming\n",
    "\n",
    "token_vocab_size = len(target_tokens)\n",
    "\n",
    "target_token_index = dict(\n",
    "    [(token, i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "reverse_target_token_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(token_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_int_sequences(target_texts_batches, sequence_lengths_batches):\n",
    "    decoder_input_data_batches = []\n",
    "    decoder_target_data_batches = []\n",
    "    \n",
    "    for idx, target_texts_batch in enumerate(target_texts_batches):\n",
    "    \n",
    "        # get max dec seq length for that batch\n",
    "        max_decoder_seq_length = max(sequence_lengths_batches[idx])\n",
    "\n",
    "        batch_size = len(target_texts_batch)\n",
    "        \n",
    "        decoder_input_data = np.zeros(\n",
    "                (batch_size, max_decoder_seq_length),\n",
    "                dtype='int32')\n",
    "        decoder_target_data = np.zeros(\n",
    "                (batch_size, max_decoder_seq_length),\n",
    "                dtype='int32')\n",
    "\n",
    "        num_other = 0\n",
    "\n",
    "        for i, target_text in enumerate(target_texts_batch):\n",
    "            for t, token in enumerate(target_text.split()):\n",
    "                \n",
    "                if token in target_token_index:\n",
    "                    # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "\n",
    "                    decoder_input_data[i, t] = target_token_index[token]\n",
    "\n",
    "                    if t > 0:\n",
    "                        #decoder_target_data will be ahead by one timestep\n",
    "                        # and will not include the start character.\n",
    "                        decoder_target_data[i, t - 1] = target_token_index[token]\n",
    "\n",
    "                else:\n",
    "                    print(\"Token not in vocabulary (setting to **unknown**): \", token)\n",
    "                    num_other = num_other + 1\n",
    "                    decoder_input_data[i, t] = target_token_index['**unknown**']\n",
    "\n",
    "                    if t > 0:\n",
    "                        #decoder_target_data will be ahead by one timestep\n",
    "                        # and will not include the start character.\n",
    "\n",
    "                        decoder_target_data[i, t - 1] = target_token_index['**unknown**']\n",
    "\n",
    "            decoder_target_data[i, len(target_text.split()) - 1] = target_token_index['**end**']  \n",
    "            \n",
    "            \n",
    "        \n",
    "        decoder_input_data_batches.append(decoder_input_data)\n",
    "        decoder_target_data_batches.append(decoder_target_data)\n",
    "        \n",
    "    return decoder_input_data_batches, decoder_target_data_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name, max_token_length, max_num_samples):\n",
    "    dataset = load_raw_data(dataset_name, max_token_length = max_token_length, max_num_samples = max_num_samples)\n",
    "    encoder_input_data_batches, target_texts_batches, sequence_lengths_batches = split_dataset(dataset)\n",
    "    decoder_input_data_batches, decoder_target_data_batches = create_output_int_sequences(target_texts_batches, sequence_lengths_batches)    \n",
    "\n",
    "    return encoder_input_data_batches, target_texts_batches, sequence_lengths_batches, decoder_input_data_batches, decoder_target_data_batches\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Token not in vocabulary (setting to **unknown**): ', '\\\\c')\n",
      "('Token not in vocabulary (setting to **unknown**): ', '\\\\c')\n"
     ]
    }
   ],
   "source": [
    "max_token_length = 50\n",
    "max_train_num_samples = 200\n",
    "\n",
    "\n",
    "train_dataset = load_data('train', max_token_length, max_num_samples)\n",
    "train_encoder_input_data_batches = train_dataset[0]\n",
    "train_target_texts_batches = train_dataset[1]\n",
    "train_sequence_lengths_batches = train_dataset[2]\n",
    "train_decoder_input_data_batches = train_dataset[3]\n",
    "train_decoder_target_data_batches = train_dataset[4]\n",
    "\n",
    "\n",
    "val_dataset = load_data('train', max_token_length, max_num_samples)\n",
    "val_encoder_input_data_batches = val_dataset[0]\n",
    "val_target_texts_batches = val_dataset[1]\n",
    "val_sequence_lengths_batches = val_dataset[2]\n",
    "val_decoder_input_data_batches = val_dataset[3]\n",
    "val_decoder_target_data_batches = val_dataset[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e6a0790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Make a histogram of how long the sequences.\n",
    "## Helps us decide on a cut off point\n",
    "\n",
    "formula_file_path = \"data/train.formulas.norm.txt\"\n",
    "\n",
    "formula_lengths = []\n",
    "image_folder = 'data/images_train/'\n",
    "image_diagonals = []\n",
    "image_widths = []\n",
    "image_heights = []\n",
    "import math\n",
    "with open (formula_file_path, \"r\") as myfile:\n",
    "    for idx, token_sequence in enumerate(myfile):\n",
    "        tokens = token_sequence.split()\n",
    "        formula_lengths.append(len(tokens))\n",
    "        file_name = str(idx) + '.png'\n",
    "        image = cv2.imread(image_folder + file_name, 0)\n",
    "        if image is not None:\n",
    "            image_heights.append(image.shape[0])\n",
    "            image_widths.append(image.shape[1])\n",
    "            \n",
    "            diagonal = math.sqrt(image.shape[0]**2 + image.shape[1]**2)\n",
    "            image_diagonals.append(int(diagonal))\n",
    "\n",
    "\n",
    "        \n",
    "%matplotlib inline\n",
    "plt.hist(formula_lengths, normed=False, bins=40)\n",
    "plt.ylabel('Num of expressions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(40, 160),\n",
       " (40, 200),\n",
       " (40, 240),\n",
       " (40, 280),\n",
       " (40, 320),\n",
       " (40, 360),\n",
       " (50, 120),\n",
       " (50, 200),\n",
       " (50, 240),\n",
       " (50, 280),\n",
       " (50, 320),\n",
       " (50, 360),\n",
       " (50, 400),\n",
       " (60, 360),\n",
       " (100, 360),\n",
       " (100, 500),\n",
       " (160, 400),\n",
       " (200, 500),\n",
       " (800, 800)}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height_width_pair = zip(image_heights, image_widths)\n",
    "\n",
    "height_width_pair_set = set(height_width_pair)\n",
    "print(len(height_width_pair_set))\n",
    "height_width_pair_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_input_data = tf.convert_to_tensor(encoder_input_data)\n",
    "#target_sequences = tf.convert_to_tensor(target_texts)\n",
    "\n",
    "\n",
    "#image_dataset = tf.data.Dataset.from_tensor_slices(encoder_input_data)\n",
    "#seq_dataset = tf.data.Dataset.from_tensor_slices(target_sequences)\n",
    "#seq_dataset = seq_dataset.map(lambda tgt: tf.string_split([tgt]).values)\n",
    "#seq_dataset = seq_dataset.map(lambda tgt: tf.cast(target_token_index[tgt], tf.int32))\n",
    "\n",
    "#seq_dataset = src_dataset.map(lambda tgt: (tgt, tf.size(tgt)))\n",
    "#seq_dataset = src_dataset.map(lambda tgt: (tf.concat(([target_token_index[\"**start**\"]], tgt), 0),\n",
    "                                           #tf.concat((tgt, [target_token_index[\"**end**\"]],), 0)))\n",
    "\n",
    "#seq_dataset = seq_dataset.map(lambda tgt_in, tgt_out: (tgt_in, tgt_out, tf.size(tgt_in)))\n",
    "                              \n",
    "#src_tgt_dataset = tf.data.Dataset.zip(image_dataset, seq_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 50)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_decoder_target_data_batches[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACuCAYAAAAmsfauAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIMuIpITatBFRHJCDbqISE78PxwxnXD1E7csAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1042a2710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[401 396 174 396 371  54 398 396 371  86 398 398  29  12 396 174 396 392\n",
      "  86 398 396  86 365 396  62 365 396  16 398 398 364 396  18 398 398 398\n",
      "  64  14   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "\n",
      "[396 174 396 371  54 398 396 371  86 398 398  29  12 396 174 396 392  86\n",
      " 398 396  86 365 396  62 365 396  16 398 398 364 396  18 398 398 398  64\n",
      "  14 402   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "#encoder_input_data.shape\n",
    "\n",
    "plt.imshow(train_encoder_input_data_batches[0][0].squeeze(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(train_decoder_input_data_batches[0][0])\n",
    "\n",
    "print(\"\")\n",
    "print(train_decoder_target_data_batches[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder\n",
    "\n",
    "## One of Genthails's encoder implementations (from paper)\n",
    "\n",
    "\n",
    "\n",
    "img = tf.placeholder(tf.uint8, [None, None, None, 1], name='img')\n",
    "\n",
    "img = tf.cast(img, tf.float32) / 255\n",
    "\n",
    "\n",
    "batch_size = tf.shape(img)[0]\n",
    "\n",
    "# Conv + max pooling\n",
    "out = tf.layers.conv2d(img, 64, 3, 1, \"SAME\", activation=tf.nn.relu)\n",
    "# Conv + max pooling\n",
    "out = tf.layers.conv2d(out, 128, 3, 1, \"SAME\", activation=tf.nn.relu)\n",
    "\n",
    "out = tf.layers.conv2d(out, 256, 3, 1, \"SAME\", activation=tf.nn.relu) # regular conv -> id\n",
    "out = tf.layers.batch_normalization(out)\n",
    "\n",
    "\n",
    "out = tf.layers.conv2d(out, 256, 3, 1, \"SAME\", activation=tf.nn.relu) # regular conv -> id\n",
    "out = tf.layers.max_pooling2d(out, (2, 1), (2, 1), \"SAME\")\n",
    "\n",
    "out = tf.layers.conv2d(out, 512, 3, 1, \"SAME\", activation=tf.nn.relu) # regular conv -> id\n",
    "out = tf.layers.max_pooling2d(out, (1, 2), (1, 2), \"SAME\")\n",
    "\n",
    "\n",
    "# Conv valid\n",
    "out = tf.layers.conv2d(out, 512, 3, 1, \"VALID\", activation=tf.nn.relu, name=\"last_conv_layer\") # conv\n",
    "out = tf.layers.batch_normalization(out)\n",
    "\n",
    "## Out is now a H'*W' encoding of the image\n",
    "\n",
    "## We want to turn this into a sequence of vectors: (e1, e2 ... en)\n",
    "#H= out.shape[1]\n",
    "#W= out.shape[2] \n",
    "#C= out.shape[3]\n",
    "\n",
    "H = tf.shape(out)[1]\n",
    "W = tf.shape(out)[2]\n",
    "\n",
    "#out = add_timing_signal_nd(out)\n",
    "seq = tf.reshape(tensor=out, shape=[-1, H*W, 512])\n",
    "\n",
    "# TODO: Add positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First state of the decoder consists of two vectors, the hidden state (h0) and the memory (c0).\n",
    "# Usually the hidden state refers to [h0, c0]. So a little bit of overloading of hidden state (I think)\n",
    "# This is how Genthail implements it\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "num_units = 512\n",
    "\n",
    "img_mean = tf.reduce_mean(seq, axis=1)\n",
    "\n",
    "img_mean = tf.layers.batch_normalization(img_mean)\n",
    "\n",
    "W = tf.get_variable(\"W\", shape=[512, num_units])\n",
    "b = tf.get_variable(\"b\", shape=[num_units])\n",
    "h0 = tf.tanh(tf.matmul(img_mean, W) + b)\n",
    "\n",
    "W_ = tf.get_variable(\"W_\", shape=[512, num_units])\n",
    "b_ = tf.get_variable(\"b_\", shape=[num_units])\n",
    "c0 = tf.tanh(tf.matmul(img_mean, W_) + b_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoder_state = tf.contrib.rnn.LSTMStateTuple(c0, h0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'Tanh_1:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'Tanh:0' shape=(?, 512) dtype=float32>)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention_states: [batch_size, max_time, num_units]\n",
    "attention_states = seq\n",
    "\n",
    "\n",
    "attention_depth = num_units\n",
    "\n",
    "# Create an attention mechanism\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "    attention_depth, attention_states, scale=True) # Can try scale = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 80)\n"
     ]
    }
   ],
   "source": [
    "# Decoder: from seq2seq tutorial \n",
    "\n",
    "\n",
    "\n",
    "embedding_size = 80 # In Genthail's paper he says he has 80 embeddings which I believe corresponds to embedding_size\n",
    "\n",
    "decoder_inputs = tf.placeholder(tf.int32, [None, None], name='decoder_inputs')  # Supposed to be a sequence of numbers corresponding to the different tokens in the sentence\n",
    "\n",
    "# Embedding of target tokens\n",
    "\n",
    "# Embedding matrix \n",
    "embedding_decoder = tf.get_variable(\n",
    "    \"embedding_encoder\", [token_vocab_size, embedding_size], tf.float32, initializer=tf.orthogonal_initializer) #  tf.float32 was default in the NMT tutorial\n",
    "\n",
    "\n",
    "\n",
    "# Look up embedding:\n",
    "#   decoder_inputs: [max_time, batch_size]\n",
    "#   decoder_emb_inp: [max_time, batch_size, embedding_size]\n",
    "decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "    embedding_decoder, decoder_inputs)\n",
    "\n",
    "print(decoder_emb_inp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build RNN cell\n",
    "#decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "\n",
    "# Using this instead compared to NMT tutorial so we can initialize with orthogonal intializer (like Genthail)\n",
    "decoder_cell = tf.nn.rnn_cell.LSTMCell(\n",
    "    num_units,\n",
    "    initializer=tf.orthogonal_initializer,\n",
    ")\n",
    "\n",
    "attention = True\n",
    "if attention:\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        decoder_cell, attention_mechanism,\n",
    "        attention_layer_size=512)\n",
    "\n",
    "    ## Set initial state of decoder to zero (possible to use previous state)\n",
    "\n",
    "    use_encoder_state = True\n",
    "    if use_encoder_state:\n",
    "        decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n",
    "    else:\n",
    "        decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "else:\n",
    "    decoder_initial_state = encoder_state\n",
    "\n",
    "    \n",
    "decoder_lengths = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    decoder_emb_inp, decoder_lengths, time_major=False)\n",
    "\n",
    "# Projection layer\n",
    "projection_layer = layers_core.Dense(token_vocab_size, use_bias=False, name=\"output_projection\")# Said layers_core before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decoder\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    decoder_cell, helper, decoder_initial_state,\n",
    "    output_layer=projection_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0-dev20171121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'Tanh_1:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'Tanh:0' shape=(?, 512) dtype=float32>), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(?, 512) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(?, ?) dtype=float32>, alignment_history=())"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "decoder_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 403)\n"
     ]
    }
   ],
   "source": [
    "# Dynamic decoding\n",
    "outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=False)  ## Understand parameter Impute finished\n",
    "logits = outputs.rnn_output\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential learning rate\n",
    "#starter_learning_rate = 0.0006\n",
    "#learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           #5, 0.93, staircase=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Piece wise learning rate\n",
    "\n",
    "#steps_per_epoch = num_samples / mini_batch_size\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "boundaries = [30,60, 90, 120, 150, 180, 210, 240]  \n",
    "\n",
    "#boundaries = [steps_per_epoch * epoch ]\n",
    "values = [0.0001, 0.0001, 0.001, 0.0005, 0.0002, 0.0001, 0.00009, 0.00008, 0.00007]\n",
    "    \n",
    "learning_rate = tf.train.piecewise_constant(global_step, boundaries, values, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_weights = tf.placeholder(tf.int8, [None, None], name='target_weights')\n",
    "#target_weights = tf.cast(target_weights, tf.float32)\n",
    "\n",
    "# Supposed to be a sequence of numbers corresponding to the different tokens in the sentence\n",
    "decoder_outputs = tf.placeholder(tf.int32, [None, None], name='decoder_outputs') \n",
    "\n",
    "\n",
    "\n",
    "#learning_rate = tf.placeholder(tf.float32, shape=[])    \n",
    "    \n",
    "# Loss function\n",
    "\n",
    "# HYPERPARAMETER: Should we divide by sequence length on each\n",
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=decoder_outputs, logits=logits)\n",
    "\n",
    "# Create the target_weights (the masking)\n",
    "max_seq_length = tf.shape(decoder_outputs)[1]\n",
    "target_weights = tf.sequence_mask(decoder_lengths, max_seq_length, dtype=logits.dtype)\n",
    "\n",
    "\n",
    "train_loss = tf.reduce_sum(crossent * target_weights) / tf.cast(batch_size, tf.float32)\n",
    "\n",
    "tf.summary.scalar('loss', train_loss)\n",
    "\n",
    "# Calculate and clip gradients\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(train_loss, params)\n",
    "\n",
    "\n",
    "max_gradient_norm = 3  # Usually a number between 1 and 5. Set to 5 in the NMT.\n",
    "\n",
    "clipped_gradients, global_norm = tf.clip_by_global_norm(\n",
    "    gradients, max_gradient_norm)\n",
    "\n",
    "tf.summary.scalar('global_norm', global_norm)\n",
    "\n",
    "\n",
    "# Optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "update_step = optimizer.apply_gradients(\n",
    "    zip(clipped_gradients, params), global_step=global_step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv2d/kernel:0/gradient is illegal; using conv2d/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0/gradient is illegal; using conv2d/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/kernel:0/gradient is illegal; using conv2d_1/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/bias:0/gradient is illegal; using conv2d_1/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0/gradient is illegal; using conv2d_2/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0/gradient is illegal; using conv2d_2/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization/gamma:0/gradient is illegal; using batch_normalization/gamma_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization/beta:0/gradient is illegal; using batch_normalization/beta_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/kernel:0/gradient is illegal; using conv2d_3/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/bias:0/gradient is illegal; using conv2d_3/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/kernel:0/gradient is illegal; using conv2d_4/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/bias:0/gradient is illegal; using conv2d_4/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/kernel:0/gradient is illegal; using last_conv_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/bias:0/gradient is illegal; using last_conv_layer/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_1/gamma:0/gradient is illegal; using batch_normalization_1/gamma_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_1/beta:0/gradient is illegal; using batch_normalization_1/beta_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/gamma:0/gradient is illegal; using batch_normalization_2/gamma_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/beta:0/gradient is illegal; using batch_normalization_2/beta_0/gradient instead.\n",
      "INFO:tensorflow:Summary name W:0/gradient is illegal; using W_0/gradient instead.\n",
      "INFO:tensorflow:Summary name b:0/gradient is illegal; using b_0/gradient instead.\n",
      "INFO:tensorflow:Summary name W_:0/gradient is illegal; using W__0/gradient instead.\n",
      "INFO:tensorflow:Summary name b_:0/gradient is illegal; using b__0/gradient instead.\n",
      "INFO:tensorflow:Summary name memory_layer/kernel:0/gradient is illegal; using memory_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name embedding_encoder:0/gradient is illegal; using embedding_encoder_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/lstm_cell/kernel:0/gradient is illegal; using decoder/attention_wrapper/lstm_cell/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/lstm_cell/bias:0/gradient is illegal; using decoder/attention_wrapper/lstm_cell/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/luong_attention/attention_g:0/gradient is illegal; using decoder/attention_wrapper/luong_attention/attention_g_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/attention_layer/kernel:0/gradient is illegal; using decoder/attention_wrapper/attention_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/output_projection/kernel:0/gradient is illegal; using decoder/output_projection/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d/kernel:0/weight is illegal; using conv2d/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0/weight is illegal; using conv2d/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/kernel:0/weight is illegal; using conv2d_1/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/bias:0/weight is illegal; using conv2d_1/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0/weight is illegal; using conv2d_2/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0/weight is illegal; using conv2d_2/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization/gamma:0/weight is illegal; using batch_normalization/gamma_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization/beta:0/weight is illegal; using batch_normalization/beta_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/kernel:0/weight is illegal; using conv2d_3/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/bias:0/weight is illegal; using conv2d_3/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/kernel:0/weight is illegal; using conv2d_4/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/bias:0/weight is illegal; using conv2d_4/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/kernel:0/weight is illegal; using last_conv_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/bias:0/weight is illegal; using last_conv_layer/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_1/gamma:0/weight is illegal; using batch_normalization_1/gamma_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_1/beta:0/weight is illegal; using batch_normalization_1/beta_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/gamma:0/weight is illegal; using batch_normalization_2/gamma_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/beta:0/weight is illegal; using batch_normalization_2/beta_0/weight instead.\n",
      "INFO:tensorflow:Summary name W:0/weight is illegal; using W_0/weight instead.\n",
      "INFO:tensorflow:Summary name b:0/weight is illegal; using b_0/weight instead.\n",
      "INFO:tensorflow:Summary name W_:0/weight is illegal; using W__0/weight instead.\n",
      "INFO:tensorflow:Summary name b_:0/weight is illegal; using b__0/weight instead.\n",
      "INFO:tensorflow:Summary name memory_layer/kernel:0/weight is illegal; using memory_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name embedding_encoder:0/weight is illegal; using embedding_encoder_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/lstm_cell/kernel:0/weight is illegal; using decoder/attention_wrapper/lstm_cell/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/lstm_cell/bias:0/weight is illegal; using decoder/attention_wrapper/lstm_cell/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/luong_attention/attention_g:0/weight is illegal; using decoder/attention_wrapper/luong_attention/attention_g_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/attention_layer/kernel:0/weight is illegal; using decoder/attention_wrapper/attention_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/output_projection/kernel:0/weight is illegal; using decoder/output_projection/kernel_0/weight instead.\n"
     ]
    }
   ],
   "source": [
    "param_names = [v.name for v in params]\n",
    "\n",
    "gradient_names = [g.name for g in gradients]\n",
    "\n",
    "gradient_norms = [tf.norm(gradient) for gradient in gradients]\n",
    "\n",
    "\n",
    "grads = list(zip(gradients, params))\n",
    "\n",
    "\n",
    "\n",
    "for grad, var in grads:\n",
    "    tf.summary.histogram(var.name + '/gradient', grad)\n",
    "\n",
    "for param in params:\n",
    "    to_summary = tf.summary.histogram(param.name + '/weight', param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the mask\n",
    "\n",
    "#decoder_mask_data = np.zeros(shape=(decoder_target_data.shape), dtype=np.int8)\n",
    "#for idx, decoder_length in enumerate(decoder_lengths_data):\n",
    "    #decoder_mask_data[idx, :(decoder_length)] = np.ones((1, decoder_length))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check so all the input data has the same number of examples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('summaries/train/',\n",
    "                                      sess.graph)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Learning rate_schedule\n",
    "\n",
    "# Epoch 0 - 2: Warmup with a lower learning rate: (1e-4)\n",
    "# Epoch 3 - 6: (5e-4)\n",
    "# Epoch 7 - 20: Exponentially decaying from (5e-4) to (1e-5)\n",
    "\n",
    "#num_epochs = 400\n",
    "\n",
    "#mini_batch_size = 128\n",
    "#num_samples = 0\n",
    "#steps_per_epoch = num_samples / mini_batch_size\n",
    "#steps_per_epoch = 1\n",
    "\n",
    "#def get_learning_rate(global_step, epoch):\n",
    "    \n",
    "    #base_learning_rate = 0.0005\n",
    "    #if epoch < 3:\n",
    "        #lr_rate = 0.0001\n",
    "   # elif epoch < 6:\n",
    "        #lr_rate = 0.0005\n",
    "    #else:\n",
    "        # Over 15 epochs decay learning rate from 0.0005 to 0.00001 <=> decay learning rate with .77 per epoch:\n",
    "        #decay_rate = 0.77\n",
    "        #decay_steps = steps_per_epoch\n",
    "        #lr_rate = base_learning_rate * decay_rate ^ ((global_step - steps_per_epoch * 6) / decay_steps)\n",
    "    \n",
    "    #return lr_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_loss():\n",
    "    num_val_batches = len(val_sequence_lengths_batches)\n",
    "    val_loss = 0    \n",
    "    \n",
    "    for i in range(num_val_batches):\n",
    "\n",
    "\n",
    "        input_data = {img: val_encoder_input_data_batches[i],\n",
    "                                        decoder_lengths: val_sequence_lengths_batches[i],\n",
    "                                         decoder_inputs: val_decoder_input_data_batches[i],\n",
    "                                          decoder_outputs: val_decoder_target_data_batches[i],\n",
    "                                         }\n",
    "   \n",
    "\n",
    "        output_tensors = [train_loss]\n",
    "\n",
    "        loss = sess.run(output_tensors, \n",
    "                               feed_dict=input_data)\n",
    "\n",
    "        \n",
    "\n",
    "        val_loss = val_loss + loss\n",
    "        \n",
    "\n",
    "    val_loss = val_loss / len(val_decoder_input_data)\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(\"Num batches: \", len(train_sequence_lengths_batches)) #(Note: they are not necessarily equal size towards the end (this will fix later))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Num batches: ', 15)\n",
      "('Epoch: ', 0)\n",
      "('i: ', 0)\n",
      "Step 16: loss = 114.26\n",
      "('Learning rate: ', 9.9999997e-05)\n",
      "('Global grad norm: ', 444.34158)\n",
      "('i: ', 1)\n",
      "Step 17: loss = 114.26\n",
      "('Learning rate: ', 9.9999997e-05)\n",
      "('Global grad norm: ', 318.50534)\n",
      "('i: ', 2)\n",
      "Step 18: loss = 114.26\n",
      "('Learning rate: ', 9.9999997e-05)\n",
      "('Global grad norm: ', 347.52213)\n"
     ]
    }
   ],
   "source": [
    "## TODO Implement checkpoints\n",
    "\n",
    "glob_step = 0\n",
    "num_epochs = 5\n",
    "val_losses = []\n",
    "num_train_batches = len(train_sequence_lengths_batches)\n",
    "print(\"Num batches: \", num_train_batches)\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i in range(num_train_batches):\n",
    "\n",
    "        input_data = {img: train_encoder_input_data_batches[i],\n",
    "                                        decoder_lengths: train_sequence_lengths_batches[i],\n",
    "                                         decoder_inputs: train_decoder_input_data_batches[i],\n",
    "                                          decoder_outputs: train_decoder_target_data_batches[i],\n",
    "                                         }\n",
    "        \n",
    "        \n",
    "        # Only catch important info in the beginning of the epoch\n",
    "        if (i == 0):\n",
    "            output_tensors = [merged, update_step,train_loss, optimizer._lr, global_norm, gradient_norms, global_step]\n",
    "            summary, _, loss, lr_rate, global_grad_norm, grad_norms, glob_step = sess.run(output_tensors, \n",
    "                               feed_dict=input_data)\n",
    "            train_writer.add_summary(summary, glob_step)\n",
    "            \n",
    "        else:\n",
    "            output_tensors = [update_step, train_loss, global_norm, global_step, optimizer._lr]\n",
    "            _, loss, global_grad_norm, glob_step, lr_rate  = sess.run(output_tensors, \n",
    "                               feed_dict=input_data)\n",
    "\n",
    "        \n",
    "\n",
    "        # Write to tensorboard every 10th step\n",
    "        if glob_step % 40 == 0:\n",
    "            train_writer.add_summary(summary, glob_step)\n",
    "\n",
    "        if i == 0:\n",
    "            print(\"Epoch: \", epoch)\n",
    "            \n",
    "\n",
    "        print(\"i: \", i)\n",
    "        #if glob_step % 5 == 0:\n",
    "        print('Step %d: loss = %.2f' % (glob_step, loss))\n",
    "        print(\"Learning rate: \", lr_rate)\n",
    "        print(\"Global grad norm: \", global_grad_norm)\n",
    "\n",
    "    val_loss = get_validation_loss()\n",
    "    val_losses.append(val_loss)\n",
    "    print(\"Val loss: \", val_loss)\n",
    "    \n",
    "## Run the following in terminal to get up tensorboard: tensorboard --logdir=summaries/train        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "saver.save(sess, 'my-test-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference\n",
    "\n",
    "tgt_sos_id = target_token_index['**start**'] # 4\n",
    "tgt_eos_id = target_token_index['**end**'] # 3\n",
    "\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "    embedding_decoder,\n",
    "    tf.fill([10], tgt_sos_id), tgt_eos_id)\n",
    "\n",
    "maximum_iterations = 15 # Do max seq_length or find some other heuristic\n",
    "\n",
    "# Decoder\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    decoder_cell, helper, decoder_initial_state,\n",
    "    output_layer=projection_layer)\n",
    "# Dynamic decoding\n",
    "outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder, maximum_iterations=maximum_iterations)\n",
    "translations = outputs.sample_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = sess.run(translations, feed_dict = {img: encoder_input_data[:20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trans\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_seq(int_sequence):\n",
    "    \n",
    "    output_string = \"\"\n",
    "    for value in int_sequence:\n",
    "        output_string += reverse_target_token_index[value]\n",
    "        if reverse_target_token_index[value] == \"**end**\":\n",
    "            break\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, seq in enumerate(decoder_target_data):\n",
    "    \n",
    "    print(\"Output: \")\n",
    "    print(get_token_seq(trans[idx]))\n",
    "    \n",
    "    print(\"Ground truth: \")\n",
    "    print(get_token_seq(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
