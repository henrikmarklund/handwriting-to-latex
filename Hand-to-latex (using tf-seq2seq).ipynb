{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from prepare_data import get_decoder_data_int_sequences\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensor2tensor.layers.common_attention import add_timing_signal_nd #currently not in use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Outline\n",
    "\n",
    "# 1. Encoder\n",
    "# 2. Decoder\n",
    "# 3. Optimization and training\n",
    "\n",
    "\n",
    "# What is bucketing?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "def get_max_shape(images):\n",
    "\n",
    "\tmax_height = 0\n",
    "\tmax_width = 0\n",
    "\tprint(\"Getting max shape\")\n",
    "\n",
    "\tfor image in images:\n",
    "\n",
    "\t\tif image.shape[0] > max_height:\n",
    "\t\t\tmax_height = image.shape[0]\n",
    "\n",
    "\t\tif image.shape[1] > max_width:\n",
    "\t\t\tmax_width = image.shape[1]\n",
    "\n",
    "\n",
    "\treturn [max_height, max_width]\n",
    "\n",
    "\n",
    "def normalize_images(images):\n",
    "\n",
    "\timages = images.astype(np.float32)\n",
    "\timages = np.multiply(images, 1.0 / 255.0)\n",
    "\n",
    "\treturn images\n",
    "\n",
    "def down_sample(images, factor): \n",
    "\ttarget_h = int(math.floor(float(images[0].shape[0]) * factor))\n",
    "\ttarget_w = int(math.floor(float(images[0].shape[1]) * factor))\n",
    "\tnum_images = len(images)\n",
    "\tdown_sampled_images = np.ones((num_images, target_h, target_w)) * 255\n",
    "\n",
    "\tfor idx, image in enumerate(images):\n",
    "\t\t\n",
    "\t\tim = image\n",
    "\n",
    "\t\t#Downsample\n",
    "\t\tim = cv2.resize(im, (0, 0), fx = factor, fy=factor, interpolation = cv2.INTER_AREA) #cv2.INTER_LINEAR\n",
    "\n",
    "\n",
    "\t\tdown_sampled_images[idx, :, :] = im\n",
    "\n",
    "\treturn down_sampled_images\n",
    "\n",
    "\n",
    "def down_sample_flexible(images, factor): \n",
    "    print(\"downsampling images\")\n",
    "    new_images = []\n",
    "\n",
    "    for image in images:\n",
    "        new_image = cv2.resize(image, (0, 0), fx = factor, fy=factor, interpolation = cv2.INTER_AREA) #cv2.INTER_LINEAR\n",
    "        new_images.append(new_image)\n",
    "\n",
    "    return new_images\n",
    "\n",
    "\n",
    "def pad_images(images, target_shape = None):\n",
    "\tprint(images[0])\n",
    "\n",
    "\tif (target_shape == None):\n",
    "\t\tmax_height, max_width = get_max_shape(images)\n",
    "\n",
    "\tnum_images = len(images)\n",
    "\n",
    "\tpadded_images = np.ones((num_images, max_height, max_width)) * 255\n",
    "\n",
    "\tfor idx, image in enumerate(images):\n",
    "\n",
    "\t\th = image.shape[0]\n",
    "\t\tw = image.shape[1]\n",
    "\n",
    "\t\tpadded_images[idx, :h, :w] = image\n",
    "\n",
    "\n",
    "\treturn padded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(dataset):\n",
    "    if dataset == \"small\":\n",
    "        vocab = [line for line in open('data/tin/tiny_vocab.txt')]\n",
    "    elif dataset == \"test\":\n",
    "        vocab = [line for line in open('data/vocab.txt')]\n",
    "    elif dataset == \"train\":\n",
    "        vocab = [line for line in open('data/vocab.txt')]\n",
    "\n",
    "    vocab = [x.strip('\\n') for x in vocab]\n",
    "    return vocab\n",
    "\n",
    "def load_raw_data(dataset, max_token_length = 400, max_image_size = (60, 200), max_num_samples = 5000):\n",
    "    decoder_lengths_data = []\n",
    "    \n",
    "    token_vocabulary = []\n",
    "    token_sequences = []\n",
    "    images = []\n",
    "    \n",
    "    if dataset == \"small\":\n",
    "        image_folder = 'data/tin/tiny/'\n",
    "        formula_file_path = \"data/tin/tiny.formulas.norm.txt\"\n",
    "    elif dataset == \"test\":\n",
    "        image_folder = 'data/images_test/'\n",
    "        formula_file_path = \"data/test.formulas.norm.txt\"\n",
    "    elif dataset == \"train\":\n",
    "        image_folder = 'data/images_train/'\n",
    "        formula_file_path = \"data/train.formulas.norm.txt\"\n",
    "\n",
    "\n",
    "        \n",
    "    included_counter = 0\n",
    "    examples_counter = 0\n",
    "    with open (formula_file_path, \"r\") as myfile:\n",
    "\n",
    "        for idx, token_sequence in enumerate(myfile):\n",
    "            examples_counter += 1\n",
    "            #Check token size:\n",
    "            token_sequence = token_sequence.rstrip('\\n')\n",
    "            tokens = token_sequence.split()\n",
    "\n",
    "            file_name = str(idx) + '.png'\n",
    "            image = cv2.imread(image_folder + file_name, 0)\n",
    "            \n",
    "            if image is None:\n",
    "                print(\"Id:\", idx)\n",
    "                continue\n",
    "            \n",
    "            #print(tokens)\n",
    "            if len(tokens) <= max_token_length and image.shape[0] <= max_image_size[0] and image.shape[1] <= max_image_size[1]:\n",
    "                token_sequences.append('**start** ' + token_sequence + ' **end**')\n",
    "                #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #Grey scale\n",
    "                #print(image)\n",
    "                decoder_lengths_data.append(len(token_sequence))\n",
    "                images.append(image)\n",
    "                for token in tokens:\n",
    "                    if token not in token_vocabulary:\n",
    "                        token_vocabulary.append(token)\n",
    "\n",
    "                included_counter += 1\n",
    "                if included_counter == max_num_samples:\n",
    "                    break\n",
    "\n",
    "        \n",
    "    token_vocabulary.append(\"**start**\")\n",
    "    token_vocabulary.append(\"**end**\")\n",
    "    \n",
    "    return images, token_sequences, token_vocabulary, decoder_lengths_data\n",
    "\n",
    "\n",
    "def preprocess_images(images):\n",
    "\n",
    "    down_sample_factor = 0.7\n",
    "    encoder_input = down_sample_flexible(images, 0.6)\n",
    "    encoder_input = pad_images(encoder_input)\n",
    "    encoder_input = normalize_images(encoder_input)\n",
    "\n",
    "    # Add dimension for TensorFlow Conv Layers to work properly as it needs (None, Height, Width, 1)\n",
    "    encoder_input = encoder_input.reshape(encoder_input.shape[0], encoder_input.shape[1], encoder_input.shape[2], 1)\n",
    "\n",
    "    return encoder_input\n",
    "\n",
    "\n",
    "def load_data(dataset, max_token_length, max_image_size, max_num_samples):\n",
    "\n",
    "    if (dataset == \"small\"):\n",
    "        images, token_sequences, token_vocabulary, decoder_lengths_data = load_raw_data(dataset=\"small\", max_num_samples=max_num_samples)\n",
    "        images = preprocess_images(images)\n",
    "    elif (dataset == \"test\"):\n",
    "        images, token_sequences, token_vocabulary, decoder_lengths_data = load_raw_data(dataset=\"test\",  max_token_length = max_token_length, max_image_size = max_image_size, max_num_samples=max_num_samples)\n",
    "        images = preprocess_images(images)\n",
    "    elif (dataset == \"train\"):\n",
    "        images, token_sequences, token_vocabulary, decoder_lengths_data = load_raw_data(dataset=\"train\",  max_token_length = max_token_length, max_image_size = max_image_size, max_num_samples=max_num_samples)\n",
    "        images = preprocess_images(images)\n",
    "    return images, token_sequences, token_vocabulary, decoder_lengths_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1043b0e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Make a histogram of how long the sequences.\n",
    "## Helps us decide on a cut off point\n",
    "\n",
    "formula_file_path = \"data/train.formulas.norm.txt\"\n",
    "\n",
    "formula_lengths = []\n",
    "\n",
    "with open (formula_file_path, \"r\") as myfile:\n",
    "    for idx, token_sequence in enumerate(myfile):\n",
    "        tokens = token_sequence.split()\n",
    "        formula_lengths.append(len(tokens))\n",
    "\n",
    "%matplotlib inline\n",
    "plt.hist(formula_lengths, normed=False, bins=40)\n",
    "plt.ylabel('Num of expressions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downsampling images\n",
      "[[255 255 255 ..., 255 255 255]\n",
      " [255 255 255 ..., 255 255 255]\n",
      " [255 255 255 ..., 255 255 255]\n",
      " ..., \n",
      " [255 255 255 ..., 255 255 255]\n",
      " [255 255 255 ..., 255 255 255]\n",
      " [255 255 255 ..., 255 255 255]]\n",
      "Getting max shape\n",
      "Maximum output sequence lenght: 63\n",
      "Examples of sequences: \n",
      "Ex. 1: **start** \\widetilde \\gamma _ { \\mathrm { h o p f } } \\simeq \\sum _ { n > 0 } \\widetilde { G } _ { n } { \\frac { ( - a ) ^ { n } } { 2 ^ { 2 n - 1 } } } **end**\n",
      "Ex. 1: **start** ( { \\cal L } _ { a } g ) _ { i j } = 0 , \\ \\ \\ \\ ( { \\cal L } _ { a } H ) _ { i j k } = 0 , **end**\n",
      "\n",
      "Number of examples: 10\n",
      "Number of tokens in our vocabulary: 74\n",
      "5 example of tokens: ['\\\\widetilde', '\\\\gamma', '_', '{', '\\\\mathrm']\n",
      "\n",
      "Example pairs (token, index) in dictionary: \n",
      "('\\\\Phi', 66)\n",
      "('\\\\frac', 16)\n",
      "('\\\\widetilde', 0)\n",
      "('\\\\gamma', 1)\n",
      "('\\\\sqrt', 38)\n",
      "('**end**', 73)\n",
      "('\\\\partial', 54)\n",
      "('B', 52)\n",
      "('\\\\nu', 57)\n",
      "('.', 53)\n",
      "('\\\\alpha', 64)\n",
      "('\\\\phi', 50)\n"
     ]
    }
   ],
   "source": [
    "## Load and process data (takes a up to 10 minutes)\n",
    "\n",
    "# max token length: 50\n",
    "# max_image_size: (42, 200)\n",
    "\n",
    "encoder_input_data, target_texts, token_vocabulary, decoder_lengths_data = load_data(dataset=\"train\", \n",
    "                                                               max_token_length = 70,\n",
    "                                                               max_image_size = (60, 270),\n",
    "                                                               max_num_samples=10)\n",
    "\n",
    "target_tokens = token_vocabulary\n",
    "\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "\n",
    "max_decoder_seq_length = max([len(txt.split()) for txt in target_texts])\n",
    "\n",
    "target_token_index = dict(\n",
    "    [(token, i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "reverse_target_token_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items()) ## Will be used in the inference model\n",
    "\n",
    "print(\"Maximum output sequence lenght: \" + str(max_decoder_seq_length))\n",
    "print(\"Examples of sequences: \")\n",
    "print(\"Ex. 1: \" + str(target_texts[0]))\n",
    "print(\"Ex. 1: \" + str(target_texts[1]) + \"\\n\")\n",
    "\n",
    "print(\"Number of examples: \" + str(len(encoder_input_data)))\n",
    "\n",
    "\n",
    "print(\"Number of tokens in our vocabulary: \" + str(num_decoder_tokens))\n",
    "print(\"5 example of tokens: \" + str(target_tokens[0:5]) + \"\\n\")\n",
    "\n",
    "print(\"Example pairs (token, index) in dictionary: \")\n",
    "\n",
    "for i, key in enumerate(target_token_index):\n",
    "    print(key, target_token_index[key])\n",
    "    if i > 10:\n",
    "        break\n",
    "\n",
    "_, image_h, image_w, _  = encoder_input_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#decoder_input_data, decoder_target_data = get_decoder_data_int_sequences(target_texts,\n",
    "                                                                        #target_tokens,\n",
    "                                                                        #num_decoder_tokens,\n",
    "                                                                        #max_decoder_seq_length,\n",
    "                                                                        #target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data \n",
    "\n",
    "\n",
    "\n",
    "#num_samples = 128\n",
    "#data_path = 'swe-eng/swe.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "#eng_texts = []\n",
    "\n",
    "#eng_characters = set()\n",
    "\n",
    "#decoder_lengths_data = []\n",
    "\n",
    "#lines = open(data_path).read().split('\\n')\n",
    "#for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    #eng_text, swe_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    #eng_text = '\\t' + eng_text + '\\n'\n",
    "    #eng_texts.append(eng_text)\n",
    "    \n",
    "    #decoder_lengths_data.append(len(eng_text))\n",
    "    #for char in eng_text:\n",
    "        #if char not in eng_characters:\n",
    "            #eng_characters.add(char)\n",
    "            \n",
    "            \n",
    "\n",
    "decoder_lengths_data = np.array(decoder_lengths_data)            \n",
    "#random.shuffle(eng_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image from text\n",
    "\n",
    "\n",
    "# Draw (Bitmap Font) Text to Image\n",
    "#from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "#def reverseColor(r, g, b):\n",
    "#    return (255 - r, 255 - g, 255 - b)\n",
    "#def grayscaleColor(r, g, b):\n",
    "    #a = (r + g + b) / 3\n",
    "    #return (a, a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images = []\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "#for idx, eng_text in enumerate(eng_texts):\n",
    "    #img = Image.new('L', (50, 20), 'white')\n",
    "    #d = ImageDraw.Draw(img)\n",
    "    #text = eng_text.strip('\\n')\n",
    "    #text = text.strip('\\t')\n",
    "    #d.text((4, 4), text, fill=(0))\n",
    "    #images.append(np.asarray(img))\n",
    "    #if idx < 16:\n",
    "        #plt.imshow(np.asarray(img), cmap='gray')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA: Images \n",
    "\n",
    "#encoder_input_data = np.array(images)\n",
    "\n",
    "#encoder_input_data = encoder_input_data.reshape((encoder_input_data.shape[0], encoder_input_data.shape[1], encoder_input_data.shape[2], 1))\n",
    "\n",
    "#encoder_input_data_non_normalized = encoder_input_data\n",
    "\n",
    "#encoder_input_data = encoder_input_data.astype('float32')\n",
    "#encoder_input_data = encoder_input_data / 255.0\n",
    "#print(encoder_input_data.shape)\n",
    "\n",
    "_, image_h, image_w, _ = encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of unique output tokens:', 74)\n",
      "('Max sequence length for outputs:', 167)\n",
      "**start** \\widetilde \\gamma _ { \\mathrm { h o p f } } \\simeq \\sum _ { n > 0 } \\widetilde { G } _ { n } { \\frac { ( - a ) ^ { n } } { 2 ^ { 2 n - 1 } } } **end**\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# DATA: Token sequences\n",
    "\n",
    "token_vocabulary = sorted(list(token_vocabulary))\n",
    "num_decoder_tokens = len(token_vocabulary)\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(token_vocabulary)])\n",
    "\n",
    "\n",
    "reverse_target_token_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items()) ## Will be used in the inference model\n",
    "\n",
    "\n",
    "print(target_texts[0])\n",
    "print(len(token_vocabulary[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16\n",
    "#'Number of unique output tokens:', 31)\n",
    "#('Max sequence length for outputs:', 9)\n",
    "\n",
    "# 17\n",
    "#('Number of unique output tokens:', 32)\n",
    "#('Max sequence length for outputs:', 10)\n",
    "\n",
    "#18\n",
    "\n",
    "#('Number of unique output tokens:', 34)\n",
    "#('Max sequence length for outputs:', 10)\n",
    "#\tRun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder_input_data = np.zeros(\n",
    "        (len(target_texts), max_decoder_seq_length),\n",
    "        dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "        (len(target_texts), max_decoder_seq_length),\n",
    "        dtype='float32')\n",
    "#decoder_target_data = np.zeros(\n",
    "        #(len(eng_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        #dtype='float32')\n",
    "\n",
    "num_other = 0\n",
    "\n",
    "\n",
    "for i, target_text in enumerate(target_texts):\n",
    "    for t, token in enumerate(target_text.split()):\n",
    "\n",
    "        if token in target_token_index:\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = target_token_index[token]\n",
    "\n",
    "            if t > 0:\n",
    "                #decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1] = target_token_index[token]\n",
    "                \n",
    "        else:\n",
    "            print(token)\n",
    "            num_other = num_other + 1\n",
    "            decoder_input_data[i, t] = target_token_index['other']\n",
    "\n",
    "            if t > 0:\n",
    "                #decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                \n",
    "                decoder_target_data[i, t - 1] = target_token_index['other']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABoCAYAAADhAAsHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0J0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12556b410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.  51.  36. ...,   0.   0.   0.]\n",
      " [  4.   0.  72. ...,   0.   0.   0.]\n",
      " [  4.  25.  53. ...,   0.   0.   0.]\n",
      " ..., \n",
      " [  4.  22.  53. ...,   0.   0.   0.]\n",
      " [  4.  38.  17. ...,   0.   0.   0.]\n",
      " [  4.  72.  35. ...,   0.   0.   0.]]\n",
      "\n",
      "[ 51.  36.  53.  72.  39.  72.  59.  65.  66.  57.  73.  73.  47.  49.  53.\n",
      "  72.  64.  15.   9.  73.  51.  72.  19.  73.  53.  72.  64.  73.  72.  35.\n",
      "  72.   0.   7.  54.   1.  52.  72.  64.  73.  73.  72.  11.  52.  72.  11.\n",
      "  64.   7.  10.  73.  73.  73.   3.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "#encoder_input_data.shape\n",
    "\n",
    "plt.imshow(encoder_input_data[0].squeeze(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(decoder_input_data)\n",
    "\n",
    "print(\"\")\n",
    "print(decoder_target_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "img_height = encoder_input_data[0].shape[0]\n",
    "img_width = encoder_input_data[0].shape[1]\n",
    "\n",
    "print(img_height)\n",
    "print(img_width)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder\n",
    "\n",
    "## One of Genthails's encoder implementations (on github not in paper)\n",
    "\n",
    "img_height = encoder_input_data[0].shape[0]\n",
    "img_width = encoder_input_data[0].shape[1]\n",
    "\n",
    "\n",
    "img = tf.placeholder(tf.float32, [None, img_height, img_width, 1], name='img')\n",
    "\n",
    "\n",
    "#batch_size = tf.placeholder(tf.float32, [1], name='batch_size')    \n",
    "batch_size = tf.shape(img)[0]\n",
    "\n",
    "# Conv + max pooling\n",
    "out = tf.layers.conv2d(img, 64, 3, 1, \"SAME\", activation=tf.nn.relu)\n",
    "out = tf.layers.max_pooling2d(out, 2, 2, \"SAME\")\n",
    "# Conv + max pooling\n",
    "out = tf.layers.conv2d(out, 128, 3, 1, \"SAME\", activation=tf.nn.relu)\n",
    "   \n",
    "out = tf.layers.batch_normalization(out)\n",
    "  \n",
    "\n",
    "# Conv\n",
    "out = tf.layers.conv2d(out, 256, 3, 1, \"SAME\", activation=tf.nn.relu) # regular conv -> id\n",
    "\n",
    "# Conv + max pooling\n",
    "out = tf.layers.conv2d(out, 256, 3, 1, \"SAME\", activation=tf.nn.relu)\n",
    "out = tf.layers.max_pooling2d(out, (2, 2), (2, 2), \"SAME\")\n",
    "\n",
    "#out = tf.layers.batch_normalization(out)\n",
    "\n",
    "\n",
    "# Conv + max pooling\n",
    "out = tf.layers.conv2d(out, 512, 3, 1, \"SAME\", activation=tf.nn.relu)\n",
    "out = tf.layers.max_pooling2d(out, (2, 2), (2, 2), \"SAME\")\n",
    "\n",
    "\n",
    "\n",
    "# Conv valid\n",
    "out = tf.layers.conv2d(out, 512, 3, 1, \"VALID\", activation=tf.nn.relu, name=\"last_conv_layer\") # conv\n",
    "out = tf.layers.batch_normalization(out)\n",
    "\n",
    "## Out is now a H'*W' encoding of the image\n",
    "\n",
    "## We want to turn this into a sequence of vectors: (e1, e2 ... en)\n",
    "H= out.shape[1]\n",
    "W= out.shape[2] \n",
    "\n",
    "out = add_timing_signal_nd(out)\n",
    "seq = tf.reshape(tensor=out, shape=[-1, int(H*W), 512])\n",
    "\n",
    "# TODO: Add positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 512)\n",
      "2\n",
      "16\n",
      "30\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_height = encoder_input_data[0].shape[0]\n",
    "img_width = encoder_input_data[0].shape[1]\n",
    "\n",
    "print(seq.shape)\n",
    "\n",
    "l = seq.shape[1].value\n",
    "\n",
    "print(H)\n",
    "print(W)\n",
    "\n",
    "print(img_height)\n",
    "print(img_width)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First state of the decoder consists of two vectors, the hidden state (h0) and the memory (c0).\n",
    "# Usually the hidden state refers to [h0, c0]. So a little bit of overloading of hidden state (I think)\n",
    "# This is how Genthail implements it\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "num_units = 512\n",
    "\n",
    "\n",
    "img_mean = tf.reduce_mean(seq, axis=1)\n",
    "\n",
    "img_mean = tf.layers.batch_normalization(img_mean)\n",
    "\n",
    "W = tf.get_variable(\"W\", shape=[512, num_units])\n",
    "b = tf.get_variable(\"b\", shape=[num_units])\n",
    "h0 = tf.tanh(tf.matmul(img_mean, W) + b)\n",
    "\n",
    "W_ = tf.get_variable(\"W_\", shape=[512, num_units])\n",
    "b_ = tf.get_variable(\"b_\", shape=[num_units])\n",
    "c0 = tf.tanh(tf.matmul(img_mean, W_) + b_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoder_state = tf.contrib.rnn.LSTMStateTuple(c0, h0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention_states: [batch_size, max_time, num_units]\n",
    "attention_states = seq\n",
    "\n",
    "\n",
    "attention_depth = num_units\n",
    "\n",
    "# Create an attention mechanism\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "    attention_depth, attention_states) ## Todo: Add sequence length. Check tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 512)\n",
      "(?, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(c0.shape)\n",
    "print(h0.shape)\n",
    "\n",
    "#encoder_state.shape\n",
    "max_decoder_seq_length\n",
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Token vocab size: ', 74)\n",
      "('Embedding size: ', 80)\n",
      "(?, 167, 80)\n"
     ]
    }
   ],
   "source": [
    "# Decoder: from seq2seq tutorial \n",
    "\n",
    "\n",
    "\n",
    "token_vocab_size = num_decoder_tokens # Number of tokens in our vocabulary\n",
    "\n",
    "embedding_size = 80 # In Genthail's paper he says he has 80 embeddings which I believe corresponds to embedding_size\n",
    "\n",
    "decoder_inputs = tf.placeholder(tf.int32, [None, max_decoder_seq_length], name='decoder_inputs')  # Supposed to be a sequence of numbers corresponding to the different tokens in the sentence\n",
    "\n",
    "# Embedding of target tokens\n",
    "\n",
    "# Embedding matrix \n",
    "embedding_decoder = tf.get_variable(\n",
    "    \"embedding_encoder\", [token_vocab_size, embedding_size], tf.float32, initializer=tf.orthogonal_initializer) #  tf.float32 was default in the NMT tutorial\n",
    "\n",
    "print(\"Token vocab size: \", token_vocab_size)\n",
    "print(\"Embedding size: \", embedding_size)\n",
    "\n",
    "\n",
    "# Look up embedding:\n",
    "#   decoder_inputs: [max_time, batch_size]\n",
    "#   decoder_emb_inp: [max_time, batch_size, embedding_size]\n",
    "decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "    embedding_decoder, decoder_inputs)\n",
    "\n",
    "print(decoder_emb_inp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build RNN cell\n",
    "decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "\n",
    "attention = True\n",
    "if attention:\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        decoder_cell, attention_mechanism,\n",
    "        attention_layer_size=512)\n",
    "\n",
    "    ## Set initial state of decoder to zero (possible to use previous state)\n",
    "\n",
    "    use_encoder_state = True\n",
    "    if use_encoder_state:\n",
    "        decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n",
    "    else:\n",
    "        decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "else:\n",
    "    decoder_initial_state = encoder_state\n",
    "\n",
    "    \n",
    "decoder_lengths = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    decoder_emb_inp, decoder_lengths, time_major=False)\n",
    "\n",
    "# Projection layer\n",
    "projection_layer = layers_core.Dense(token_vocab_size, use_bias=False, name=\"output_projection\")# Said layers_core before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decoder\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    decoder_cell, helper, decoder_initial_state,\n",
    "    output_layer=projection_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0-dev20171121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'Tanh_1:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'Tanh:0' shape=(?, 512) dtype=float32>), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(?, 512) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(?, 32) dtype=float32>, alignment_history=())"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "decoder_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 74)\n"
     ]
    }
   ],
   "source": [
    "# Dynamic decoding\n",
    "outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=False)  ## Understand parameter Impute finished\n",
    "logits = outputs.rnn_output\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_weights = tf.placeholder(tf.float32, [None, None], name='target_weights')\n",
    "\n",
    "\n",
    "# Supposed to be a sequence of numbers corresponding to the different tokens in the sentence\n",
    "decoder_outputs = tf.placeholder(tf.int32, [None, None], name='decoder_outputs') \n",
    "\n",
    "max_gradient_norm = 10\n",
    "#learning_rate = 0.001\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.0001\n",
    "#learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           #5, 0.90, staircase=False)\n",
    "\n",
    "boundaries = [30, 60, 90, 120, 150, 180, 210, 240]    \n",
    "values = [0.00005, 0.00005, 0.00005, 0.00004, 0.00003, 0.00002, 0.00001, 0.00001, 0.00001]\n",
    "    \n",
    "learning_rate =  tf.train.piecewise_constant(global_step, boundaries, values, name=None)\n",
    "    \n",
    "    \n",
    "# Loss function\n",
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=decoder_outputs, logits=logits)\n",
    "train_loss = tf.divide(tf.reduce_sum(crossent * target_weights), tf.cast(batch_size, tf.float32)) \n",
    "\n",
    "\n",
    "tf.summary.scalar('loss', train_loss)\n",
    "\n",
    "# Calculate and clip gradients\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(train_loss, params)\n",
    "\n",
    "#gradient_global_norm = tf.norm(gradients)\n",
    "\n",
    "\n",
    "clipped_gradients, global_norm = tf.clip_by_global_norm(\n",
    "    gradients, max_gradient_norm)\n",
    "\n",
    "tf.summary.scalar('global_norm', global_norm)\n",
    "\n",
    "\n",
    "# Optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "update_step = optimizer.apply_gradients(\n",
    "    zip(clipped_gradients, params), global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv2d/kernel:0/gradient is illegal; using conv2d/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0/gradient is illegal; using conv2d/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/kernel:0/gradient is illegal; using conv2d_1/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/bias:0/gradient is illegal; using conv2d_1/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization/gamma:0/gradient is illegal; using batch_normalization/gamma_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization/beta:0/gradient is illegal; using batch_normalization/beta_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0/gradient is illegal; using conv2d_2/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0/gradient is illegal; using conv2d_2/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/kernel:0/gradient is illegal; using conv2d_3/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/bias:0/gradient is illegal; using conv2d_3/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/kernel:0/gradient is illegal; using conv2d_4/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/bias:0/gradient is illegal; using conv2d_4/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/kernel:0/gradient is illegal; using last_conv_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/bias:0/gradient is illegal; using last_conv_layer/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_1/gamma:0/gradient is illegal; using batch_normalization_1/gamma_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_1/beta:0/gradient is illegal; using batch_normalization_1/beta_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/gamma:0/gradient is illegal; using batch_normalization_2/gamma_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/beta:0/gradient is illegal; using batch_normalization_2/beta_0/gradient instead.\n",
      "INFO:tensorflow:Summary name W:0/gradient is illegal; using W_0/gradient instead.\n",
      "INFO:tensorflow:Summary name b:0/gradient is illegal; using b_0/gradient instead.\n",
      "INFO:tensorflow:Summary name W_:0/gradient is illegal; using W__0/gradient instead.\n",
      "INFO:tensorflow:Summary name b_:0/gradient is illegal; using b__0/gradient instead.\n",
      "INFO:tensorflow:Summary name memory_layer/kernel:0/gradient is illegal; using memory_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name embedding_encoder:0/gradient is illegal; using embedding_encoder_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/kernel:0/gradient is illegal; using decoder/attention_wrapper/basic_lstm_cell/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/bias:0/gradient is illegal; using decoder/attention_wrapper/basic_lstm_cell/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/attention_layer/kernel:0/gradient is illegal; using decoder/attention_wrapper/attention_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/output_projection/kernel:0/gradient is illegal; using decoder/output_projection/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d/kernel:0/weight is illegal; using conv2d/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0/weight is illegal; using conv2d/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/kernel:0/weight is illegal; using conv2d_1/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/bias:0/weight is illegal; using conv2d_1/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization/gamma:0/weight is illegal; using batch_normalization/gamma_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization/beta:0/weight is illegal; using batch_normalization/beta_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0/weight is illegal; using conv2d_2/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0/weight is illegal; using conv2d_2/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/kernel:0/weight is illegal; using conv2d_3/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/bias:0/weight is illegal; using conv2d_3/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/kernel:0/weight is illegal; using conv2d_4/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/bias:0/weight is illegal; using conv2d_4/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/kernel:0/weight is illegal; using last_conv_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/bias:0/weight is illegal; using last_conv_layer/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_1/gamma:0/weight is illegal; using batch_normalization_1/gamma_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_1/beta:0/weight is illegal; using batch_normalization_1/beta_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/gamma:0/weight is illegal; using batch_normalization_2/gamma_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/beta:0/weight is illegal; using batch_normalization_2/beta_0/weight instead.\n",
      "INFO:tensorflow:Summary name W:0/weight is illegal; using W_0/weight instead.\n",
      "INFO:tensorflow:Summary name b:0/weight is illegal; using b_0/weight instead.\n",
      "INFO:tensorflow:Summary name W_:0/weight is illegal; using W__0/weight instead.\n",
      "INFO:tensorflow:Summary name b_:0/weight is illegal; using b__0/weight instead.\n",
      "INFO:tensorflow:Summary name memory_layer/kernel:0/weight is illegal; using memory_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name embedding_encoder:0/weight is illegal; using embedding_encoder_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/kernel:0/weight is illegal; using decoder/attention_wrapper/basic_lstm_cell/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/basic_lstm_cell/bias:0/weight is illegal; using decoder/attention_wrapper/basic_lstm_cell/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/attention_layer/kernel:0/weight is illegal; using decoder/attention_wrapper/attention_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/output_projection/kernel:0/weight is illegal; using decoder/output_projection/kernel_0/weight instead.\n"
     ]
    }
   ],
   "source": [
    "param_names = [v.name for v in params]\n",
    "\n",
    "gradient_names = [g.name for g in gradients]\n",
    "\n",
    "\n",
    "gradient_norms = [tf.norm(gradient) for gradient in gradients]\n",
    "\n",
    "#print(len(param_names))\n",
    "#print(len(gradient_names))\n",
    "\n",
    "grads = list(zip(gradients, params))\n",
    "\n",
    "\n",
    "\n",
    "for grad, var in grads:\n",
    "    tf.summary.histogram(var.name + '/gradient', grad)\n",
    "# Merge all summaries into a single op\n",
    "#merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "for param in params:\n",
    "    to_summary = tf.summary.histogram(param.name + '/weight', param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the mask\n",
    "\n",
    "decoder_mask_data = np.zeros(shape=(decoder_target_data.shape))\n",
    "for idx, decoder_length in enumerate(decoder_lengths_data):\n",
    "    decoder_mask_data[idx, :decoder_length] = np.ones((1, decoder_length))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_mask_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Encoder input data shape: ', (10, 30, 144, 1))\n",
      "('Decoder input data shape: ', (10, 167))\n",
      "('Decoder target data shape: ', (10, 167))\n",
      "('Decoder lengths data shape: ', (10,))\n",
      "('Decoder mask data shape: ', (10, 167))\n",
      "('Encoder input data shape: ', (10, 30, 144, 1))\n",
      "('Decoder input data shape: ', (10, 167))\n",
      "('Decoder target data shape: ', (10, 167))\n",
      "('Decoder lengths data shape: ', (10,))\n",
      "('Decoder mask data shape: ', (10, 167))\n"
     ]
    }
   ],
   "source": [
    "# Check so all the input data has the same number of examples\n",
    "\n",
    "assert (encoder_input_data.shape[0] == decoder_input_data.shape[0])\n",
    "\n",
    "assert (decoder_input_data.shape[0] == decoder_target_data.shape[0])\n",
    "\n",
    "assert (decoder_target_data.shape[0] == decoder_lengths_data.shape[0])\n",
    "\n",
    "assert (decoder_lengths_data.shape[0] == decoder_mask_data.shape[0])\n",
    "\n",
    "\n",
    "print(\"Encoder input data shape: \", encoder_input_data.shape)\n",
    "\n",
    "print(\"Decoder input data shape: \", decoder_input_data.shape)\n",
    "\n",
    "print(\"Decoder target data shape: \", decoder_target_data.shape)\n",
    "\n",
    "print(\"Decoder lengths data shape: \", decoder_lengths_data.shape)\n",
    "\n",
    "print(\"Decoder mask data shape: \", decoder_mask_data.shape)\n",
    "\n",
    "\n",
    "assert (encoder_input_data[:12].shape[0] == decoder_input_data[:12].shape[0])\n",
    "\n",
    "assert (decoder_input_data[:12].shape[0] == decoder_target_data[:12].shape[0])\n",
    "\n",
    "assert (decoder_target_data[:12].shape[0] == decoder_lengths_data[:12].shape[0])\n",
    "\n",
    "assert (decoder_lengths_data[:12].shape[0] == decoder_mask_data[:12].shape[0])\n",
    "\n",
    "\n",
    "print(\"Encoder input data shape: \", encoder_input_data[:12].shape)\n",
    "\n",
    "print(\"Decoder input data shape: \", decoder_input_data[:12].shape)\n",
    "\n",
    "print(\"Decoder target data shape: \", decoder_target_data[:12].shape)\n",
    "\n",
    "print(\"Decoder lengths data shape: \", decoder_lengths_data[:12].shape)\n",
    "\n",
    "print(\"Decoder mask data shape: \", decoder_mask_data[:12].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('summaries/train/',\n",
    "                                      sess.graph)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: ', 0)\n",
      "Step 1: loss = 392.97\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 1905.6798)\n",
      "('Epoch: ', 1)\n",
      "Step 2: loss = 333.35\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 1694.339)\n",
      "('Epoch: ', 2)\n",
      "Step 3: loss = 284.59\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 1469.3441)\n",
      "('Epoch: ', 3)\n",
      "Step 4: loss = 245.29\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 1101.22)\n",
      "('Epoch: ', 4)\n",
      "Step 5: loss = 219.88\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 570.57764)\n",
      "('Epoch: ', 5)\n",
      "Step 6: loss = 211.56\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 276.49408)\n",
      "('Epoch: ', 6)\n",
      "Step 7: loss = 212.34\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 494.71344)\n",
      "('Epoch: ', 7)\n",
      "Step 8: loss = 212.41\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 604.15051)\n",
      "('Epoch: ', 8)\n",
      "Step 9: loss = 209.20\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 614.67566)\n",
      "('Epoch: ', 9)\n",
      "Step 10: loss = 203.32\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 551.03021)\n",
      "('Epoch: ', 10)\n",
      "Step 11: loss = 196.38\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 414.64548)\n",
      "('Epoch: ', 11)\n",
      "Step 12: loss = 190.94\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 203.98351)\n",
      "('Epoch: ', 12)\n",
      "Step 13: loss = 189.14\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 415.13141)\n",
      "('Epoch: ', 13)\n",
      "Step 14: loss = 188.79\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 368.63657)\n",
      "('Epoch: ', 14)\n",
      "Step 15: loss = 188.26\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 455.14325)\n",
      "('Epoch: ', 15)\n",
      "Step 16: loss = 185.64\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 466.97397)\n",
      "('Epoch: ', 16)\n",
      "Step 17: loss = 181.10\n",
      "('Learning rate: ', 4.9999999e-05)\n",
      "('Global grad norm: ', 429.57999)\n"
     ]
    }
   ],
   "source": [
    "## Minibatch implementation insipred by: https://wiseodd.github.io/techblog/2016/06/21/nn-sgd/\n",
    "\n",
    "#for step in num_steps:\n",
    "\n",
    "num_epochs = 100\n",
    "mini_batch_size = 128\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i in range(0, decoder_input_data.shape[0], mini_batch_size):\n",
    "\n",
    "        max_length = max(decoder_lengths_data[i:i+mini_batch_size])\n",
    "\n",
    "        input_data = {img: encoder_input_data[i:i+mini_batch_size],\n",
    "                                        decoder_lengths: decoder_lengths_data[i:i+mini_batch_size],\n",
    "                                         decoder_inputs: decoder_input_data[i:i+mini_batch_size],\n",
    "                                          target_weights: decoder_mask_data[i:i+mini_batch_size, :max_length],\n",
    "                                          decoder_outputs: decoder_target_data[i:i+mini_batch_size, :max_length]\n",
    "                                         }\n",
    "\n",
    "        output_tensors = [merged, update_step,train_loss, optimizer._lr, global_norm, gradient_norms, global_step]\n",
    "\n",
    "        summary, _, loss, lr_rate, global_grad_norm, grad_norms, glob_step = sess.run(output_tensors, \n",
    "                               feed_dict=input_data)\n",
    "\n",
    "        train_writer.add_summary(summary, glob_step)\n",
    "\n",
    "        if i == 0:\n",
    "            print(\"Epoch: \", epoch)\n",
    "\n",
    "        #if glob_step % 5 == 0:\n",
    "        print('Step %d: loss = %.2f' % (glob_step, loss))\n",
    "        print(\"Learning rate: \", lr_rate)\n",
    "        print(\"Global grad norm: \", global_grad_norm)\n",
    "\n",
    "        \n",
    "        \n",
    "## Run the following in terminal to get up tensorboard: tensorboard --logdir=summaries/train        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference\n",
    "\n",
    "tgt_sos_id = target_token_index['**start**'] # 4\n",
    "tgt_eos_id = target_token_index['**end**'] # 3\n",
    "\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "    embedding_decoder,\n",
    "    tf.fill([20], tgt_sos_id), tgt_eos_id)\n",
    "\n",
    "maximum_iterations = 15 # Do max seq_length or find some other heuristic\n",
    "\n",
    "# Decoder\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    decoder_cell, helper, decoder_initial_state,\n",
    "    output_layer=projection_layer)\n",
    "# Dynamic decoding\n",
    "outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder, maximum_iterations=maximum_iterations)\n",
    "translations = outputs.sample_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = sess.run(translations, feed_dict = {img: encoder_input_data[:20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trans\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_seq(int_sequence):\n",
    "    \n",
    "    output_string = \"\"\n",
    "    for value in int_sequence:\n",
    "        output_string += reverse_target_token_index[value]\n",
    "    \n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, seq in enumerate(decoder_target_data):\n",
    "    \n",
    "    print(\"Output: \")\n",
    "    print(get_token_seq(trans[idx]))\n",
    "    \n",
    "    print(\"Ground truth: \")\n",
    "    print(get_token_seq(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
