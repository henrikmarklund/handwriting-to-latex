{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensor2tensor.layers.common_attention import add_timing_signal_nd #currently not in use\n",
    "import cv2\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Outline\n",
    "\n",
    "# 1. Encoder\n",
    "# 2. Decoder\n",
    "# 3. Optimization and training\n",
    "\n",
    "# 4. Inference\n",
    "\n",
    "## CONFIG:\n",
    "\n",
    "max_token_length = 50\n",
    "mini_batch_size = 32\n",
    "max_train_num_samples = 400\n",
    "max_val_num_samples = 10\n",
    "use_attention = True # I have not tried without attention so not sure if it breaks\n",
    "use_encoding_average_as_initial_state = True  #Only relevant when use_attention is True.\n",
    "num_units = 512 # LSTM number of units\n",
    "calculate_val_loss = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_shape(data_batch):\n",
    "\n",
    "    max_height = 0\n",
    "    max_width = 0\n",
    "\n",
    "    \n",
    "    \n",
    "    for sample in data_batch:\n",
    "        image = sample[0]\n",
    "        \n",
    "        if image.shape[0] > max_height:\n",
    "            max_height = image.shape[0]\n",
    "\n",
    "        if image.shape[1] > max_width:\n",
    "            max_width = image.shape[1]\n",
    "\n",
    "\n",
    "    return (max_height, max_width)\n",
    "\n",
    "\n",
    "def pad_images(data_batch):\n",
    "\n",
    "    new_data_batch = data_batch\n",
    "    \n",
    "    target_shape = get_max_shape(data_batch)\n",
    "\n",
    "    new_height = target_shape[0]\n",
    "    new_width = target_shape[1]\n",
    "    \n",
    "    for idx, sample in enumerate(data_batch):\n",
    "        padded_image = np.ones((new_height, new_width)) * 255\n",
    "\n",
    "        image = sample[0] # A sample consist of an image (0), a target text (1), and a sequence length (2)\n",
    "        \n",
    "        h = image.shape[0]\n",
    "        w = image.shape[1]\n",
    "\n",
    "        padded_image[:h, :w] = image\n",
    "        \n",
    "        new_data_batch[idx][0] = padded_image\n",
    "\n",
    "    return new_data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def load_raw_data(dataset_name, mini_batch_size, max_token_length = 400, max_num_samples = 5000):\n",
    "\n",
    "    buckets_dict = {(40, 160): 0,\n",
    "        (40, 200): 1,\n",
    "        (40, 240): 2,\n",
    "        (40, 280): 3,\n",
    "        (40, 320): 4,\n",
    "        (40, 360): 5,\n",
    "        (50, 120): 6,\n",
    "        (50, 200): 7,\n",
    "        (50, 240): 8,\n",
    "        (50, 280): 9,\n",
    "        (50, 320): 10,\n",
    "        (50, 360): 11,\n",
    "        (50, 400): 12,\n",
    "        (60, 360): 13,\n",
    "        (100, 360): 14,\n",
    "        (100, 500): 15,\n",
    "        (160, 400): 16,\n",
    "        (200, 500): 17,\n",
    "        (800, 800): 18}\n",
    "\n",
    "\n",
    "\n",
    "    buckets = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    if dataset_name == \"small\":\n",
    "        image_folder = 'data/tin/tiny/'\n",
    "        formula_file_path = \"data/tin/tiny.formulas.norm.txt\"\n",
    "    elif dataset_name == \"test\":\n",
    "        image_folder = 'data/images_test/'\n",
    "        formula_file_path = \"data/test.formulas.norm.txt\"\n",
    "    elif dataset_name == \"train\":\n",
    "        image_folder = 'data/images_train/'\n",
    "        formula_file_path = \"data/train.formulas.norm.txt\"\n",
    "    elif dataset_name == \"val\":\n",
    "        image_folder = 'data/images_val/'\n",
    "        formula_file_path = \"data/val.formulas.norm.txt\"\n",
    "    elif dataset_name == \"digital_numbers\":\n",
    "        image_folder = 'datasets/digital_numbers/images/'\n",
    "        formula_file_path = \"datasets/digital_numbers/number_sequences.txt\"\n",
    "\n",
    "        \n",
    "    in_counter = 0\n",
    "    examples_counter = 0\n",
    "    with open (formula_file_path, \"r\") as myfile:\n",
    "\n",
    "        for idx, token_sequence in enumerate(myfile):\n",
    "            \n",
    "            examples_counter += 1\n",
    "            #Check token size:\n",
    "            token_sequence = token_sequence.rstrip('\\n')\n",
    "            tokens = token_sequence.split()\n",
    "\n",
    "            file_name = str(idx) + '.png'\n",
    "            image = cv2.imread(image_folder + file_name, 0)\n",
    "            \n",
    "            if image is None:\n",
    "                print(\"Id:\", idx)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            \n",
    "            if len(tokens) <= max_token_length:\n",
    "                \n",
    "                token_sequence = '**start** ' + token_sequence\n",
    "                #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #Grey scale\n",
    "                \n",
    "                \n",
    "                seq_length = len(token_sequence.split())\n",
    "                \n",
    "                relevant_bucket_id = buckets_dict[image.shape]\n",
    "                \n",
    "                \n",
    "                buckets[relevant_bucket_id].append([image, token_sequence, seq_length])\n",
    "\n",
    "                \n",
    "                if len(buckets[buckets_dict[image.shape]]) == mini_batch_size:\n",
    "                    data_batch = np.array(buckets[buckets_dict[image.shape]])\n",
    "                    dataset.append(data_batch)\n",
    "                    buckets[buckets_dict[image.shape]] = []\n",
    "\n",
    "                in_counter += 1\n",
    "  \n",
    "            if in_counter == max_num_samples:\n",
    "                break\n",
    "\n",
    "        #put what's left in the buckets into batches (padding will be needed)  \n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        data_batch = []\n",
    "        \n",
    "        for idx, bucket in enumerate(buckets):\n",
    "            \n",
    "            for j, sample in enumerate(bucket):\n",
    "                data_batch.append(sample)\n",
    "                if len(data_batch) == mini_batch_size:\n",
    "                    padded_data_batch = pad_images(data_batch)\n",
    "                    padded_data_batch = np.array(padded_data_batch)\n",
    "                    dataset.append(padded_data_batch)\n",
    "                    data_batch = []\n",
    "            \n",
    "        \n",
    "        padded_data_batch = pad_images(data_batch)\n",
    "        padded_data_batch = np.array(padded_data_batch)\n",
    "        dataset.append(padded_data_batch)\n",
    "        \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is list of all batches containing (image, target_text, sequence_length)\n",
    "# we split that up into three lists\n",
    "\n",
    "\n",
    "def split_dataset(dataset):\n",
    "\n",
    "    encoder_input_data_batches = []\n",
    "    target_texts_batches = []\n",
    "    sequence_lengths_batches = []\n",
    "\n",
    "    for batch in range(len(dataset)):\n",
    "        image_batch = dataset[batch][:,0]\n",
    "        image_batch = image_batch.tolist()\n",
    "        image_batch = np.array(image_batch)\n",
    "        # Add one dimension so that the conv net can take it (it expects four dimensions)\n",
    "        image_batch = np.reshape(image_batch, (image_batch.shape[0], image_batch.shape[1], image_batch.shape[2], 1))\n",
    "        image_batch = image_batch.astype('uint8')\n",
    "\n",
    "        \n",
    "        encoder_input_data_batches.append(image_batch)\n",
    "\n",
    "        target_text = dataset[batch][:,1]\n",
    "        target_texts_batches.append(target_text)\n",
    "\n",
    "        decoder_length = dataset[batch][:,2]\n",
    "        decoder_length = np.array(decoder_length, dtype=np.int32)\n",
    "        sequence_lengths_batches.append(decoder_length)\n",
    "\n",
    "    # Make sure we have equal number of batches\n",
    "    assert (len(encoder_input_data_batches) == len(dataset))\n",
    "    assert (len(target_texts_batches) == len(dataset))\n",
    "    assert (len(sequence_lengths_batches) == len(dataset))\n",
    "    \n",
    "    return encoder_input_data_batches, target_texts_batches, sequence_lengths_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(dataset):\n",
    "    if dataset == \"small\":\n",
    "        vocab = [line for line in open('data/tin/tiny_vocab.txt')]\n",
    "    elif dataset == \"test\":\n",
    "        vocab = [line for line in open('data/vocab.txt')]\n",
    "    elif dataset == \"train\":\n",
    "        vocab = [line for line in open('data/vocab.txt')]\n",
    "\n",
    "    vocab = [x.strip('\\n') for x in vocab]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary\n",
    "\n",
    "token_vocabulary = []\n",
    "token_vocabulary.append(\"**end**\") # Want this to be the first token\n",
    "token_vocabulary.append(\"**start**\")\n",
    "token_vocabulary.append(\"**unknown**\")\n",
    "\n",
    "token_vocabulary.extend(get_vocabulary(\"train\"))\n",
    "\n",
    "target_tokens = token_vocabulary # TODO: Refactor this. Currently duplicate naming\n",
    "\n",
    "token_vocab_size = len(target_tokens)\n",
    "\n",
    "target_token_index = dict(\n",
    "    [(token, i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "reverse_target_token_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "#target_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_int_sequences(target_texts_batches, sequence_lengths_batches):\n",
    "    decoder_input_data_batches = []\n",
    "    decoder_target_data_batches = []\n",
    "    \n",
    "    for idx, target_texts_batch in enumerate(target_texts_batches):\n",
    "    \n",
    "        # get max dec seq length for that batch\n",
    "        max_decoder_seq_length = max(sequence_lengths_batches[idx])\n",
    "\n",
    "        batch_size = len(target_texts_batch)\n",
    "        \n",
    "        decoder_input_data = np.zeros(\n",
    "                (batch_size, max_decoder_seq_length),\n",
    "                dtype='int32')\n",
    "        decoder_target_data = np.zeros(\n",
    "                (batch_size, max_decoder_seq_length),\n",
    "                dtype='int32')\n",
    "\n",
    "        num_other = 0\n",
    "\n",
    "        for i, target_text in enumerate(target_texts_batch):\n",
    "            for t, token in enumerate(target_text.split()):\n",
    "                \n",
    "                if token in target_token_index:\n",
    "                    # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "\n",
    "                    decoder_input_data[i, t] = target_token_index[token]\n",
    "\n",
    "                    if t > 0:\n",
    "                        #decoder_target_data will be ahead by one timestep\n",
    "                        # and will not include the start character.\n",
    "                        decoder_target_data[i, t - 1] = target_token_index[token]\n",
    "\n",
    "                else:\n",
    "                    print(\"Token not in vocabulary (setting to **unknown**): \", token)\n",
    "                    num_other = num_other + 1\n",
    "                    decoder_input_data[i, t] = target_token_index['**unknown**']\n",
    "\n",
    "                    if t > 0:\n",
    "                        #decoder_target_data will be ahead by one timestep\n",
    "                        # and will not include the start character.\n",
    "\n",
    "                        decoder_target_data[i, t - 1] = target_token_index['**unknown**']\n",
    "\n",
    "            decoder_target_data[i, len(target_text.split()) - 1] = target_token_index['**end**']  \n",
    "            \n",
    "            \n",
    "        \n",
    "        decoder_input_data_batches.append(decoder_input_data)\n",
    "        decoder_target_data_batches.append(decoder_target_data)\n",
    "        \n",
    "    return decoder_input_data_batches, decoder_target_data_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name, mini_batch_size, max_token_length, max_num_samples):\n",
    "    dataset = load_raw_data(dataset_name, mini_batch_size, max_token_length = max_token_length, max_num_samples = max_num_samples)\n",
    "    \n",
    "    for k in range(len(dataset) - 1):\n",
    "        assert(len(dataset[k]) == mini_batch_size)\n",
    "\n",
    "    \n",
    "    encoder_input_data_batches, target_texts_batches, sequence_lengths_batches = split_dataset(dataset)\n",
    "    decoder_input_data_batches, decoder_target_data_batches = create_output_int_sequences(target_texts_batches, sequence_lengths_batches)    \n",
    "\n",
    "    return encoder_input_data_batches, target_texts_batches, sequence_lengths_batches, decoder_input_data_batches, decoder_target_data_batches\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Token not in vocabulary (setting to **unknown**): ', '\\\\c')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_data('train', mini_batch_size, max_token_length, max_train_num_samples)\n",
    "train_encoder_input_data_batches = train_dataset[0]\n",
    "train_target_texts_batches = train_dataset[1]\n",
    "train_sequence_lengths_batches = train_dataset[2]\n",
    "train_decoder_input_data_batches = train_dataset[3]\n",
    "train_decoder_target_data_batches = train_dataset[4]\n",
    "\n",
    "val_dataset = load_data('train', mini_batch_size, max_token_length, max_val_num_samples)\n",
    "val_encoder_input_data_batches = val_dataset[0]\n",
    "val_target_texts_batches = val_dataset[1]\n",
    "val_sequence_lengths_batches = val_dataset[2]\n",
    "val_decoder_input_data_batches = val_dataset[3]\n",
    "val_decoder_target_data_batches = val_dataset[4]\n",
    "\n",
    "\n",
    "num_train_batches = len(train_target_texts_batches)\n",
    "num_val_batches = len(val_target_texts_batches)\n",
    "\n",
    "num_train_samples = (num_train_batches - 1) * mini_batch_size + train_target_texts_batches[-1].shape[0]\n",
    "num_val_samples = (num_val_batches - 1) * mini_batch_size + val_target_texts_batches[-1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Num train batches: ', 13)\n",
      "('Num val batches: ', 1)\n",
      "('Num train samples: ', 400)\n",
      "('Num val samples: ', 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Num train batches: \", num_train_batches)\n",
    "print(\"Num val batches: \", num_val_batches)\n",
    "\n",
    "print(\"Num train samples: \", num_train_samples)\n",
    "print(\"Num val samples: \", num_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACuCAYAAAAmsfauAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIMuIpITatBFRHJCDbqISE78PxwxnXD1E7csAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13aaadc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1 396 174 396 371  54 398 396 371  86 398 398  29  12 396 174 396 392\n",
      "  86 398 396  86 365 396  62 365 396  16 398 398 364 396  18 398 398 398\n",
      "  64  14   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "\n",
      "[396 174 396 371  54 398 396 371  86 398 398  29  12 396 174 396 392  86\n",
      " 398 396  86 365 396  62 365 396  16 398 398 364 396  18 398 398 398  64\n",
      "  14   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "**start** { \\frac { d V } { d \\Phi } } = - { \\frac { w \\Phi } { \\Phi _ { \\! _ { 0 } } ^ { 2 } } } \\, .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.imshow(train_encoder_input_data_batches[0][0].squeeze(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(train_decoder_input_data_batches[0][0])\n",
    "\n",
    "print(\"\")\n",
    "print(train_decoder_target_data_batches[0][0])\n",
    "\n",
    "\n",
    "print(train_target_texts_batches[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder\n",
    "\n",
    "## One of Genthails's encoder implementations (from paper)\n",
    "\n",
    "\n",
    "\n",
    "img = tf.placeholder(tf.uint8, [None, None, None, 1], name='img')\n",
    "\n",
    "img = tf.cast(img, tf.float32) / 255\n",
    "\n",
    "\n",
    "batch_size = tf.shape(img)[0]\n",
    "\n",
    "# Conv + max pooling\n",
    "out = tf.layers.conv2d(img, 64, 3, 1, \"SAME\", activation=tf.nn.relu)\n",
    "# Conv + max pooling\n",
    "out = tf.layers.conv2d(out, 128, 3, 1, \"SAME\", activation=tf.nn.relu)\n",
    "\n",
    "out = tf.layers.conv2d(out, 256, 3, 1, \"SAME\", activation=tf.nn.relu) # regular conv -> id\n",
    "out = tf.layers.batch_normalization(out)\n",
    "\n",
    "\n",
    "out = tf.layers.conv2d(out, 256, 3, 1, \"SAME\", activation=tf.nn.relu) # regular conv -> id\n",
    "out = tf.layers.max_pooling2d(out, (2, 1), (2, 1), \"SAME\")\n",
    "\n",
    "out = tf.layers.conv2d(out, 512, 3, 1, \"SAME\", activation=tf.nn.relu) # regular conv -> id\n",
    "out = tf.layers.max_pooling2d(out, (1, 2), (1, 2), \"SAME\")\n",
    "\n",
    "\n",
    "# Conv valid\n",
    "out = tf.layers.conv2d(out, 512, 3, 1, \"VALID\", activation=tf.nn.relu, name=\"last_conv_layer\") # conv\n",
    "out = tf.layers.batch_normalization(out)\n",
    "\n",
    "## Out is now a H'*W' encoding of the image\n",
    "\n",
    "## We want to turn this into a sequence of vectors: (e1, e2 ... en)\n",
    "#H= out.shape[1]\n",
    "#W= out.shape[2] \n",
    "#C= out.shape[3]\n",
    "\n",
    "H = tf.shape(out)[1]\n",
    "W = tf.shape(out)[2]\n",
    "\n",
    "#out = add_timing_signal_nd(out)\n",
    "seq = tf.reshape(tensor=out, shape=[-1, H*W, 512])\n",
    "\n",
    "# TODO: Add positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First state of the decoder consists of two vectors, the hidden state (h0) and the memory (c0).\n",
    "# Usually the hidden state refers to [h0, c0]. So a little bit of overloading of hidden state (I think)\n",
    "# This is how Genthail implements it\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if use_encoding_average_as_initial_state:\n",
    "    img_mean = tf.reduce_mean(seq, axis=1)\n",
    "\n",
    "    img_mean = tf.layers.batch_normalization(img_mean)\n",
    "\n",
    "    W = tf.get_variable(\"W\", shape=[512, num_units])\n",
    "    b = tf.get_variable(\"b\", shape=[num_units])\n",
    "    h0 = tf.tanh(tf.matmul(img_mean, W) + b)\n",
    "\n",
    "    W_ = tf.get_variable(\"W_\", shape=[512, num_units])\n",
    "    b_ = tf.get_variable(\"b_\", shape=[num_units])\n",
    "    c0 = tf.tanh(tf.matmul(img_mean, W_) + b_)\n",
    "\n",
    "\n",
    "    encoder_state = tf.contrib.rnn.LSTMStateTuple(c0, h0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention_states: [batch_size, max_time, num_units]\n",
    "attention_states = seq\n",
    "\n",
    "\n",
    "attention_depth = num_units\n",
    "\n",
    "# Create an attention mechanism\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "    attention_depth, attention_states, scale=True) # Can try scale = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 80)\n"
     ]
    }
   ],
   "source": [
    "# Decoder: from seq2seq tutorial \n",
    "\n",
    "\n",
    "\n",
    "embedding_size = 80 # In Genthail's paper he says he has 80 embeddings which I believe corresponds to embedding_size\n",
    "\n",
    "decoder_inputs = tf.placeholder(tf.int32, [None, None], name='decoder_inputs')  # Supposed to be a sequence of numbers corresponding to the different tokens in the sentence\n",
    "\n",
    "# Embedding of target tokens\n",
    "\n",
    "# Embedding matrix \n",
    "embedding_decoder = tf.get_variable(\n",
    "    \"embedding_encoder\", [token_vocab_size, embedding_size], tf.float32, initializer=tf.orthogonal_initializer) #  tf.float32 was default in the NMT tutorial\n",
    "\n",
    "\n",
    "\n",
    "# Look up embedding:\n",
    "#   decoder_inputs: [max_time, batch_size]\n",
    "#   decoder_emb_inp: [max_time, batch_size, embedding_size]\n",
    "decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "    embedding_decoder, decoder_inputs)\n",
    "\n",
    "print(decoder_emb_inp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build RNN cell\n",
    "#decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "\n",
    "# Using this instead compared to NMT tutorial so we can initialize with orthogonal intializer (like Genthail)\n",
    "decoder_cell = tf.nn.rnn_cell.LSTMCell(\n",
    "    num_units,\n",
    "    initializer=tf.orthogonal_initializer,\n",
    ")\n",
    "\n",
    "\n",
    "if use_attention:\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        decoder_cell, attention_mechanism,\n",
    "        attention_layer_size=512)\n",
    "\n",
    "    ## Set initial state of decoder to zero (possible to use previous state)\n",
    "\n",
    "    \n",
    "    if use_encoding_average_as_initial_state:\n",
    "        decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n",
    "    else:\n",
    "        decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "else:\n",
    "    decoder_initial_state = encoder_state\n",
    "\n",
    "    \n",
    "decoder_lengths = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    decoder_emb_inp, decoder_lengths, time_major=False)\n",
    "\n",
    "# Projection layer\n",
    "projection_layer = layers_core.Dense(token_vocab_size, use_bias=False, name=\"output_projection\")# Said layers_core before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decoder\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    decoder_cell, helper, decoder_initial_state,\n",
    "    output_layer=projection_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0-dev20171121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'Tanh_1:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'Tanh:0' shape=(?, 512) dtype=float32>), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(?, 512) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(?, ?) dtype=float32>, alignment_history=())"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "decoder_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 400)\n"
     ]
    }
   ],
   "source": [
    "# Dynamic decoding\n",
    "outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=False)  ## Understand parameter Impute finished\n",
    "logits = outputs.rnn_output\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Don't move or remove ()\n",
    "\n",
    "\n",
    "# Exponential learning rate\n",
    "#starter_learning_rate = 0.0006\n",
    "#learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           #5, 0.93, staircase=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Piece wise learning rate\n",
    "\n",
    "#steps_per_epoch = num_samples / mini_batch_size\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)  ## IMPORTANT\n",
    "#boundaries = [30,60, 90, 120, 150, 180, 210, 240]  \n",
    "\n",
    "#boundaries = [steps_per_epoch * epoch ]\n",
    "#values = [0.0001, 0.0001, 0.001, 0.0005, 0.0002, 0.0001, 0.00009, 0.00008, 0.00007]\n",
    "    \n",
    "#learning_rate = tf.train.piecewise_constant(global_step, boundaries, values, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_weights = tf.placeholder(tf.int8, [None, None], name='target_weights')\n",
    "#target_weights = tf.cast(target_weights, tf.float32)\n",
    "\n",
    "# Supposed to be a sequence of numbers corresponding to the different tokens in the sentence\n",
    "decoder_outputs = tf.placeholder(tf.int32, [None, None], name='decoder_outputs') \n",
    "\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])    \n",
    "    \n",
    "# Loss function\n",
    "\n",
    "# HYPERPARAMETER: Should we divide by sequence length on each\n",
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=decoder_outputs, logits=logits)\n",
    "\n",
    "# Create the target_weights (the masking)\n",
    "max_seq_length = tf.shape(decoder_outputs)[1]\n",
    "target_weights = tf.sequence_mask(decoder_lengths, max_seq_length, dtype=logits.dtype)\n",
    "\n",
    "\n",
    "train_loss = tf.reduce_sum(crossent * target_weights) / tf.cast(batch_size, tf.float32)\n",
    "\n",
    "tf.summary.scalar('loss', train_loss)\n",
    "\n",
    "# Calculate and clip gradients\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(train_loss, params)\n",
    "\n",
    "\n",
    "max_gradient_norm = 3  # Usually a number between 1 and 5. Set to 5 in the NMT.\n",
    "\n",
    "clipped_gradients, global_norm = tf.clip_by_global_norm(\n",
    "    gradients, max_gradient_norm)\n",
    "\n",
    "tf.summary.scalar('global_norm', global_norm)\n",
    "\n",
    "\n",
    "# Optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "update_step = optimizer.apply_gradients(\n",
    "    zip(clipped_gradients, params), global_step=global_step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv2d/kernel:0/gradient is illegal; using conv2d/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0/gradient is illegal; using conv2d/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/kernel:0/gradient is illegal; using conv2d_1/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/bias:0/gradient is illegal; using conv2d_1/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0/gradient is illegal; using conv2d_2/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0/gradient is illegal; using conv2d_2/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization/gamma:0/gradient is illegal; using batch_normalization/gamma_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization/beta:0/gradient is illegal; using batch_normalization/beta_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/kernel:0/gradient is illegal; using conv2d_3/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/bias:0/gradient is illegal; using conv2d_3/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/kernel:0/gradient is illegal; using conv2d_4/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/bias:0/gradient is illegal; using conv2d_4/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/kernel:0/gradient is illegal; using last_conv_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/bias:0/gradient is illegal; using last_conv_layer/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_1/gamma:0/gradient is illegal; using batch_normalization_1/gamma_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_1/beta:0/gradient is illegal; using batch_normalization_1/beta_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/gamma:0/gradient is illegal; using batch_normalization_2/gamma_0/gradient instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/beta:0/gradient is illegal; using batch_normalization_2/beta_0/gradient instead.\n",
      "INFO:tensorflow:Summary name W:0/gradient is illegal; using W_0/gradient instead.\n",
      "INFO:tensorflow:Summary name b:0/gradient is illegal; using b_0/gradient instead.\n",
      "INFO:tensorflow:Summary name W_:0/gradient is illegal; using W__0/gradient instead.\n",
      "INFO:tensorflow:Summary name b_:0/gradient is illegal; using b__0/gradient instead.\n",
      "INFO:tensorflow:Summary name memory_layer/kernel:0/gradient is illegal; using memory_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name embedding_encoder:0/gradient is illegal; using embedding_encoder_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/lstm_cell/kernel:0/gradient is illegal; using decoder/attention_wrapper/lstm_cell/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/lstm_cell/bias:0/gradient is illegal; using decoder/attention_wrapper/lstm_cell/bias_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/luong_attention/attention_g:0/gradient is illegal; using decoder/attention_wrapper/luong_attention/attention_g_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/attention_layer/kernel:0/gradient is illegal; using decoder/attention_wrapper/attention_layer/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name decoder/output_projection/kernel:0/gradient is illegal; using decoder/output_projection/kernel_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2d/kernel:0/weight is illegal; using conv2d/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0/weight is illegal; using conv2d/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/kernel:0/weight is illegal; using conv2d_1/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/bias:0/weight is illegal; using conv2d_1/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0/weight is illegal; using conv2d_2/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0/weight is illegal; using conv2d_2/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization/gamma:0/weight is illegal; using batch_normalization/gamma_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization/beta:0/weight is illegal; using batch_normalization/beta_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/kernel:0/weight is illegal; using conv2d_3/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_3/bias:0/weight is illegal; using conv2d_3/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/kernel:0/weight is illegal; using conv2d_4/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name conv2d_4/bias:0/weight is illegal; using conv2d_4/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/kernel:0/weight is illegal; using last_conv_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name last_conv_layer/bias:0/weight is illegal; using last_conv_layer/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_1/gamma:0/weight is illegal; using batch_normalization_1/gamma_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_1/beta:0/weight is illegal; using batch_normalization_1/beta_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/gamma:0/weight is illegal; using batch_normalization_2/gamma_0/weight instead.\n",
      "INFO:tensorflow:Summary name batch_normalization_2/beta:0/weight is illegal; using batch_normalization_2/beta_0/weight instead.\n",
      "INFO:tensorflow:Summary name W:0/weight is illegal; using W_0/weight instead.\n",
      "INFO:tensorflow:Summary name b:0/weight is illegal; using b_0/weight instead.\n",
      "INFO:tensorflow:Summary name W_:0/weight is illegal; using W__0/weight instead.\n",
      "INFO:tensorflow:Summary name b_:0/weight is illegal; using b__0/weight instead.\n",
      "INFO:tensorflow:Summary name memory_layer/kernel:0/weight is illegal; using memory_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name embedding_encoder:0/weight is illegal; using embedding_encoder_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/lstm_cell/kernel:0/weight is illegal; using decoder/attention_wrapper/lstm_cell/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/lstm_cell/bias:0/weight is illegal; using decoder/attention_wrapper/lstm_cell/bias_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/luong_attention/attention_g:0/weight is illegal; using decoder/attention_wrapper/luong_attention/attention_g_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/attention_wrapper/attention_layer/kernel:0/weight is illegal; using decoder/attention_wrapper/attention_layer/kernel_0/weight instead.\n",
      "INFO:tensorflow:Summary name decoder/output_projection/kernel:0/weight is illegal; using decoder/output_projection/kernel_0/weight instead.\n"
     ]
    }
   ],
   "source": [
    "param_names = [v.name for v in params]\n",
    "\n",
    "gradient_names = [g.name for g in gradients]\n",
    "\n",
    "gradient_norms = [tf.norm(gradient) for gradient in gradients]\n",
    "\n",
    "\n",
    "grads = list(zip(gradients, params))\n",
    "\n",
    "\n",
    "\n",
    "for grad, var in grads:\n",
    "    tf.summary.histogram(var.name + '/gradient', grad)\n",
    "\n",
    "for param in params:\n",
    "    to_summary = tf.summary.histogram(param.name + '/weight', param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the mask\n",
    "\n",
    "#decoder_mask_data = np.zeros(shape=(decoder_target_data.shape), dtype=np.int8)\n",
    "#for idx, decoder_length in enumerate(decoder_lengths_data):\n",
    "    #decoder_mask_data[idx, :(decoder_length)] = np.ones((1, decoder_length))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check so all the input data has the same number of examples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('summaries/train/',\n",
    "                                      sess.graph)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Learning rate_schedule\n",
    "\n",
    "# Epoch 0 - 2: Warmup with a lower learning rate: (1e-4)\n",
    "# Epoch 3 - 6: (5e-4)\n",
    "# Epoch 7 - 20: Exponentially decaying from (5e-4) to (1e-5)\n",
    "\n",
    "\n",
    "def get_learning_rate(global_step):\n",
    "    \n",
    "    epoch = int(float(global_step) / num_train_batches)\n",
    "    \n",
    "    \n",
    "    if epoch < 3:\n",
    "        #Warm up\n",
    "        lr_rate = 0.0001\n",
    "    elif epoch < 6:\n",
    "        lr_rate = 0.0005\n",
    "    elif epoch < 16:\n",
    "        # Over 10 epochs decay learning rate from 0.0005 to 0.00001\n",
    "    \n",
    "        decay_rate = 0.00001 / 0.0005\n",
    "        decay_steps = num_train_batches * 10\n",
    "        lr_rate = base_learning_rate * decay_rate ** (float((global_step - num_train_batches * 6)) / decay_steps)\n",
    "    else:\n",
    "        lr_rate = 0.00001\n",
    "    return lr_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n51xSeEJxzjmXFP8fItZOj+WwRswAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12fec1dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rates = []\n",
    "\n",
    "\n",
    "for step in range(num_train_batches*20):\n",
    "    learning_rates.append(get_learning_rate(step))\n",
    "\n",
    "plt.title('Learning rate (10^) over the steps')\n",
    "plt.ylabel('learning rate (10 ^)')\n",
    "plt.xlabel('steps #')\n",
    "plt.plot(np.log10(learning_rates))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_loss():\n",
    "    #num_val_batches = len(val_sequence_lengths_batches)\n",
    "    val_loss = 0    \n",
    "    \n",
    "    for i in range(num_val_batches):\n",
    "\n",
    "\n",
    "        input_data = {img: val_encoder_input_data_batches[i],\n",
    "                                        decoder_lengths: val_sequence_lengths_batches[i],\n",
    "                                         decoder_inputs: val_decoder_input_data_batches[i],\n",
    "                                          decoder_outputs: val_decoder_target_data_batches[i],\n",
    "                                         }\n",
    "   \n",
    "\n",
    "        output_tensors = [train_loss]\n",
    "\n",
    "        loss = sess.run(output_tensors, \n",
    "                               feed_dict=input_data)\n",
    "\n",
    "        \n",
    "        print(loss)\n",
    "        val_loss = val_loss + loss[0]\n",
    "        \n",
    "\n",
    "    val_loss = val_loss / len(val_decoder_input_data_batches[i])\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Num batches: ', 13)\n",
      "[38 23 27 39 26 14 38 16 38 42 25 23 32 13 36 22 25 30 50 20 20 15 26 26 25\n",
      " 25 24  7 29 28 35 26]\n",
      "(32, 50, 120, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Num batches: \", len(train_sequence_lengths_batches)) #(Note: they are not necessarily equal size towards the end (this will fix later))\n",
    "\n",
    "\n",
    "train_decoder_target_data_batches\n",
    "\n",
    "print(train_sequence_lengths_batches[0])\n",
    "\n",
    "print(train_encoder_input_data_batches[0].shape)\n",
    "\n",
    "\n",
    "#train_decoder_input_data_batches[0][0] = np.array(train_decoder_input_data_batches[0][0])\n",
    "train_decoder_target_data_batches[0][0] = np.array(train_decoder_target_data_batches[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "(32, 50)\n",
      "(32, 50)\n"
     ]
    }
   ],
   "source": [
    "#print(train_decoder_input_data_batches[0].shape)\n",
    "\n",
    "print(train_sequence_lengths_batches[0].shape)\n",
    "print(train_decoder_target_data_batches[0].shape)\n",
    "print(train_decoder_input_data_batches[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total number of parameters: ', 8313857)\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "# Get total number of parameters\n",
    "\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    #print(shape)\n",
    "    #print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        #print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    #print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(\"Total number of parameters: \", total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Num batches: ', 13)\n",
      "('Epoch: ', 0)\n",
      "('i: ', 0)\n",
      "Step 1: loss = 236.06\n",
      "('Learning rate: ', array(9.999999747378752e-05, dtype=float32))\n",
      "('Global grad norm: ', 2947.5652)\n",
      "('Time for batch in seconds: ', 93.786908)\n",
      "('i: ', 1)\n",
      "Step 2: loss = 204.12\n",
      "('Learning rate: ', array(9.999999747378752e-05, dtype=float32))\n",
      "('Global grad norm: ', 5095.9351)\n",
      "('Time for batch in seconds: ', 185.984883)\n"
     ]
    }
   ],
   "source": [
    "## TODO Implement checkpoints\n",
    "\n",
    "num_epochs = 3\n",
    "val_losses = []\n",
    "print(\"Num batches: \", num_train_batches)\n",
    "\n",
    "\n",
    "glob_step = sess.run(global_step) # Get what global step we are at in training already (so that the learning_rate is set correct)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i in range(num_train_batches):\n",
    "        \n",
    "        # Calculate running time for batch1 and batch2 ( since they are both a bit different)\n",
    "        if i == 0 or i == 1:\n",
    "            start_time = datetime.datetime.now()\n",
    "        \n",
    "        \n",
    "        # Calculate the right learning rate for this step.\n",
    "        lrate = get_learning_rate(glob_step)\n",
    "        \n",
    "        \n",
    "        input_data = {img: train_encoder_input_data_batches[i],\n",
    "                                        decoder_lengths: train_sequence_lengths_batches[i],\n",
    "                                         decoder_inputs: train_decoder_input_data_batches[i],\n",
    "                                          decoder_outputs: train_decoder_target_data_batches[i],\n",
    "                                          learning_rate: lrate\n",
    "                                         }\n",
    "        \n",
    "        \n",
    "        # Only catch important info for tracking in the beginning of the epoch\n",
    "        if (i == 0):\n",
    "            output_tensors = [merged, update_step,train_loss, optimizer._lr, global_norm, gradient_norms, global_step]\n",
    "            summary, _, loss, lr_rate, global_grad_norm, grad_norms, glob_step = sess.run(output_tensors, \n",
    "                               feed_dict=input_data)\n",
    "            \n",
    "            train_writer.add_summary(summary, glob_step)\n",
    "            \n",
    "        else:\n",
    "            output_tensors = [update_step, train_loss, global_norm, global_step, optimizer._lr]\n",
    "            _, loss, global_grad_norm, glob_step, lr_rate  = sess.run(output_tensors, \n",
    "                               feed_dict=input_data)\n",
    "\n",
    "\n",
    "        if i == 0:\n",
    "            print(\"Epoch: \", epoch)\n",
    "            \n",
    "\n",
    "        print(\"i: \", i)\n",
    "        print('Step %d: loss = %.2f' % (glob_step, loss))\n",
    "        print(\"Learning rate: \", lr_rate)\n",
    "        print(\"Global grad norm: \", global_grad_norm)\n",
    "\n",
    "        if i == 0 or i == 1: \n",
    "            end_time = datetime.datetime.now()\n",
    "            \n",
    "            delta = end_time - start_time\n",
    "            print(\"Time for batch in seconds: \", delta.total_seconds())\n",
    "        \n",
    "        # Write to tensorboard every 10th step\n",
    "        if glob_step % 40 == 0:\n",
    "            train_writer.add_summary(summary, glob_step)\n",
    "        ## Run the following in terminal to get up tensorboard: tensorboard --logdir=summaries/train        \n",
    "        \n",
    "        # TODO Add saving of checkpoint every now and then\n",
    "        \n",
    "    if calculate_val_loss:\n",
    "        val_loss = get_validation_loss()\n",
    "        val_losses.append(val_loss)\n",
    "        print(\"Val loss: \", val_loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "saver.save(sess, 'my-test-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference\n",
    "\n",
    "tgt_sos_id = target_token_index['**start**'] # 1\n",
    "tgt_eos_id = target_token_index['**end**'] # 0\n",
    "\n",
    "\n",
    "batch_for_inference = train_encoder_input_data_batches[0]\n",
    "inference_batch_size = len(batch_for_inference)\n",
    "\n",
    "# Helper\n",
    "inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "    embedding_decoder,\n",
    "    tf.fill([inference_batch_size], tgt_sos_id), tgt_eos_id)\n",
    "\n",
    "maximum_iterations = 15 # Do max seq_length or find some other heuristic\n",
    "\n",
    "# Decoder\n",
    "inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    decoder_cell, inference_helper, decoder_initial_state,\n",
    "    output_layer=projection_layer)\n",
    "# Dynamic decoding\n",
    "outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "    inference_decoder, maximum_iterations=maximum_iterations)\n",
    "translations = outputs.sample_id\n",
    "\n",
    "\n",
    "trans = sess.run(translations, feed_dict = {img: batch_for_inference})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 49,  49, 161, 161, 161,  48,  48,  48,  48,  48,  48,  48,  48,\n",
       "         48,  48],\n",
       "       [ 49, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
       "        161, 161],\n",
       "       [ 49,  49,  49,  49, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
       "        161, 161]], dtype=int32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "trans\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_seq(int_sequence):\n",
    "    \n",
    "    output_string = \"\"\n",
    "    for value in int_sequence:\n",
    "        output_string += reverse_target_token_index[value] + \" \"\n",
    "        if reverse_target_token_index[value] == \"**end**\":\n",
    "            break\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "Q Q \\end{array} \\end{array} \\end{array} P P P P P P P P P P \n",
      "\n",
      "Ground truth: \n",
      "**start** \\, ^ { * } d \\, ^ { * } H = \\kappa \\, ^ { * } d \\phi = J _ { B } .\n",
      "\n",
      "Output: \n",
      "Q \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \n",
      "\n",
      "Ground truth: \n",
      "**start** ( { \\cal L } _ { a } g ) _ { i j } = 0 , \\ \\ \\ \\ ( { \\cal L } _ { a } H ) _ { i j k } = 0 ,\n",
      "\n",
      "Output: \n",
      "Q Q Q Q \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \\end{array} \n",
      "\n",
      "Ground truth: \n",
      "**start** \\hat { N } _ { 3 } = \\sum \\sp f _ { j = 1 } a _ { j } \\sp { \\dagger } a _ { j } \\, .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx, seq in enumerate(train_target_texts_batches[0]):\n",
    "    \n",
    "    print(\"Output: \")\n",
    "    print(get_token_seq(trans[idx]))\n",
    "    print(\"\")\n",
    "    print(\"Ground truth: \")\n",
    "    print(train_target_texts_batches[0][idx])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
