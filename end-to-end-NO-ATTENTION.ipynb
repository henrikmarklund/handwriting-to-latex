{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Numpy, Tensorflow and Keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "#from tensor2tensor.layers.common_attention import add_timing_signal_nd #currently not in use\n",
    "\n",
    "from keras.layers import Input, LSTM, Dense, Lambda, GlobalAveragePooling1D, Add, Concatenate, Multiply\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Reshape, Flatten, BatchNormalization, Embedding\n",
    "from keras import layers, backend ## IS THIS BEING USED?\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, TensorBoard, Callback\n",
    "import keras.backend as K\n",
    "from keras import optimizers, metrics\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import our own helper functions\n",
    "from prepare_data import get_decoder_data, get_decoder_data_int_sequences\n",
    "import evaluation_model_keras as evalm\n",
    "\n",
    "\n",
    "import helper_functions as hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This model takes a fixed image input size. This is contrast to our attention model that achieves a lot higher performance.\n",
    "\n",
    "\n",
    "#Hyperparameters\n",
    "\n",
    "hparams = {}\n",
    "\n",
    "# Model\n",
    "hparams['dsample_factor'] = 0.9\n",
    "hparams['encoder_model'] = \"CONVNET1\" #\"CONVNET1, CONVNET2\n",
    "hparams['lstm_dim'] = 512\n",
    "\n",
    "\n",
    "# Data\n",
    "hparams['max_num_samples'] = 100000\n",
    "\n",
    "# Training\n",
    "hparams['epochs'] = 20\n",
    "hparams['batch_size'] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains a bunch of functions for loading and preprocessing the data\n",
    "\n",
    "def create_metric_output_files():\n",
    "\n",
    "    file = open(\"/output/metrics.txt\",\"w\") \n",
    "\n",
    "    file.write(\"Train loss\" + \"\\t\" + \"Val loss\" + \"\\n\")\n",
    "\n",
    "    file.close()\n",
    "\n",
    "def from_one_hot_to_latex_sequence(one_hot_sequence):\n",
    "    tokens = []\n",
    "    for idx, token_vector in enumerate(one_hot_sequence):\n",
    "\n",
    "        sampled_token_index = np.argmax(token_vector)\n",
    "        sampled_char = reverse_target_token_index[sampled_token_index]\n",
    "\n",
    "        tokens.append(sampled_char)\n",
    "        if sampled_char == '**end**':\n",
    "            break\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def shuffle_data(X,Y,Z,seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    num_samples = X.shape[0]\n",
    "    p = np.random.permutation(num_samples)\n",
    "    if len(X.shape) == 3:\n",
    "        X = X[p,:,:]\n",
    "    elif len(X.shape) == 4:\n",
    "        X = X[p,:,:,:]\n",
    "    Y = Y[p]\n",
    "    Z = Z[p]\n",
    "\n",
    "    \n",
    "    return X,Y,Z\n",
    "\n",
    "def get_max_shape(images):\n",
    "\n",
    "    max_height = 0\n",
    "    max_width = 0\n",
    "\n",
    "    for image in images:\n",
    "        if image.shape[0] > max_height:\n",
    "            max_height = image.shape[0]\n",
    "\n",
    "        if image.shape[1] > max_width:\n",
    "            max_width = image.shape[1]\n",
    "\n",
    "\n",
    "    return [max_height, max_width]\n",
    "\n",
    "\n",
    "def normalize_images(images):\n",
    "\n",
    "    images = images.astype(np.float32)\n",
    "    images = np.multiply(images, 1.0 / 255.0)\n",
    "\n",
    "    return images\n",
    "\n",
    "def down_sample(images, factor): \n",
    "    target_h = int(math.floor(float(images[0].shape[0]) * factor))\n",
    "    target_w = int(math.floor(float(images[0].shape[1]) * factor))\n",
    "    num_images = len(images)\n",
    "    down_sampled_images = np.ones((num_images, target_h, target_w)) * 255\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "\n",
    "        im = image\n",
    "\n",
    "        #Downsample\n",
    "        im = cv2.resize(im, (0, 0), fx = factor, fy=factor, interpolation = cv2.INTER_AREA) #cv2.INTER_LINEAR\n",
    "\n",
    "\n",
    "        down_sampled_images[idx, :, :] = im\n",
    "\n",
    "    return down_sampled_images\n",
    "\n",
    "\n",
    "def down_sample_flexible(images, factor): \n",
    "    print(\"downsampling images\")\n",
    "    new_images = []\n",
    "\n",
    "    for image in images:\n",
    "        new_image = cv2.resize(image, (0, 0), fx = factor, fy=factor, interpolation = cv2.INTER_AREA) #cv2.INTER_LINEAR\n",
    "        new_images.append(new_image)\n",
    "\n",
    "    return new_images\n",
    "\n",
    "\n",
    "def pad_images(images, target_shape):\n",
    "    \n",
    "\n",
    "    if (target_shape == None):\n",
    "        max_height, max_width = get_max_shape(images)\n",
    "    else:\n",
    "        max_height = target_shape[0]\n",
    "        max_width = target_shape[1]\n",
    "\n",
    "    target_shape = (max_height, max_width)\n",
    "    print(\"target shape: \", target_shape)\n",
    "    \n",
    "    num_images = len(images)\n",
    "\n",
    "    padded_images = np.ones((num_images, max_height, max_width)) * 255\n",
    "    for idx, image in enumerate(images):\n",
    "\n",
    "        h = image.shape[0]\n",
    "        w = image.shape[1]\n",
    "\n",
    "        padded_images[idx, :h, :w] = image\n",
    "\n",
    "    \n",
    "    return padded_images, target_shape\n",
    "\n",
    "def get_vocabulary_size():\n",
    "    return len(get_vocabulary())\n",
    "\n",
    "def get_vocabulary(dataset):\n",
    "    if dataset == \"small\":\n",
    "        vocab = [line for line in open('data/tin/tiny_vocab.txt')]\n",
    "    elif dataset == \"test\":\n",
    "        vocab = [line for line in open('data/vocab.txt')]\n",
    "    elif dataset == \"train\":\n",
    "        vocab = [line for line in open('data/vocab.txt')]\n",
    "\n",
    "    vocab = [x.strip('\\n') for x in vocab]\n",
    "    return vocab\n",
    "\n",
    "def load_raw_data(dataset, max_token_length = 400, max_image_size = (60, 200), max_num_samples = 5000):\n",
    "    \n",
    "    token_vocabulary = []\n",
    "    token_sequences = []\n",
    "    images = []\n",
    "    \n",
    "    if dataset == \"small\":\n",
    "        image_folder = 'data/tin/tiny/'\n",
    "        formula_file_path = \"data/tin/tiny.formulas.norm.txt\"\n",
    "    elif dataset == \"test\":\n",
    "        image_folder = '../data/images_test/'\n",
    "        formula_file_path = \"../data/test.formulas.norm.txt\"\n",
    "    elif dataset == \"train\":\n",
    "        image_folder = '../data/images_train/'\n",
    "        formula_file_path = \"../data/train.formulas.norm.txt\"\n",
    "    elif dataset == \"val\":\n",
    "        image_folder = '../data/images_val/'\n",
    "        formula_file_path = \"../data/val.formulas.norm.txt\"\n",
    "        \n",
    "    included_counter = 0\n",
    "    examples_counter = 0\n",
    "    with open (formula_file_path, \"r\") as myfile:\n",
    "\n",
    "        for idx, token_sequence in enumerate(myfile):\n",
    "            examples_counter += 1\n",
    "            #Check token size:\n",
    "            token_sequence = token_sequence.rstrip('\\n')\n",
    "            tokens = token_sequence.split()\n",
    "\n",
    "            file_name = str(idx) + '.png'\n",
    "            image = cv2.imread(image_folder + file_name, 0)\n",
    "            \n",
    "            if image is None:\n",
    "                print(\"Not loading image with id:\", idx)\n",
    "                continue\n",
    "            \n",
    "            #print(tokens)\n",
    "            if len(tokens) <= max_token_length and image.shape[0] <= max_image_size[0] and image.shape[1] <= max_image_size[1]:\n",
    "                token_sequences.append('**start** ' + token_sequence + ' **end**')\n",
    "                #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #Grey scale\n",
    "                #print(image)\n",
    "                \n",
    "                images.append(image)\n",
    "                for token in tokens:\n",
    "                    if token not in token_vocabulary:\n",
    "                        token_vocabulary.append(token)\n",
    "\n",
    "                included_counter += 1\n",
    "                if included_counter == max_num_samples:\n",
    "                    break\n",
    "    \n",
    "    token_vocabulary.append(\"**start**\")\n",
    "    token_vocabulary.append(\"**end**\")\n",
    "    token_vocabulary.append(\"**other**\")\n",
    "    \n",
    "    return images, token_sequences, token_vocabulary\n",
    "\n",
    "\n",
    "def preprocess_images(images, target_shape):\n",
    "    \n",
    "    encoder_input = down_sample_flexible(images, hparams['dsample_factor'])\n",
    "    encoder_input, target_shape = pad_images(encoder_input, target_shape)\n",
    "    encoder_input = normalize_images(encoder_input)\n",
    "\n",
    "    # Add dimension for TensorFlow Conv Layers to work properly as it needs (None, Height, Width, 1)\n",
    "    encoder_input = encoder_input.reshape(encoder_input.shape[0], encoder_input.shape[1], encoder_input.shape[2], 1)\n",
    "\n",
    "    return encoder_input, target_shape\n",
    "\n",
    "\n",
    "def load_data(dataset, max_token_length, max_image_size, max_num_samples):\n",
    "    \n",
    "    ## First get all the training and validation data\n",
    "    images_train, token_sequences_train, token_vocabulary_train = load_raw_data(dataset=\"train\",  max_token_length = max_token_length, max_image_size = max_image_size, max_num_samples=max_num_samples)\n",
    "    images_val, token_sequences_val, token_vocabulary_val = load_raw_data(dataset=\"val\",  max_token_length = max_token_length, max_image_size = max_image_size, max_num_samples=max_num_samples)\n",
    "    images_train_val = images_train + images_val\n",
    "    token_sequences_train_val = token_sequences_train + token_sequences_val\n",
    "    \n",
    "    token_vocabulary = token_vocabulary_val + token_vocabulary_train\n",
    "    token_vocabulary = list(set(token_vocabulary))\n",
    "        \n",
    "    images_train_val, target_shape = preprocess_images(images_train_val, target_shape=None)\n",
    "    \n",
    "    \n",
    "    images_test, token_sequences_test, token_vocabulary_test = load_raw_data(dataset=\"test\",  max_token_length = max_token_length, max_image_size = max_image_size, max_num_samples=max_num_samples)\n",
    "    \n",
    "    images_test, target_shape = preprocess_images(images_test, target_shape)\n",
    "    \n",
    "    return images_train_val, token_sequences_train_val, token_vocabulary, images_test, token_sequences_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,u'Histogram: Sequence lengths')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZlV4sRhZmaV/H8hE/bAT6+QogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b26e250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Make a histogram of how long the sequences are\n",
    "## Helps us decide on a cut off point\n",
    "\n",
    "formula_file_path = \"../data/train.formulas.norm.txt\"\n",
    "\n",
    "formula_lengths = []\n",
    "\n",
    "with open (formula_file_path, \"r\") as myfile:\n",
    "    for idx, token_sequence in enumerate(myfile):\n",
    "        tokens = token_sequence.split()\n",
    "        formula_lengths.append(len(tokens))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(formula_lengths, normed=False, bins=40)\n",
    "plt.ylabel('Num of expressions');\n",
    "plt.xlabel('Sequence lengths')\n",
    "plt.title('Histogram: Sequence lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Not loading image with id:', 2622)\n",
      "('Not loading image with id:', 8140)\n",
      "('Not loading image with id:', 14140)\n",
      "('Not loading image with id:', 17337)\n",
      "('Not loading image with id:', 23305)\n",
      "('Not loading image with id:', 27738)\n",
      "('Not loading image with id:', 29229)\n",
      "('Not loading image with id:', 39602)\n",
      "('Not loading image with id:', 42008)\n",
      "('Not loading image with id:', 49252)\n",
      "('Not loading image with id:', 49445)\n",
      "('Not loading image with id:', 55767)\n",
      "('Not loading image with id:', 63914)\n",
      "('Not loading image with id:', 64058)\n",
      "('Not loading image with id:', 64336)\n",
      "('Not loading image with id:', 64600)\n",
      "('Not loading image with id:', 66155)\n",
      "('Not loading image with id:', 71501)\n",
      "('Not loading image with id:', 3368)\n",
      "downsampling images\n",
      "('target shape: ', (45, 216))\n",
      "('Not loading image with id:', 2931)\n",
      "('Not loading image with id:', 5556)\n",
      "downsampling images\n",
      "('target shape: ', (45, 216))\n"
     ]
    }
   ],
   "source": [
    "## Load and process data (takes a up to 10 minutes)\n",
    "\n",
    "max_token_length = 70\n",
    "max_image_size = (60, 270)\n",
    "max_num_samples = hparams['max_num_samples']\n",
    "images_train_val, token_sequences_train_val, token_vocabulary, images_test, token_sequences_test = load_data(dataset=\"train\", \n",
    "                                                               max_token_length=max_token_length,\n",
    "                                                               max_image_size=max_image_size,\n",
    "                                                               max_num_samples=max_num_samples)\n",
    "\n",
    "\n",
    "\n",
    "## Note: Approx 15 images are missing and will not be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47156\n",
      "(47156, 45, 216, 1)\n",
      "(5136, 45, 216, 1)\n"
     ]
    }
   ],
   "source": [
    "print(len(token_sequences_train_val))\n",
    "print(images_train_val.shape)\n",
    "print(images_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum output sequence lenght: 72\n",
      "\n",
      "Examples of sequences: \n",
      "Ex. 1: **start** \\widetilde \\gamma _ { \\mathrm { h o p f } } \\simeq \\sum _ { n > 0 } \\widetilde { G } _ { n } { \\frac { ( - a ) ^ { n } } { 2 ^ { 2 n - 1 } } } **end**\n",
      "\n",
      "Ex. 1: **start** ( { \\cal L } _ { a } g ) _ { i j } = 0 , \\ \\ \\ \\ ( { \\cal L } _ { a } H ) _ { i j k } = 0 , **end**\n",
      " \n",
      "\n",
      "Number of examples: 47156\n",
      "Number of tokens in our vocabulary: 484\n",
      "5 example of tokens: ['\\\\perp', '\\\\leq', '\\\\begin{picture}', '\\\\supset', '\\\\protectm']\n",
      "\n",
      "\n",
      " Example pairs (token, index) in dictionary: \n",
      "('V', 75)\n",
      "('\\\\perp', 0)\n",
      "('\\\\leq', 1)\n",
      "('\\\\setminus', 27)\n",
      "('\\\\supset', 3)\n",
      "('/', 414)\n",
      "('\\\\raisebox', 5)\n",
      "('0', 6)\n",
      "('\\\\left\\\\Vert', 7)\n",
      "('\\\\Re', 8)\n",
      "('\\\\smallint', 9)\n",
      "('\\\\vspace', 73)\n"
     ]
    }
   ],
   "source": [
    "# Let's check the data out:\n",
    "\n",
    "num_decoder_tokens = len(token_vocabulary)\n",
    "\n",
    "max_decoder_seq_length = max([len(txt.split()) for txt in token_sequences_train_val])\n",
    "\n",
    "target_token_index = dict(\n",
    "    [(token, i) for i, token in enumerate(token_vocabulary)])\n",
    "\n",
    "reverse_target_token_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items()) ## Will be used in the inference model\n",
    "\n",
    "print(\"Maximum output sequence lenght: \" + str(max_decoder_seq_length) + \"\\n\")\n",
    "print(\"Examples of sequences: \")\n",
    "print(\"Ex. 1: \" + str(token_sequences_train_val[0]) + \"\\n\")\n",
    "print(\"Ex. 1: \" + str(token_sequences_train_val[1]) + \"\\n \\n\")\n",
    "\n",
    "print(\"Number of examples: \" + str(len(images_train_val)))\n",
    "\n",
    "\n",
    "print(\"Number of tokens in our vocabulary: \" + str(num_decoder_tokens))\n",
    "print(\"5 example of tokens: \" + str(token_vocabulary[0:5]) + \"\\n\")\n",
    "\n",
    "print(\"\\n Example pairs (token, index) in dictionary: \")\n",
    "\n",
    "for i, key in enumerate(target_token_index):\n",
    "    print(key, target_token_index[key])\n",
    "    if i > 10:\n",
    "        break\n",
    "\n",
    "_, image_h, image_w, _  = images_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of tokens not in the vocab: ', 0)\n",
      "Each row is a one-hot encoded token in the sequence.\n",
      "We have 10 columns because there are 10 tokens in our vocabulary\n",
      "We have 9 rows, because maximum output length is 9\n",
      "\n",
      "Decoder INPUT sequence example 1\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      "Decoder TARGET sequence example 1 (the same as above offset by one time step)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# For forced teaching, we need decoder_input data and decoder target data. (takes a few minutes)\n",
    "# Decoder target data is just decoder_input_data offset by one time step.\n",
    "\n",
    "decoder_input_data, decoder_target_data = get_decoder_data(token_sequences_train_val,\n",
    "                                                            token_vocabulary,\n",
    "                                                           num_decoder_tokens,\n",
    "                                                           max_decoder_seq_length,\n",
    "                                                           target_token_index)\n",
    "\n",
    "print(\"Each row is a one-hot encoded token in the sequence.\")\n",
    "print(\"We have 10 columns because there are 10 tokens in our vocabulary\")\n",
    "print(\"We have 9 rows, because maximum output length is 9\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Decoder INPUT sequence example 1\")\n",
    "print(decoder_input_data[0]) #Each row is a one-hot encoded token in the sequence.\n",
    "print(\"\")\n",
    "print(\"Decoder TARGET sequence example 1 (the same as above offset by one time step)\")\n",
    "print(decoder_target_data[0]) #Each row is a one-hot encoded token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffle our data\n",
    "\n",
    "\n",
    "images_train_val, decoder_input_data, decoder_target_data = shuffle_data(images_train_val, decoder_input_data, decoder_target_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of tokens not in the vocab: ', 13)\n"
     ]
    }
   ],
   "source": [
    "_, test_target_data = get_decoder_data(token_sequences_test,\n",
    "                                                            token_vocabulary,\n",
    "                                                           num_decoder_tokens,\n",
    "                                                           max_decoder_seq_length,\n",
    "                                                           target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latex: \n",
      "['{', '\\\\cal', 'L', '}', '_', '{', '\\\\theta', '}', '=', '\\\\frac', '{', '\\\\theta', '}', '{', '3', '2', '{', '\\\\pi', '}', '^', '{', '2', '}', '}', '\\\\epsilon', '^', '{', '\\\\mu', '\\\\nu', '\\\\lambda', '\\\\sigma', '}', 'F', '_', '{', '\\\\mu', '\\\\nu', '}', '^', '{', 'a', '}', 'F', '_', '{', '\\\\lambda', '\\\\sigma', '}', '^', '{', 'a', '}', ',', '**end**']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABoCAYAAADhAAsHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEkyhG4ZhZAmm0A3DMLKE/wfYigPWo0lL2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1044f3f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latex: \n",
      "['S', '\\\\left(', 'x', ',', 'y', ',', 'A', '\\\\right)', '=', '\\\\left(', 'i', '\\\\beta', '_', '{', '\\\\mu', '}', 'D', '^', '{', '\\\\mu', '}', '-', 'm', '\\\\right)', '^', '{', '-', '1', '}', '\\\\delta', '^', '{', '4', '}', '\\\\left(', 'x', '-', 'y', '\\\\right)', '**end**']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABoCAYAAADhAAsHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a6ad9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latex: \n",
      "['\\\\int', 'D', '\\\\varphi', 'e', '^', '{', 'S', '[', '\\\\varphi', ']', '}', '\\\\approx', 'e', '^', '{', 'S', '[', '\\\\varphi', '_', '{', '0', '}', ']', '}', '**end**']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABoCAYAAADhAAsHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ced3210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Look at a few examples (we have downsampled them further from what is in the data folder):\n",
    "## Note that we have added the \"**start**\" and \"**end**\" token to all sentences.\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3,6):\n",
    "\n",
    "    print(\"Latex: \")\n",
    "    print(from_one_hot_to_latex_sequence(decoder_target_data[i]))\n",
    "\n",
    "    plt.imshow(images_train_val[i,:,:,0], cmap='gray')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time to build our model: Image -> ConvNet Encoder -> LSTM Decoder --> Latex\n",
    "\n",
    "\n",
    "## Encoder step I: Encoding image into vectors (e1, e2, ..., en)\n",
    "## Convnet design from Guillaume Genthial https://github.com/guillaumegenthial/im2latex/blob/master/model/encoder.py\n",
    "\n",
    "def get_encoded1(image_h, image_w):\n",
    "\n",
    "    encoder_inputs = Input(shape=(image_h, image_w,1), name=\"encoder_input_image\", dtype='float32')\n",
    "\n",
    "    \n",
    "    # Conv + max_pool / 2\n",
    "    encoded = Convolution2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(encoder_inputs)\n",
    "    encoded = MaxPooling2D(pool_size=2, padding='same')(encoded)\n",
    "\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    # Conv + max_pool /2\n",
    "    encoded = Convolution2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    encoded = MaxPooling2D(pool_size=2, padding='same')(encoded)\n",
    "\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    \n",
    "    # 2 Conv\n",
    "    encoded = Convolution2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    encoded = Convolution2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    \n",
    "    # Pooling + Convnet + Pooling (Note pool_size)\n",
    "    encoded = MaxPooling2D(pool_size=(2,1))(encoded)\n",
    "    encoded = Convolution2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    encoded = MaxPooling2D(pool_size=(1,2))(encoded)\n",
    "    \n",
    "    # BatchNormalization, Convolution\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Convolution2D(filters=512, kernel_size=(3,3), padding='valid', activation='relu')(encoded)\n",
    "\n",
    "    #encoded = time_signal()(encoded)\n",
    "\n",
    "    encoded_shape = encoded.get_shape().as_list()\n",
    "    _, h, w, c = encoded_shape\n",
    "\n",
    "    #Unroll the encoding to a series of vectors (e1, e2, e3..... en)\n",
    "    encoded = Reshape((w*h, c), name=\"unroll_encoding\")(encoded)\n",
    "    \n",
    "    return encoder_inputs, encoded\n",
    "\n",
    "\n",
    "def get_encoded2(image_h, image_w):\n",
    "\n",
    "    encoder_inputs = Input(shape=(image_h, image_w,1), name=\"encoder_input_image\", dtype='float32')\n",
    "\n",
    "    \n",
    "    # Conv\n",
    "    encoded = Convolution2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(encoder_inputs)\n",
    "\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    # Conv\n",
    "    encoded = Convolution2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    \n",
    "    # 2 Conv\n",
    "    encoded = Convolution2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    encoded = Convolution2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    \n",
    "    # Pooling + Convnet + Pooling (Note pool_size)\n",
    "    encoded = MaxPooling2D(pool_size=(2,1))(encoded)\n",
    "    encoded = Convolution2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    encoded = MaxPooling2D(pool_size=(1,2))(encoded)\n",
    "    \n",
    "    # BatchNormalization, Convolution\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Convolution2D(filters=512, kernel_size=(3,3), padding='valid', activation='relu')(encoded)\n",
    "\n",
    "    #encoded = time_signal()(encoded)\n",
    "\n",
    "    encoded_shape = encoded.get_shape().as_list()\n",
    "    _, h, w, c = encoded_shape\n",
    "\n",
    "    #Unroll the encoding to a series of vectors (e1, e2, e3..... en)\n",
    "    encoded = Reshape((w*h, c), name=\"unroll_encoding\")(encoded)\n",
    "    \n",
    "    return encoder_inputs, encoded\n",
    "\n",
    "## Taken from paper: http://cs231n.stanford.edu/reports/2017/pdfs/815.pdf\n",
    "def get_encoded3(image_h, image_w):\n",
    "\n",
    "    encoder_inputs = Input(shape=(image_h, image_w,1), name=\"encoder_input_image\", dtype='float32')\n",
    "\n",
    "    \n",
    "    # Conv\n",
    "    encoded = Convolution2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(encoder_inputs)\n",
    "\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    # Conv\n",
    "    encoded = Convolution2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    \n",
    "    # 2 Conv\n",
    "    encoded = Convolution2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    encoded = Convolution2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    \n",
    "    # \n",
    "    encoded = Convolution2D(filters=512, kernel_size=(4,2), strides=(2,2), activation='relu')(encoded)\n",
    "    \n",
    "    # BatchNormalization, Convolution\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Convolution2D(filters=512, kernel_size=(3,3), padding='valid', activation='relu')(encoded)\n",
    "\n",
    "    #encoded = time_signal()(encoded)\n",
    "\n",
    "    encoded_shape = encoded.get_shape().as_list()\n",
    "    _, h, w, c = encoded_shape\n",
    "\n",
    "    #Unroll the encoding to a series of vectors (e1, e2, e3..... en)\n",
    "    encoded = Reshape((w*h, c), name=\"unroll_encoding\")(encoded)\n",
    "    \n",
    "    return encoder_inputs, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder step II: transforming (e1, e2... en) to h0 and c0 \n",
    "# h0, and c0  will be the initial state of the decoder\n",
    "\n",
    "# Call convolutional encoder\n",
    "if hparams['encoder_model'] == \"CONVNET1\":\n",
    "    encoder_inputs, encoded = get_encoded1(image_h, image_w)\n",
    "elif hparams['encoder_model'] == \"CONVNET2\":\n",
    "    encoder_inputs, encoded = get_encoded2(image_h, image_w)\n",
    "elif hparams['encoder_model'] == \"CONVNET3\":\n",
    "    encoder_inputs, encoded = get_encoded3(image_h, image_w)\n",
    "    \n",
    "encoded_shape = encoded.get_shape().as_list()\n",
    "\n",
    "#Compute the average e from encoding.\n",
    "\n",
    "e_average = GlobalAveragePooling1D(name='average_e')(encoded)\n",
    "\n",
    "e_average = BatchNormalization()(e_average)\n",
    "\n",
    "\n",
    "#Compute h0 and c0, from e_average, following Genthial's suggestion\n",
    "h0 = Dense(hparams['lstm_dim'], activation='tanh', name=\"h0\")(e_average)\n",
    "c0 = Dense(hparams['lstm_dim'], activation='tanh', name=\"c0\")(e_average)\n",
    "\n",
    "h0 = BatchNormalization()(h0)\n",
    "c0 = BatchNormalization()(c0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder. LSTM + Softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "#decoder_inputs = Input(shape=(max_decoder_seq_length, num_decoder_tokens), name='decoder_input_sequence')\n",
    "\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "\n",
    "\n",
    "decoder_inputs = Input(shape=(max_decoder_seq_length, num_decoder_tokens), name='decoder_input_sequence')\n",
    "\n",
    "decoder_lstm = LSTM(hparams['lstm_dim'], return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=[h0, c0])\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Putting the training model together\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs],outputs=decoder_outputs)\n",
    "\n",
    "## Visualize the training model\n",
    "\n",
    "#plot_model(model, to_file='model_visualizations/training_model.png', show_shapes=False)\n",
    "\n",
    "#Image(filename='model_visualizations/training_model.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to get losses for each batch (and not each epoch)\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_loss = logs.get('loss')\n",
    "        val_loss = logs.get('val_loss')\n",
    "\n",
    "        file = open(\"/output/\" + \"metrics.txt\",\"a\") \n",
    "        \n",
    "        file.write(str(train_loss) + \"\\t\" + str(val_loss) + \"\\n\")\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "loss_history = LossHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "learning_rate = 0.003 # OBS: Learning rate is set with a callback instead (see next cell)\n",
    "beta_1 = 0.9 # Keras default\n",
    "beta_2 = 0.999 # Keras default\n",
    "epsilon=1e-08 # Keras default\n",
    "decay=0.0004 # \n",
    "clipvalue = 5 # \n",
    "\n",
    "adam_optimizer = optimizers.Adam(lr=learning_rate,\n",
    "                                       beta_1=beta_1,\n",
    "                                       beta_2=beta_2, \n",
    "                                       epsilon=epsilon,\n",
    "                                       decay=decay,\n",
    "                                        clipvalue=clipvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile and train the model\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"checkpoints/weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=False, period=2)\n",
    "\n",
    "callbacks_list = [loss_history]\n",
    "\n",
    "model.compile(adam_optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "batch_size = hparams['batch_size']\n",
    "\n",
    "model_history = model.fit([images_train_val, decoder_input_data],\n",
    "          decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=hparams['epochs'],\n",
    "          validation_split=0.1,\n",
    "         callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.visualize_training_history(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model\n",
    "\n",
    "#print(\"Saving model\")\n",
    "model.save_weights('/output/my_model_weights.h5')\n",
    "#model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE:\n",
    "\n",
    "### Uncomment to use pretrained weights (though only for 5 epochs, \n",
    "## so not nearly enough to make meaningful predictions for this large dataset)\n",
    "\n",
    "# model.load_weights(\"checkpoints/weights.best.hdf5\")\n",
    "\n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and show summary\n",
    "\n",
    "#model.save('s2s.h5')\n",
    "\n",
    "print(\"Encoder / decoder model training: \")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Time for inference\n",
    "\n",
    "\n",
    "# Step 1. Set up the encoder as a separate model:\n",
    "\n",
    "encoder_model = Model(encoder_inputs, [h0, c0]) #encoded and e_average are included for debugging purposes\n",
    "encoder_model.save(\"/output/encoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Set up the decoder as a separate model.\n",
    "\n",
    "# The decoder takes three inputs: the input_state_h, input_state_c and a vector (last prediction)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(hparams['lstm_dim'],), name='decoder_state_input_h')\n",
    "decoder_state_input_c = Input(shape=(hparams['lstm_dim'],), name='decoder_state_input_c')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Will be a one-hot encoded vector\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_inputs')\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "decoder_model.save(\"/output/decoder.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the inference Encoder Model \n",
    "#plot_model(encoder_model, to_file='model_visualizations/inference_encoder_model.png', show_shapes=True) \n",
    "#Image(filename='inference_encoder_model.png') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the inference Decoder model\n",
    "#plot_model(decoder_model, to_file='model_visualizations/inference_decoder_model.png', show_shapes=True)\n",
    "#Image(filename='inference_decoder_model.png') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decode sequence using our two models\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    h0_ = states_value[0]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    #Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"**start**\"]] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    \n",
    "    decoded_sentence_list = []\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "        [target_seq] + states_value)\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token_prob = np.max(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_token_index[sampled_token_index]\n",
    "        \n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop token.\n",
    "        if (sampled_char == '**end**' or\n",
    "            len(decoded_sentence.split()) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        else: \n",
    "            decoded_sentence = decoded_sentence + ' ' + sampled_char\n",
    "            decoded_sentence_list.append(sampled_char)\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    #Return h0_ for debugging\n",
    "    return decoded_sentence, decoded_sentence_list, h0_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(images, seq_target_data, max_token_length = 100):\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_target_data = []\n",
    "    \n",
    "    for idx, sentence in enumerate(evaluation_target_data):\n",
    "        if len(sentence) <= max_token_length:\n",
    "            filtered_images.append(images[idx])\n",
    "            filtered_target_data.append(sentence)\n",
    "\n",
    "    return filtered_images, filtered_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(images):\n",
    "\n",
    "    #images_test, token_sequences_test\n",
    "    num_test = images.shape[0] #\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    print(\"Predicting \" + str(num_test) + \" images\")\n",
    "    \n",
    "    for seq_index in range(num_test):\n",
    "\n",
    "        input_seq = evaluation_images[seq_index: seq_index + 1]\n",
    "\n",
    "        decoded_sentence, decoded_sentence_list, h0_ = decode_sequence(input_seq)\n",
    "\n",
    "        predictions.append(decoded_sentence_list)\n",
    "\n",
    "        if seq_index % 200 == 0:\n",
    "            print(\"Finished \" + str(seq_index))\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_set = \"VAL\"\n",
    "# 4716\n",
    "if evaluation_set == \"TEST\":\n",
    "    evaluation_images = images_test\n",
    "    evaluation_target_data = test_target_data\n",
    "elif evaluation_set == \"VAL\":\n",
    "    evaluation_images = images_train_val[-4716:,:,:,:]\n",
    "    evaluation_images = np.reshape(evaluation_images, (evaluation_images.shape[0], evaluation_images.shape[1], evaluation_images.shape[2], 1))\n",
    "    evaluation_target_data = decoder_target_data[-4716:]\n",
    "elif evaluation_set == \"TRAIN\":\n",
    "    evaluation_images = images_train_val[:3000,:,:,:]\n",
    "    evaluation_images = np.reshape(evaluation_images, (evaluation_images.shape[0], evaluation_images.shape[1], evaluation_images.shape[2], 1))\n",
    "    evaluation_target_data = decoder_target_data[:3000]    \n",
    "\n",
    "# Filter data and only look at images with shorter sequences\n",
    "\n",
    "#evaluation_images, evaluation_target_data = filter_data(evaluation_images, evaluation_target_data, max_token_length = 40)\n",
    "\n",
    "eval_ground_truth = []\n",
    "\n",
    "for test_sequence in evaluation_target_data:\n",
    "    ground_truth = from_one_hot_to_latex_sequence(test_sequence)\n",
    "    eval_ground_truth.append(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(evaluation_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for exact match and token accuracy was adapted from NMT Tutorial https://github.com/tensorflow/nmt\n",
    "\n",
    "import distance\n",
    "\n",
    "# CHECK\n",
    "def exact_match(labels, predictions, max_token_check=None):\n",
    "    #\"\"\"Compute exact match\"\"\"\n",
    "    match = 0.0\n",
    "    count = 0.0\n",
    "    if max_token_check == None:\n",
    "        max_token_check = 1000\n",
    "    for idx in range(len(labels)):\n",
    "        if np.all(labels[idx][:max_token_check] == predictions[idx][:max_token_check]):\n",
    "            match += 1\n",
    "\n",
    "        count += 1\n",
    "    return 100 * match / count\n",
    "\n",
    "\n",
    "def token_accuracy(labels, predictions, max_token_check=None):\n",
    "    #\"\"\"Compute accuracy on per word basis.\"\"\"\n",
    "    total_acc, total_count = 0., 0.\n",
    "    if max_token_check==None:\n",
    "        m_length = 1000\n",
    "    else:\n",
    "        m_length = max_token_check\n",
    "\n",
    "    for idx, target_sentence in enumerate(labels):\n",
    "        prediction = predictions[idx]\n",
    "\n",
    "        match = 0.0\n",
    "\n",
    "        total_count += 1 \n",
    "        for pos in range(min(len(target_sentence), len(prediction), m_length)):\n",
    "            label = target_sentence[pos]\n",
    "            pred = prediction[pos]\n",
    "            if label == pred:\n",
    "                match += 1\n",
    "        \n",
    "             \n",
    "        if max_token_check==None:\n",
    "            total_acc += 100 * match / max(len(target_sentence), len(prediction))\n",
    "        else:\n",
    "            total_acc += 100 * match / min(len(target_sentence), len(prediction), m_length)\n",
    "            \n",
    "    return total_acc / total_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lev_dist(labels, predictions, max_token_check=None):\n",
    "    \n",
    "    avg_distance = 0\n",
    "    count = 0.0\n",
    "  \n",
    "\n",
    "    for idx in range(len(labels)):\n",
    "\n",
    "\n",
    "        if max_token_check is not None:\n",
    "            lev_distance = distance.levenshtein(labels[idx][:max_token_check], predictions[idx][:max_token_check])\n",
    "        else:\n",
    "            lev_distance = distance.levenshtein(labels[idx], predictions[idx])\n",
    "            lev_distance = lev_distance / len(labels[idx])\n",
    "\n",
    "        avg_distance = avg_distance + lev_distance\n",
    "        count += 1\n",
    "\n",
    "\n",
    "\n",
    "    avg_distance = float(avg_distance) / count\n",
    "    return avg_distance\n",
    "\n",
    "\n",
    "\n",
    "def get_metrics(labels, predictions, max_token_check=None):\n",
    "    exact_match_avg = exact_match(labels, predictions, max_token_check)\n",
    "    token_accuracy_avg = token_accuracy(labels, predictions, max_token_check)\n",
    "    edit_distance_avg = lev_dist(labels, predictions, max_token_check)\n",
    "    return exact_match_avg, token_accuracy_avg, edit_distance_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics1 = get_metrics(eval_ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics1)\n",
    "print(metrics2)\n",
    "print(metrics3)\n",
    "print(metrics4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show some predictions with ground truth\n",
    "\n",
    "n = 20\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    plt.imshow(np.squeeze(evaluation_images[i]), cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Ground truth\")\n",
    "    print(*eval_ground_truth[i])\n",
    "    \n",
    "    print(\"Prediction\")\n",
    "    print(*predictions[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
