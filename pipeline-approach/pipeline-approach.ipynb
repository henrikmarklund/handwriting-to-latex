{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning images of equations into LateX: The Pipeline Approach\n",
    "##### Adam Jensen and Henrik Marklund"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import keras modules\n",
    "from keras import optimizers, metrics\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import cv2\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "#Import our own functions\n",
    "import load_data\n",
    "import helper_functions as hf\n",
    "import segment_helper_functions as seg_hf\n",
    "import recursive_profile_cutting as rpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_VERSION = \"PYTHON2\"\n",
    "\n",
    "\n",
    "\n",
    "if PYTHON_VERSION == \"PYTHON2\":\n",
    "    reload(load_data)\n",
    "    reload(hf)\n",
    "    reload(seg_hf)\n",
    "    reload(rpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hparams = {}\n",
    "hparams['model'] = \"BASELINE\" #\"BASELINE\", \"CONVNET1\", \"CONVNET2\"\n",
    "hparams['augmented_data'] = False\n",
    "hparams['crop_width'] = 30\n",
    "hparams['crop_height'] = 35\n",
    "hparams['dsample_factor'] = 0.6\n",
    "\n",
    "hparams['epochs'] = 4\n",
    "\n",
    "\n",
    "hparams['load_from_pickle'] = False\n",
    "hparams['save_to_pickle'] = False # Currently super slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset INFTY-CDB 3 (images of math symbsols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Max width: ', 120)\n",
      "('Min width: ', 5)\n",
      "('Mean width: ', 35.558135254894744)\n",
      "('Max height: ', 188)\n",
      "('Min heigh: ', 2)\n",
      "('Mean height: ', 47.714696263997624)\n",
      "('Number of examples: ', 70637)\n",
      "Math symbols loaded\n"
     ]
    }
   ],
   "source": [
    "if hparams['load_from_pickle']:\n",
    "    data_pickle = pickle.load( open( \"data_pickle.p\", \"rb\" ) )\n",
    "    X = data_pickle['X']\n",
    "    Y = data_pickle['Y']\n",
    "    num_unique = data_pickle['num_unique']\n",
    "    target_token_index = data_pickle['target_token_index']  \n",
    "else:\n",
    "    X, Y, num_unique, target_token_index = load_data.load_math_symbols()\n",
    "    if hparams['save_to_pickle']:\n",
    "        data_pickle = {'X': X, 'Y': Y, 'num_unique': num_unique, 'target_token_index': target_token_index}\n",
    "        pickle.dump( data_pickle, open( \"data_pickle.p\", \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_token_index = dict(\n",
    "    (i, token) for token, i in target_token_index.items())\n",
    "\n",
    "orig_shape = (X.shape[1], X.shape[2])\n",
    "X, Y = hf.shuffle_data(X,Y, seed=100)\n",
    "\n",
    "hex_to_token_dict = load_data.get_hex_to_token_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Down sample images\n",
    "\n",
    "X_small = hf.down_sample(X, hparams['dsample_factor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split up to train/val set and test set\n",
    "\n",
    "Y = np_utils.to_categorical(Y, num_classes = num_unique)\n",
    "\n",
    "X_train = X_small[:56509]\n",
    "X_val = X_small[56509:(56509+7063)]\n",
    "X_test = X_small[(56509+7063):]\n",
    "\n",
    "Y_train = Y[:56509]\n",
    "Y_val = Y[56509:(56509+7063)]\n",
    "Y_test = Y[(56509+7063):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X,Y, num_k):\n",
    "    shift = 0.05\n",
    "    zoom = 0.1\n",
    "    shear = 0.2\n",
    "    datagen = ImageDataGenerator(zoom_range=zoom, width_shift_range=shift, height_shift_range=shift)\n",
    "    #datagen = ImageDataGenerator(zoom_range=zoom)  \n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1))\n",
    "    X_augmented = X\n",
    "    Y_augmented = Y\n",
    "    \n",
    "    # fit parameters from data\n",
    "    datagen.fit(X)\n",
    "    # Configure batch size and retrieve one batch of images\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    for X_batch, y_batch in datagen.flow(X, Y, batch_size=1000):\n",
    "        # Show 9 images\n",
    "        \n",
    "        \n",
    "        if counter == 0:\n",
    "            for i in range(0, 9):\n",
    "                plt.subplot(330 + 1 + i)\n",
    "                plt.imshow(np.squeeze(X_batch[i]), cmap='gray')\n",
    "            # show the plot\n",
    "            plt.show()\n",
    "        \n",
    "        X_augmented = np.concatenate((X_augmented, X_batch), axis=0)\n",
    "        Y_augmented = np.concatenate((Y_augmented, y_batch), axis=0)\n",
    "        \n",
    "        counter = counter + 1\n",
    "        \n",
    "        if counter % 10 == 0:\n",
    "            print(\"Progress: \" + str(counter) + \"/ \" + str(num_k))\n",
    "        \n",
    "        if counter == num_k:\n",
    "            break\n",
    "    \n",
    "    return X_augmented, Y_augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hparams['augmented_data']:\n",
    "    X_train_non_augmented = X_train\n",
    "    Y_train_non_augmented = Y_train\n",
    "    \n",
    "    X_train, Y_train = augment_data(X_train,Y_train, 100)\n",
    "    \n",
    "    \n",
    "X_train, Y_train = hf.shuffle_data(X_train,Y_train, seed=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56509, 113, 72)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = hf.crop_images(X_test, hparams['crop_width'], hparams['crop_height'])\n",
    "X_val = hf.crop_images(X_val, hparams['crop_width'], hparams['crop_height'])\n",
    "X_train = hf.crop_images(X_train, hparams['crop_width'], hparams['crop_height'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize and invert\n",
    "X_test_normalized = hf.normalize_and_invert(X_test)\n",
    "X_val_normalized = hf.normalize_and_invert(X_val)\n",
    "X_train_normalized = hf.normalize_and_invert(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape images depending on model\n",
    "if hparams['model'] == \"BASELINE\":\n",
    "\n",
    "    X_train_vecs = hf.flatten(X_train_normalized)\n",
    "    X_val_vecs = hf.flatten(X_val_normalized)\n",
    "    X_test_vecs = hf.flatten(X_test_normalized)\n",
    "\n",
    "#If we are using a convnet\n",
    "else:\n",
    "    X_train_vecs = hf.add_dimension(X_train_normalized)\n",
    "    X_val_vecs = hf.add_dimension(X_val_normalized)\n",
    "    X_test_vecs = hf.add_dimension(X_test_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Y_train.shape[0] == X_train_vecs.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS adapated from https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n",
    "num_pixels = X_train_vecs.shape[1]\n",
    "num_classes = Y_test.shape[1]\n",
    "\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def convnet1(image_size):\n",
    "    \n",
    "    height = image_size[0]\n",
    "    width = image_size[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(height, width, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def convnet2(image_size):\n",
    "    \n",
    "    height = image_size[0]\n",
    "    width = image_size[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(height, width, 1), activation='relu'))\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BASELINE\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "\n",
    "if hparams['model'] == \"BASELINE\":\n",
    "    print(\"Training BASELINE\")\n",
    "    model = baseline_model()\n",
    "elif hparams['model'] == \"CONVNET1\":\n",
    "    print(\"Training CONVNET1\")\n",
    "    model = convnet1((hparams['crop_height'], hparams['crop_width']))\n",
    "elif hparams['model'] == \"CONVNET2\":\n",
    "    model = convnet2((hparams['crop_height'], hparams['crop_width']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 # OBS: Learning rate is set with a callback instead (see next cell)\n",
    "beta_1 = 0.9 # Keras default\n",
    "beta_2 = 0.999 # Keras default\n",
    "epsilon=1e-08 # Keras default\n",
    "decay=1e-6 # \n",
    "\n",
    "adam_optimizer = optimizers.Adam(lr=learning_rate,\n",
    "                                       beta_1=beta_1,\n",
    "                                       beta_2=beta_2, \n",
    "                                       epsilon=epsilon,\n",
    "                                       decay=decay)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fit_history = model.fit(X_train_vecs, Y_train, validation_data=(X_val_vecs, Y_val), epochs=hparams['epochs'], batch_size=200, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test_vecs, Y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.visualize_training_history(fit_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1050)              1103550   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 275)               289025    \n",
      "=================================================================\n",
      "Total params: 1,392,575\n",
      "Trainable params: 1,392,575\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# between 80k and 1.5 million parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEGMENTING EQUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Equation dataset\n",
    "\n",
    "equation_images = load_data.load_equations(30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_segmented_equation(equation_image, smaller_images, show_each_symbol=False):\n",
    "\n",
    "\n",
    "    backtorgb = np.copy(equation_image)\n",
    "    plt.imshow(equation_image, cmap='gray')\n",
    "\n",
    "    for image_with_position in smaller_images:\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        smaller_image = image_with_position['image']\n",
    "        smaller_image_shape = image_with_position['image'].shape\n",
    "\n",
    "        corner1y = image_with_position['position'][0]\n",
    "        corner1x = image_with_position['position'][1]\n",
    "        corner1 = (corner1x, corner1y)\n",
    "\n",
    "        corner2y = corner1y + smaller_image_shape[0] - 1\n",
    "        corner2x = corner1x + smaller_image_shape[1]\n",
    "\n",
    "        corner2 = (corner2x, corner2y) # x, y\n",
    "\n",
    "        cv2.rectangle(backtorgb,corner1,corner2,(0,255,0),2)\n",
    "\n",
    "\n",
    "        if show_each_symbol:\n",
    "            \n",
    "            print(\"Predicted token: \", image_with_position['predicted_token'])\n",
    "            plt.imshow(smaller_image, cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    plt.imshow(backtorgb, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#40 35\n",
    "\n",
    "ta_size = orig_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_for_predict(smaller_images, orig_shape, down_sample_symbol=True):\n",
    "    padded_images = hf.pad_images(smaller_images, orig_shape)\n",
    "\n",
    "    if down_sample_symbol:\n",
    "        down_sampled_images = hf.down_sample(padded_images,hparams['dsample_factor'])\n",
    "\n",
    "        h = down_sampled_images.shape[1]\n",
    "        w = down_sampled_images.shape[2]\n",
    "    else:\n",
    "        h = padded_images.shape[1]\n",
    "        w = padded_images.shape[2]\n",
    "\n",
    "    h_start = int(h / 2 - hparams['crop_height'] / 2)\n",
    "    w_start = int(w / 2 - hparams['crop_width'] / 2)\n",
    "\n",
    "    if down_sample_symbol:\n",
    "        final_images = down_sampled_images[:, h_start:h_start+hparams['crop_height'], w_start:w_start+hparams['crop_width']]\n",
    "    else:\n",
    "        final_images = padded_images[:, h_start:h_start+hparams['crop_height'], w_start:w_start+hparams['crop_width']]\n",
    "\n",
    "    return final_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#image_to_predict = np.reshape(X_train[611], (1, X_train[611].shape[0]*X_train[611].shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_step_before_predict(final_images):\n",
    "    images_to_predict = np.reshape(final_images, (final_images.shape[0], final_images.shape[1]*final_images.shape[2]))\n",
    "    images_to_predict = 255 - images_to_predict\n",
    "    images_to_predict = images_to_predict.astype('float')\n",
    "    images_to_predict = images_to_predict / 255.0\n",
    "    return images_to_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_smaller_images(images, orig_shape, down_sample_symbol=True):\n",
    "    images_with_positions = images\n",
    "\n",
    "    smaller_images = seg_hf.get_just_images(images)\n",
    "    \n",
    "    final_images = preprocess_images_for_predict(smaller_images, orig_shape, down_sample_symbol)\n",
    "    images_to_predict = last_step_before_predict(final_images)\n",
    "    output = model.predict(images_to_predict)\n",
    "    tokenids = np.argmax(output, axis=1)\n",
    "    #print(len(tokenids))\n",
    "    \n",
    "    for idx, o in enumerate(tokenids):\n",
    "        hexa = reverse_target_token_index[o]\n",
    "\n",
    "        token = hex_to_token_dict[hexa]\n",
    "        #real_output.append([token, o])\n",
    "        images[idx]['predicted_token_id'] = o\n",
    "\n",
    "        images[idx]['id'] = idx\n",
    "        images[idx]['predicted_token'] = token\n",
    "        \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learned this from: https://stackoverflow.com/questions/8242832/python-all-possible-pairs-of-2-list-elements-and-getting-the-index-of-that-pair\n",
    "import itertools\n",
    "def all_pairs(lst):\n",
    "    for p in itertools.combinations(lst,2):\n",
    "        i = iter(p)\n",
    "        yield zip(i,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_equal_sign(image_1,image_2):\n",
    "    predicted1 = image_1['predicted_token']\n",
    "    predicted2 = image_2['predicted_token']\n",
    "    \n",
    "    if predicted1 != 'minus' or predicted2 != 'minus':\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    w1 = image_1['image'].shape[1]\n",
    "    w2 = image_2['image'].shape[1]\n",
    "    \n",
    "    cx1 = seg_hf.get_center_from_image(image_1)[1]\n",
    "    cx2 = seg_hf.get_center_from_image(image_2)[1]\n",
    "    \n",
    "    if np.abs(w1-w2) > 4:\n",
    "        return False\n",
    "    if np.abs(cx1 - cx2) > 4:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_lc_i(image_1,image_2):\n",
    "    predicted1 = image_1['predicted_token']\n",
    "    predicted2 = image_2['predicted_token']\n",
    "    \n",
    "    top_symbols = ['comma', 'prime', 'ast']\n",
    "    bottom_symbols = ['one', 'l', 'i', 'iota']\n",
    "    \n",
    "    w1 = image_1['image'].shape[1]\n",
    "    w2 = image_2['image'].shape[1]\n",
    "    \n",
    "    cx1 = seg_hf.get_center_from_image(image_1)[1]\n",
    "    cx2 = seg_hf.get_center_from_image(image_2)[1]\n",
    "    \n",
    "    if np.abs(w1-w2) > 15:\n",
    "        return False\n",
    "    if np.abs(cx1 - cx2) > 15:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    if is_on_top(image_1, image_2):\n",
    "        if (predicted1 in top_symbols and predicted2 in bottom_symbols):\n",
    "            return True\n",
    "    elif is_on_top(image_2, image_1):\n",
    "        if (predicted1 in bottom_symbols and predicted2 in top_symbols):\n",
    "            return True\n",
    "    \n",
    "    \n",
    "    \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Works\n",
    "\n",
    "def is_on_top(image_1, image_2):\n",
    "    centery1, centerx1 = seg_hf.get_center_from_image(image_1)\n",
    "    centery2, centerx2 = seg_hf.get_center_from_image(image_2)\n",
    "    \n",
    "    if centery1 < centery2:\n",
    "        \n",
    "        return True\n",
    "    elif centery1 >= centery2:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_images(image_1, image_2):\n",
    "    \n",
    "    if is_on_top(image_1, image_2):\n",
    "        print(\"Is on top\")\n",
    "        new_y1 = image_1['position'][0]\n",
    "        new_x1 = min(image_1['position'][1],image_2['position'][1])\n",
    "        \n",
    "        yoffset = image_2['position'][0] - image_1['position'][0] - image_1['image'].shape[0]\n",
    "        \n",
    "        image_2_height = image_2['image'].shape[0]\n",
    "        image_2_width = image_2['image'].shape[1]\n",
    "        \n",
    "        image_1_height = image_1['image'].shape[0]\n",
    "        image_1_width = image_1['image'].shape[1]\n",
    "        \n",
    "        new_height = image_1_height + image_2_height + yoffset\n",
    "        new_width = max(image_1_width, image_2_width)\n",
    "        \n",
    "        new_image = 255 * np.ones((new_height, new_width))\n",
    "        \n",
    "        new_image[:image_1_height, :image_1_width] = image_1['image']\n",
    "        new_image[image_1_height+yoffset:, :image_2_width] = image_2['image']\n",
    "    else:\n",
    "        new_y1 = image_2['position'][0]\n",
    "        new_x1 = min(image_1['position'][1],image_2['position'][1])\n",
    "\n",
    "        yoffset = image_1['position'][0] - image_2['position'][0] - image_2['image'].shape[0]\n",
    "        image_2_height = image_2['image'].shape[0]\n",
    "        image_2_width = image_2['image'].shape[1]\n",
    "        \n",
    "        image_1_height = image_1['image'].shape[0]\n",
    "        image_1_width = image_1['image'].shape[1]\n",
    "        \n",
    "        new_height = image_1_height + image_2_height + yoffset\n",
    "        new_width = max(image_1_width, image_2_width)\n",
    "        \n",
    "        new_image = 255 * np.ones((new_height, new_width))\n",
    "\n",
    "        \n",
    "        new_image[:image_2_height, :image_2_width] = image_2['image']\n",
    "        print(\"new_height: \", new_height)\n",
    "        print(\"yoffset: \", yoffset)\n",
    "        print(\"image_1 shape\", image_1['image'].shape)\n",
    "        new_image[image_2_height+yoffset:, :image_1_width] = image_1['image']\n",
    "    \n",
    "    \n",
    "    new_image_with_position = {}\n",
    "    new_image_with_position['image'] = new_image\n",
    "    new_image_with_position['position'] = (new_y1, new_x1)\n",
    "    \n",
    "    \n",
    "    return new_image_with_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lc_is(images):\n",
    "    ids_to_remove = []\n",
    "    \n",
    "    new_predicted_images = []\n",
    "    new_list = all_pairs(images)\n",
    "    \n",
    "    lower_case_is = []\n",
    "    \n",
    "    for pair in new_list:\n",
    "        \n",
    "        if PYTHON_VERSION == \"PYTHON3\":\n",
    "            pair = list(pair)\n",
    "        \n",
    "        image_1 = pair[0][0]\n",
    "        image_2 = pair[0][1]\n",
    "        #print(image_1['id'])\n",
    "        #print(image_2['id'])\n",
    "        if check_if_lc_i(image_1,image_2):\n",
    "            new_image = concat_images(image_1, image_2)\n",
    "            #print(\"found one equal sign\")\n",
    "            ids_to_remove.append(image_1['id'])\n",
    "            ids_to_remove.append(image_2['id'])\n",
    "\n",
    "            new_image['id'] = image_1['id']\n",
    "            new_image['predicted_token'] = 'i'\n",
    "            lower_case_is.append(new_image)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for image in images:\n",
    "        if image['id'] not in ids_to_remove:\n",
    "            new_predicted_images.append(image)\n",
    "    \n",
    "    for lower_case_i in lower_case_is:\n",
    "        new_predicted_images.append(lower_case_i)\n",
    "    \n",
    "    return new_predicted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_equal_signs(images):\n",
    "    ids_to_remove = []\n",
    "    \n",
    "    new_predicted_images = []\n",
    "    \n",
    "    equal_signs = []\n",
    "    new_list = all_pairs(images)\n",
    "    for pair in new_list:\n",
    "        \n",
    "        if PYTHON_VERSION == \"PYTHON3\":\n",
    "            pair = list(pair)\n",
    "            \n",
    "        image_1 = pair[0][0]\n",
    "        image_2 = pair[0][1]\n",
    "\n",
    "        if check_if_equal_sign(image_1,image_2):\n",
    "            new_image = concat_images(image_1, image_2)\n",
    "            #print(\"found one equal sign\")\n",
    "            ids_to_remove.append(image_1['id'])\n",
    "            ids_to_remove.append(image_2['id'])\n",
    "\n",
    "            \n",
    "            new_image['id'] = image_1['id']\n",
    "            new_image['predicted_token'] = 'equal'\n",
    "            equal_signs.append(new_image)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for image in images:\n",
    "        if image['id'] not in ids_to_remove:\n",
    "            new_predicted_images.append(image)\n",
    "    \n",
    "    for equal_sign in equal_signs:\n",
    "        new_predicted_images.append(equal_sign)\n",
    "    \n",
    "    return new_predicted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rules(predicted_images):\n",
    "    images_processed = find_equal_signs(predicted_images)\n",
    "    images_processed = find_lc_is(images_processed)\n",
    "    \n",
    "    return images_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_list = all_pairs(predicted_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_area(image_with_position):\n",
    "    image_shape = image_with_position['image'].shape\n",
    "    return image_shape[0] * image_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def segment_and_apply_rules(equation_image, orig_shape):\n",
    "\n",
    "\n",
    "    img_with_positions = rpc.segment_equation(equation_image)\n",
    "    show_segmented_equation(equation_image, img_with_positions)\n",
    "\n",
    "    predicted_images = predict_smaller_images(img_with_positions, orig_shape)\n",
    "    rules_applied = apply_rules(predicted_images)\n",
    "\n",
    "    show_segmented_equation(equation_image, rules_applied)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_position(image_1, image_2, properties):\n",
    "    image1_shape = image_1['image'].shape\n",
    "    image2_shape = image_2['image'].shape\n",
    "    \n",
    "    if len(properties) > 0:\n",
    "        last_property = properties[-1]\n",
    "    else:\n",
    "        last_property = (None, None)\n",
    "    \n",
    "    image1_h = image1_shape[0]\n",
    "    image2_h = image2_shape[0]\n",
    "    \n",
    "    positiony1 = image_1['position'][0]\n",
    "    positionx1 = image_1['position'][1]\n",
    "    \n",
    "    positiony2 = image_2['position'][0]\n",
    "    positionx2 = image_2['position'][1]\n",
    "    \n",
    "    centery1, centerx1 = seg_hf.get_center_from_image(image_1)\n",
    "    centery2, centerx2 = seg_hf.get_center_from_image(image_2) \n",
    "    \n",
    "    # Below, above, sup or sub\n",
    "    \n",
    "    if last_property[0] == \"SUBRIGHT\":\n",
    "        if positiony2 < last_property[1] + 3:\n",
    "            properties.pop()\n",
    "            return (\"PREVIOUS\", properties)\n",
    "        \n",
    "    if last_property[0] == \"SUPRIGHT\":\n",
    "        if positiony2 + image2_h > last_property[1] - 3:\n",
    "            properties.pop()\n",
    "            return (\"PREVIOUS\", properties)\n",
    "    \n",
    "    if image2_h < image1_h:\n",
    "        ## Below\n",
    "        if (centerx2 > positionx1) and centerx2 < positionx1+image1_shape[1] and positiony2 > positiony1 + image1_shape[0]:\n",
    "            return (\"BELOW\", properties)\n",
    "\n",
    "        ## Above\n",
    "        if centerx2 > positionx1 and centerx2 < positionx1+image1_shape[1] and positiony2 + image2_shape[0] < positiony1:\n",
    "            return (\"ABOVE\", properties)\n",
    "\n",
    "\n",
    "        ## Subscript right\n",
    "        if centerx1 < positionx2 and positiony2 > centery1:\n",
    "            properties.append((\"SUBRIGHT\", centery1))\n",
    "            return (\"SUBRIGHT\", properties)\n",
    "\n",
    "        ## Superscript right\n",
    "        if centerx1 < positionx2 and positiony2+image2_shape[0] < centery1:\n",
    "            properties.append((\"SUPRIGHT\", centery1))\n",
    "            return (\"SUPRIGHT\", properties)\n",
    "        \n",
    "    ## To the right\n",
    "    return (\"RIGHT\", properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_images(images):\n",
    "    newlist = sorted(images, key=lambda k: k['id']) \n",
    "    return newlist\n",
    "\n",
    "def structural_analysis_and_latex(images):\n",
    "\n",
    "    images = sort_images(images)\n",
    "\n",
    "    latex_string = \"\"\n",
    "    prop = []\n",
    "    for idx in range(len(images)):\n",
    "        if idx == len(images) - 1:\n",
    "            latex = seg_hf.get_latex(images[idx]['predicted_token'])\n",
    "            latex_string += \" \" + latex\n",
    "            if len(prop) != 0:\n",
    "                latex_string += \" }\"\n",
    "            break\n",
    "\n",
    "\n",
    "        rel_pos, props = get_relative_position(images[idx], images[idx+1], prop)\n",
    "        prop = props\n",
    "\n",
    "\n",
    "        latex = seg_hf.get_latex(images[idx]['predicted_token'])\n",
    "\n",
    "        latex_string += \" \" + latex\n",
    "        if rel_pos == \"SUBRIGHT\":\n",
    "            latex_string += \"_{\"\n",
    "        elif rel_pos == \"PREVIOUS\":\n",
    "            latex_string += \" }\"\n",
    "        elif rel_pos ==\"SUPRIGHT\":\n",
    "            latex_string += \"^{\"\n",
    "        \n",
    "    \n",
    "    return latex_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_latex(equation_image, show_segmentation=False, show_each_symbol=False, down_sample_symbol=True):\n",
    "    img_with_positions = rpc.segment_equation(equation_image)\n",
    "    predicted_images = predict_smaller_images(img_with_positions, orig_shape, down_sample_symbol)\n",
    "    rules_applied = apply_rules(predicted_images)\n",
    "    if show_segmentation:\n",
    "        ims= sort_images(rules_applied)\n",
    "        show_segmented_equation(equation_image, ims, show_each_symbol=show_each_symbol)\n",
    "    return structural_analysis_and_latex(rules_applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is on top\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAABcCAYAAABgIn4PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x63cd8d5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAABcCAYAAABgIn4PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOTJ6IYToOTJ6IYToOTJ6IYToOf8HgF9DepU+K1gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1019ea810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " D_{ 0 } = \\LeftBrace w \\in D l R e w > c_{ 0 } \\RightBrace\n"
     ]
    }
   ],
   "source": [
    "print(image_to_latex(equation_images[4], show_segmentation=True, show_each_symbol=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
