{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone\n",
    "\n",
    "Handwriting to LaTeX<br>\n",
    "\n",
    "Adam Jensen, Henrik Marklund<br>\n",
    "oojensen@stanford.edu, marklund@stanford.edu<br>\n",
    "\n",
    "### Background\n",
    "We were typing up our CS229 homework and realized that we were spending more time on LaTeX than the actual homework. We did a quick informal survey amongst students in the Huang basement who concurred: yes, typesetting is a major inconvenience! Many said they spent over 5 and 10 hours per homework in CS221 and CS229 respectively. Another typical response was: “I chose not to typeset on the last CS229 homework, as I did not have time”.<br>\n",
    "\n",
    "There is currently no good solution for converting handwritten notes into LateX. As a consequence, STEM students around the world struggle. The long term goal is to train an algorithm that takes a scan of your a4 page and turns it into latex typesetting. <br>\n",
    "\n",
    "We limit the scope of this project, by having two aims:<br>\n",
    "1. Create a seq-to-seq model with attention turning images of digital equations into latex. <br>\n",
    "2. Train this model on handwritten equations (augmented data) and investigate its effectiveness.<br>\n",
    "\n",
    "The first goal is a request-for-research at OpenAI: https://openai.com/requests-for-research/#im2latex \n",
    "\n",
    "\n",
    "### Dataset\n",
    "Harvard Researchers have crawled wikipedia for mathematical equations and gathered a 100k equations from which one can generate images. Dataset: Harvard im-to-latex-100k (Described in __[Deng et al., 2016](https://arxiv.org/pdf/1609.04938.pdf)__) Guilluame Genthial at Stanford was kind enough to send us his generated images (as this takes quite some time). We do some additional processing (padding, and additional downsampling).\n",
    "\n",
    "Here is an example image with corresponding latex:\n",
    "\n",
    "<img src=\"model_visualizations/example_image.png\" height=\"30%\" width=\"30%\" alt=\"Learning rate schedule\" title=\"Learning rate\" />\n",
    "\n",
    "__Latex__:\n",
    "\\widetilde \\gamma \\_ { \\mathrm { h o p f } } \\simeq \\sum \\_ { n > 0 } \\widetilde { G } \\_ { n } { \\frac { ( - a ) ^ { n } } { 2 ^ { 2 n - 1 } } }\n",
    "\n",
    "##### Histogram: Sequence lengths\n",
    "<img src=\"model_visualizations/sequence_lengths.jpg\" height=\"35%\" width=\"35%\" alt=\"Learning rate schedule\" title=\"Learning rate\" />\n",
    "\n",
    "When running the code you will see more examples and more details about the dataset.\n",
    "\n",
    "### Progress (sequential):\n",
    "\n",
    "__1. Dataset loaded and processed.__ <br>\n",
    "We have approx 80k images with corresponding latex loaded and preprocessed. For now we skip looking at too long sentences and too big images.<br><br>\n",
    "__2. Encoder-Decoder model up and running.__ <br>\n",
    "Our model is based on a typical seq-to-seq model for translation. We started out with a seq-to-seq model for translation in Keras using LSTM (__[Described by Francois Chollet](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)__. We replaced the encoder with a convolution neural network as described by in the paper __[Image to Latex by Genthial & Sauvestre (2016)](http://cs231n.stanford.edu/reports/2017/pdfs/815.pdf)__. The conv. network design is one of the versions __[here](https://github.com/guillaumegenthial/im2latex/blob/master/model/encoder.py)__. We have one model for training and one model for inference (using the weights from the first model). Pictures of the models are below.<br><br>\n",
    "__3. Overfit to 10 examples__<br>\n",
    "After introducing Batch Normalization we managed to overfit to 10 examples. Still hard to overfit to many more examples. Could be that we need to train for a lot longer. When we do inference it seems to have learned a few different equations that it chooses among. As you'll see when you do inference with pretrained weights on the 40k images, it seems to choose from a set of equations that it predicts over and over.<br><br>\n",
    "__4. Training with decreasing loss on 40k images / sequences__ <br>\n",
    "Training is really slow which makes it important that we are systematic and smart about our experiments going forward.\n",
    "Implemented Clip Gradient and a Learning rate schedule.<br><br>\n",
    "__5. For debugging: Created an analogous but less complex problem.__ <br>\n",
    "Since it was hard to know why it was so hard overfitting to a larger number of examples we created a simpler but analogous problem: turning pictures of text into text (but treating each character as a separate token to keep the problem analogous). Training was a lot easier, and we could much more easily overfit on a larger number training examples.\n",
    "\n",
    "\n",
    "### Training on 60k images\n",
    "We have only tried training the whole dataset for a longer period of time once (Currently running, 12 hours on CPU so far). We have waited with this because we’ve not been able to overfit on 100 examples. The reason we don’t overfit on 100 example could be that we just have not trained for long enough.\n",
    "\n",
    "Anyhow, below is our training and validation loss. We categorical cross entropy loss and forced teaching in our model.\n",
    "\n",
    " | \\-    | Train error | Val error | Learning rate |\n",
    "|-----------------|-------------|-----------|---------------|\n",
    "| Start           | 3.64        | NA        | \\-            |\n",
    "| Epoch 1         | 1.44        | 1.30      | 0.004         |\n",
    "| Epoch 2         | 1.13        | 1.17      | 0.004         |\n",
    "| Epoch 3         | 1.04        | 1.09      | 0.004         |\n",
    "| Epoch 4         | 0.96        | 1.05      | 0.0014        |\n",
    "| Epoch 5         | 0.92        | 1.00      | 0.0011        |\n",
    "| Epoch 6 (1 / 5) | 0.89        | NA        | 0.0009        |\n",
    "\n",
    "\n",
    "Losses for each batch is not included here (We have noticed that when training, the loss goes down a lot in the beginning of the epoch and the doesn’t get much lower than this during the epoch (goes up a bit). This has been true of all epochs so far.\n",
    "\n",
    "###### Learning rate schedule\n",
    "We implement a learning rate schedule with callbacks in Keras to have more control and oversight over what’s happening with the learning rate. This is the current learning rate schedule:\n",
    "\n",
    "\n",
    "<img src=\"model_visualizations/learning_rate_schedule.jpg\" height=\"40%\" width=\"40%\" alt=\"Learning rate schedule\" title=\"Learning rate\" />\n",
    "\n",
    "\n",
    "\n",
    "### Going forward\n",
    "\n",
    "__Step 1: Find out why we have a hard time overfitting to 100 examples.__\n",
    "- Implement function to retrieve gradient norms\n",
    "- We have had problems with Keras trying to do this.\n",
    "- Train for a longer time (10 hours) and logg everything.\n",
    "- Try diagnosing the problem better.\n",
    "- Train using Embedding of token.\n",
    "We have already code written for this but have not tried training for a longer period of time. Genthail suggests initializing with orthogonal matrices helped training a lot.\n",
    "- Systematically experiment with different learning schedules: learning rates and decay\n",
    "\n",
    "__Step 2: Implement beam search to improve Inference__\n",
    "- We’ve begun this work.\n",
    "- This has had a huge impact in previous work (and does not affect training)\n",
    "\n",
    "__Step 3: Implement Attention__\n",
    "- This also has had a huge impact in previous work.\n",
    "- May make training difficult (according to previous papers)\n",
    "- OpenAI recommends trying it on a toy problem of long sequences of MNIST characters to make sure the model “is not badly broken”.\n",
    "\n",
    "__Step 4: Implement better evaluation metrics__\n",
    "\n",
    "__Step 5 (when better accuracy): Train on handwriting__\n",
    "- Train on CHORHME dataset\n",
    "- Generate synthetic handwriting (using Chrome-dataset or Detexify)\n",
    "\n",
    "\n",
    "### The models\n",
    "\n",
    "#### Training: Encoder - Decoder\n",
    "\n",
    "<img src=\"model_visualizations/training_model.png\" height=\"20%\" width=\"70%\" alt=\"Training model\" title=\"Training model\" />\n",
    "\n",
    "#### Inference: Encoder\n",
    "The first part in the picture above. The main difference during inference is that we don't feed the correct tokens as inputs to the LSTM (forced teaching). Instead we feed the prediction of the decoder to the LSTM for it to produce the next token.\n",
    "\n",
    "\n",
    "#### Inference: Decoder\n",
    "\n",
    "<img src=\"model_visualizations/inference_decoder_model.png\" height=\"40%\" width=\"70%\" alt=\"Training model\" title=\"Training model\" />\n",
    "\n",
    "\n",
    "### Try it out\n",
    "\n",
    "We've cleaned up the code somewhat to only show you what is actually in use. You will find more details about the dataset as you run the code. Happy training!\n",
    "\n",
    "1. Make sure you have /data folder\n",
    "2. Make sure you have the /checkpoints folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Numpy, Tensorflow and Keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "#from tensor2tensor.layers.common_attention import add_timing_signal_nd #currently not in use\n",
    "\n",
    "from keras.layers import Input, LSTM, Dense, Lambda, GlobalAveragePooling1D\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Reshape, Flatten, BatchNormalization, Embedding\n",
    "from keras import layers, backend ## IS THIS BEING USED?\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, TensorBoard, Callback\n",
    "import keras.backend as K\n",
    "from keras import optimizers, metrics\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import our own helper functions\n",
    "from prepare_data import get_decoder_data, get_decoder_data_int_sequences\n",
    "import evaluation_model_keras as evalm\n",
    "\n",
    "\n",
    "import helper_functions as hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "hparams = {}\n",
    "\n",
    "# Model\n",
    "hparams['dsample_factor'] = 0.6\n",
    "hparams['encoder_model'] = \"CONVNET2\" #\"CONVNET1, CONVNET2\n",
    "hparams['lstm_dim'] = 512\n",
    "\n",
    "# Data\n",
    "hparams['max_num_samples'] = 100000\n",
    "\n",
    "# Training\n",
    "hparams['epochs'] = 20\n",
    "hparams['batch_size'] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains a bunch of functions for loading and preprocessing the data\n",
    "\n",
    "def create_metric_output_files():\n",
    "\n",
    "    file = open(\"/output/metrics.txt\",\"w\") \n",
    "\n",
    "    file.write(\"Train loss\" + \"\\t\" + \"Val loss\" + \"\\n\")\n",
    "\n",
    "    file.close()\n",
    "\n",
    "def from_one_hot_to_latex_sequence(one_hot_sequence):\n",
    "    tokens = []\n",
    "    for idx, token_vector in enumerate(one_hot_sequence):\n",
    "\n",
    "        sampled_token_index = np.argmax(token_vector)\n",
    "        sampled_char = reverse_target_token_index[sampled_token_index]\n",
    "\n",
    "        tokens.append(sampled_char)\n",
    "        if sampled_char == '**end**':\n",
    "            break\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def shuffle_data(X,Y,Z,seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    num_samples = X.shape[0]\n",
    "    p = np.random.permutation(num_samples)\n",
    "    if len(X.shape) == 3:\n",
    "        X = X[p,:,:]\n",
    "    elif len(X.shape) == 4:\n",
    "        X = X[p,:,:,:]\n",
    "    Y = Y[p]\n",
    "    Z = Z[p]\n",
    "\n",
    "    \n",
    "    return X,Y,Z\n",
    "\n",
    "def get_max_shape(images):\n",
    "\n",
    "    max_height = 0\n",
    "    max_width = 0\n",
    "\n",
    "    for image in images:\n",
    "        if image.shape[0] > max_height:\n",
    "            max_height = image.shape[0]\n",
    "\n",
    "        if image.shape[1] > max_width:\n",
    "            max_width = image.shape[1]\n",
    "\n",
    "\n",
    "    return [max_height, max_width]\n",
    "\n",
    "\n",
    "def normalize_images(images):\n",
    "\n",
    "    images = images.astype(np.float32)\n",
    "    images = np.multiply(images, 1.0 / 255.0)\n",
    "\n",
    "    return images\n",
    "\n",
    "def down_sample(images, factor): \n",
    "    target_h = int(math.floor(float(images[0].shape[0]) * factor))\n",
    "    target_w = int(math.floor(float(images[0].shape[1]) * factor))\n",
    "    num_images = len(images)\n",
    "    down_sampled_images = np.ones((num_images, target_h, target_w)) * 255\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "\n",
    "        im = image\n",
    "\n",
    "        #Downsample\n",
    "        im = cv2.resize(im, (0, 0), fx = factor, fy=factor, interpolation = cv2.INTER_AREA) #cv2.INTER_LINEAR\n",
    "\n",
    "\n",
    "        down_sampled_images[idx, :, :] = im\n",
    "\n",
    "    return down_sampled_images\n",
    "\n",
    "\n",
    "def down_sample_flexible(images, factor): \n",
    "    print(\"downsampling images\")\n",
    "    new_images = []\n",
    "\n",
    "    for image in images:\n",
    "        new_image = cv2.resize(image, (0, 0), fx = factor, fy=factor, interpolation = cv2.INTER_AREA) #cv2.INTER_LINEAR\n",
    "        new_images.append(new_image)\n",
    "\n",
    "    return new_images\n",
    "\n",
    "\n",
    "def pad_images(images, target_shape):\n",
    "    \n",
    "\n",
    "    if (target_shape == None):\n",
    "        max_height, max_width = get_max_shape(images)\n",
    "    else:\n",
    "        max_height = target_shape[0]\n",
    "        max_width = target_shape[1]\n",
    "\n",
    "    target_shape = (max_height, max_width)\n",
    "    print(\"target shape: \", target_shape)\n",
    "    \n",
    "    num_images = len(images)\n",
    "\n",
    "    padded_images = np.ones((num_images, max_height, max_width)) * 255\n",
    "    for idx, image in enumerate(images):\n",
    "\n",
    "        h = image.shape[0]\n",
    "        w = image.shape[1]\n",
    "\n",
    "        padded_images[idx, :h, :w] = image\n",
    "\n",
    "    \n",
    "    return padded_images, target_shape\n",
    "\n",
    "def get_vocabulary_size():\n",
    "    return len(get_vocabulary())\n",
    "\n",
    "def get_vocabulary(dataset):\n",
    "    if dataset == \"small\":\n",
    "        vocab = [line for line in open('data/tin/tiny_vocab.txt')]\n",
    "    elif dataset == \"test\":\n",
    "        vocab = [line for line in open('data/vocab.txt')]\n",
    "    elif dataset == \"train\":\n",
    "        vocab = [line for line in open('data/vocab.txt')]\n",
    "\n",
    "    vocab = [x.strip('\\n') for x in vocab]\n",
    "    return vocab\n",
    "\n",
    "def load_raw_data(dataset, max_token_length = 400, max_image_size = (60, 200), max_num_samples = 5000):\n",
    "    \n",
    "    token_vocabulary = []\n",
    "    token_sequences = []\n",
    "    images = []\n",
    "    \n",
    "    if dataset == \"small\":\n",
    "        image_folder = 'data/tin/tiny/'\n",
    "        formula_file_path = \"data/tin/tiny.formulas.norm.txt\"\n",
    "    elif dataset == \"test\":\n",
    "        image_folder = '../data/images_test/'\n",
    "        formula_file_path = \"../data/test.formulas.norm.txt\"\n",
    "    elif dataset == \"train\":\n",
    "        image_folder = '../data/images_train/'\n",
    "        formula_file_path = \"../data/train.formulas.norm.txt\"\n",
    "    elif dataset == \"val\":\n",
    "        image_folder = '../data/images_val/'\n",
    "        formula_file_path = \"../data/val.formulas.norm.txt\"\n",
    "        \n",
    "    included_counter = 0\n",
    "    examples_counter = 0\n",
    "    with open (formula_file_path, \"r\") as myfile:\n",
    "\n",
    "        for idx, token_sequence in enumerate(myfile):\n",
    "            examples_counter += 1\n",
    "            #Check token size:\n",
    "            token_sequence = token_sequence.rstrip('\\n')\n",
    "            tokens = token_sequence.split()\n",
    "\n",
    "            file_name = str(idx) + '.png'\n",
    "            image = cv2.imread(image_folder + file_name, 0)\n",
    "            \n",
    "            if image is None:\n",
    "                print(\"Not loading image with id:\", idx)\n",
    "                continue\n",
    "            \n",
    "            #print(tokens)\n",
    "            if len(tokens) <= max_token_length and image.shape[0] <= max_image_size[0] and image.shape[1] <= max_image_size[1]:\n",
    "                token_sequences.append('**start** ' + token_sequence + ' **end**')\n",
    "                #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #Grey scale\n",
    "                #print(image)\n",
    "                \n",
    "                images.append(image)\n",
    "                for token in tokens:\n",
    "                    if token not in token_vocabulary:\n",
    "                        token_vocabulary.append(token)\n",
    "\n",
    "                included_counter += 1\n",
    "                if included_counter == max_num_samples:\n",
    "                    break\n",
    "    \n",
    "    token_vocabulary.append(\"**start**\")\n",
    "    token_vocabulary.append(\"**end**\")\n",
    "    token_vocabulary.append(\"**other**\")\n",
    "    \n",
    "    return images, token_sequences, token_vocabulary\n",
    "\n",
    "\n",
    "def preprocess_images(images, target_shape):\n",
    "    \n",
    "    encoder_input = down_sample_flexible(images, hparams['dsample_factor'])\n",
    "    encoder_input, target_shape = pad_images(encoder_input, target_shape)\n",
    "    encoder_input = normalize_images(encoder_input)\n",
    "\n",
    "    # Add dimension for TensorFlow Conv Layers to work properly as it needs (None, Height, Width, 1)\n",
    "    encoder_input = encoder_input.reshape(encoder_input.shape[0], encoder_input.shape[1], encoder_input.shape[2], 1)\n",
    "\n",
    "    return encoder_input, target_shape\n",
    "\n",
    "\n",
    "def load_data(dataset, max_token_length, max_image_size, max_num_samples):\n",
    "    \n",
    "    ## First get all the training and validation data\n",
    "    images_train, token_sequences_train, token_vocabulary_train = load_raw_data(dataset=\"train\",  max_token_length = max_token_length, max_image_size = max_image_size, max_num_samples=max_num_samples)\n",
    "    images_val, token_sequences_val, token_vocabulary_val = load_raw_data(dataset=\"val\",  max_token_length = max_token_length, max_image_size = max_image_size, max_num_samples=max_num_samples)\n",
    "    images_train_val = images_train + images_val\n",
    "    token_sequences_train_val = token_sequences_train + token_sequences_val\n",
    "    \n",
    "    token_vocabulary = token_vocabulary_val + token_vocabulary_train\n",
    "    token_vocabulary = list(set(token_vocabulary))\n",
    "        \n",
    "    images_train_val, target_shape = preprocess_images(images_train_val, target_shape=None)\n",
    "    \n",
    "    \n",
    "    images_test, token_sequences_test, token_vocabulary_test = load_raw_data(dataset=\"test\",  max_token_length = max_token_length, max_image_size = max_image_size, max_num_samples=max_num_samples)\n",
    "    \n",
    "    images_test, target_shape = preprocess_images(images_test, target_shape)\n",
    "    \n",
    "    return images_train_val, token_sequences_train_val, token_vocabulary, images_test, token_sequences_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,u'Histogram: Sequence lengths')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZlV4sRhZmaV/H8hE/bAT6+QogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b307990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Make a histogram of how long the sequences are\n",
    "## Helps us decide on a cut off point\n",
    "\n",
    "formula_file_path = \"../data/train.formulas.norm.txt\"\n",
    "\n",
    "formula_lengths = []\n",
    "\n",
    "with open (formula_file_path, \"r\") as myfile:\n",
    "    for idx, token_sequence in enumerate(myfile):\n",
    "        tokens = token_sequence.split()\n",
    "        formula_lengths.append(len(tokens))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(formula_lengths, normed=False, bins=40)\n",
    "plt.ylabel('Num of expressions');\n",
    "plt.xlabel('Sequence lengths')\n",
    "plt.title('Histogram: Sequence lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Not loading image with id:', 2622)\n",
      "('Not loading image with id:', 8140)\n",
      "('Not loading image with id:', 3368)\n",
      "downsampling images\n",
      "('target shape: ', (30, 144))\n",
      "('Not loading image with id:', 2931)\n",
      "('Not loading image with id:', 5556)\n",
      "downsampling images\n",
      "('target shape: ', (30, 144))\n"
     ]
    }
   ],
   "source": [
    "## Load and process data (takes a up to 10 minutes)\n",
    "\n",
    "max_token_length = 70\n",
    "max_image_size = (60, 270)\n",
    "max_num_samples = hparams['max_num_samples']\n",
    "images_train_val, token_sequences_train_val, token_vocabulary, images_test, token_sequences_test = load_data(dataset=\"train\", \n",
    "                                                               max_token_length=max_token_length,\n",
    "                                                               max_image_size=max_image_size,\n",
    "                                                               max_num_samples=max_num_samples)\n",
    "\n",
    "\n",
    "\n",
    "## Note: Approx 15 images are missing and will not be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9648\n",
      "(9648, 30, 144, 1)\n",
      "(5000, 30, 144, 1)\n"
     ]
    }
   ],
   "source": [
    "print(len(token_sequences_train_val))\n",
    "print(images_train_val.shape)\n",
    "print(images_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum output sequence lenght: 72\n",
      "\n",
      "Examples of sequences: \n",
      "Ex. 1: **start** \\widetilde \\gamma _ { \\mathrm { h o p f } } \\simeq \\sum _ { n > 0 } \\widetilde { G } _ { n } { \\frac { ( - a ) ^ { n } } { 2 ^ { 2 n - 1 } } } **end**\n",
      "\n",
      "Ex. 1: **start** ( { \\cal L } _ { a } g ) _ { i j } = 0 , \\ \\ \\ \\ ( { \\cal L } _ { a } H ) _ { i j k } = 0 , **end**\n",
      " \n",
      "\n",
      "Number of examples: 9648\n",
      "Number of tokens in our vocabulary: 401\n",
      "5 example of tokens: ['\\\\perp', '\\\\leq', '\\\\begin{picture}', '\\\\supset', '\\\\raisebox']\n",
      "\n",
      "\n",
      " Example pairs (token, index) in dictionary: \n",
      "('\\\\perp', 0)\n",
      "('\\\\leq', 1)\n",
      "('\\\\setminus', 19)\n",
      "('\\\\supset', 3)\n",
      "('\\\\raisebox', 4)\n",
      "('0', 5)\n",
      "('\\\\left\\\\Vert', 6)\n",
      "('\\\\Re', 7)\n",
      "('\\\\smallint', 8)\n",
      "('\\\\surd', 9)\n",
      "('\\\\hspace', 10)\n",
      "('\\\\vert', 11)\n"
     ]
    }
   ],
   "source": [
    "# Let's check the data out:\n",
    "\n",
    "num_decoder_tokens = len(token_vocabulary)\n",
    "\n",
    "max_decoder_seq_length = max([len(txt.split()) for txt in token_sequences_train_val])\n",
    "\n",
    "target_token_index = dict(\n",
    "    [(token, i) for i, token in enumerate(token_vocabulary)])\n",
    "\n",
    "reverse_target_token_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items()) ## Will be used in the inference model\n",
    "\n",
    "print(\"Maximum output sequence lenght: \" + str(max_decoder_seq_length) + \"\\n\")\n",
    "print(\"Examples of sequences: \")\n",
    "print(\"Ex. 1: \" + str(token_sequences_train_val[0]) + \"\\n\")\n",
    "print(\"Ex. 1: \" + str(token_sequences_train_val[1]) + \"\\n \\n\")\n",
    "\n",
    "print(\"Number of examples: \" + str(len(images_train_val)))\n",
    "\n",
    "\n",
    "print(\"Number of tokens in our vocabulary: \" + str(num_decoder_tokens))\n",
    "print(\"5 example of tokens: \" + str(token_vocabulary[0:5]) + \"\\n\")\n",
    "\n",
    "print(\"\\n Example pairs (token, index) in dictionary: \")\n",
    "\n",
    "for i, key in enumerate(target_token_index):\n",
    "    print(key, target_token_index[key])\n",
    "    if i > 10:\n",
    "        break\n",
    "\n",
    "_, image_h, image_w, _  = images_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of tokens not in the vocab: ', 0)\n",
      "Each row is a one-hot encoded token in the sequence.\n",
      "We have 10 columns because there are 10 tokens in our vocabulary\n",
      "We have 9 rows, because maximum output length is 9\n",
      "\n",
      "Decoder INPUT sequence example 1\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      "Decoder TARGET sequence example 1 (the same as above offset by one time step)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# For forced teaching, we need decoder_input data and decoder target data. (takes a few minutes)\n",
    "# Decoder target data is just decoder_input_data offset by one time step.\n",
    "\n",
    "decoder_input_data, decoder_target_data = get_decoder_data(token_sequences_train_val,\n",
    "                                                            token_vocabulary,\n",
    "                                                           num_decoder_tokens,\n",
    "                                                           max_decoder_seq_length,\n",
    "                                                           target_token_index)\n",
    "\n",
    "print(\"Each row is a one-hot encoded token in the sequence.\")\n",
    "print(\"We have 10 columns because there are 10 tokens in our vocabulary\")\n",
    "print(\"We have 9 rows, because maximum output length is 9\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Decoder INPUT sequence example 1\")\n",
    "print(decoder_input_data[0]) #Each row is a one-hot encoded token in the sequence.\n",
    "print(\"\")\n",
    "print(\"Decoder TARGET sequence example 1 (the same as above offset by one time step)\")\n",
    "print(decoder_target_data[0]) #Each row is a one-hot encoded token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffle our data\n",
    "\n",
    "\n",
    "images_train_val, decoder_input_data, decoder_target_data = shuffle_data(images_train_val, decoder_input_data, decoder_target_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of tokens not in the vocab: ', 39)\n"
     ]
    }
   ],
   "source": [
    "_, test_target_data = get_decoder_data(token_sequences_test,\n",
    "                                                            token_vocabulary,\n",
    "                                                           num_decoder_tokens,\n",
    "                                                           max_decoder_seq_length,\n",
    "                                                           target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latex: \n",
      "['V', '_', '{', '|', '\\\\alpha', '|', '}', '(', '\\\\alpha', '\\\\cdot', 'q', ')', '=', '{', '\\\\frac', '{', '1', '}', '{', '(', '\\\\alpha', '\\\\cdot', 'q', ')', '^', '{', '2', '}', '}', '}', ',', '\\\\quad', '\\\\mathrm', '{', 'f', 'o', 'r', '~', 'a', 'l', 'l', '~', 'r', 'o', 'o', 't', 's', '}', '.', '**end**']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABoCAYAAADhAAsHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b2b0b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latex: \n",
      "['S', '=', '\\\\frac', '{', '1', '}', '{', '8', '\\\\pi', '}', '\\\\int', 'd', '^', '{', '2', '}', '\\\\sigma', '\\\\,', '\\\\Bigl', '\\\\{', '(', '\\\\partial', '_', '{', '1', '}', 'X', ')', '^', '{', '2', '}', '+', '(', '\\\\partial', '_', '{', '2', '}', 'X', ')', '^', '{', '2', '}', '\\\\Bigr', '\\\\}', '.', '**end**']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABoCAYAAADhAAsHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHcdxMsL/AxeTu5knHnotAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bac9c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latex: \n",
      "['\\\\Sigma', '(', '0', ')', '=', '-', '\\\\frac', '{', '{', '\\\\lambda', '}', '^', '{', '2', '}', '}', '{', '3', '!', '}', '\\\\int', 'd', '^', '{', '2', '}', 'x', '\\\\,', '[', 'G', '_', '{', 'E', '}', '(', 'x', ')', ']', '^', '{', '3', '}', '**end**']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABoCAYAAADhAAsHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZDKjLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b73c150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Look at a few examples (we have downsampled them further from what is in the data folder):\n",
    "## Note that we have added the \"**start**\" and \"**end**\" token to all sentences.\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3,6):\n",
    "\n",
    "    print(\"Latex: \")\n",
    "    print(from_one_hot_to_latex_sequence(decoder_target_data[i]))\n",
    "\n",
    "    plt.imshow(images_train_val[i,:,:,0], cmap='gray')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time to build our model: Image -> ConvNet Encoder -> LSTM Decoder --> Latex\n",
    "\n",
    "\n",
    "## Encoder step I: Encoding image into vectors (e1, e2, ..., en)\n",
    "## Convnet design from Guillaume Genthial https://github.com/guillaumegenthial/im2latex/blob/master/model/encoder.py\n",
    "\n",
    "def get_encoded1(image_h, image_w):\n",
    "\n",
    "    encoder_inputs = Input(shape=(image_h, image_w,1), name=\"encoder_input_image\", dtype='float32')\n",
    "\n",
    "    \n",
    "    # Conv + max_pool / 2\n",
    "    encoded = Convolution2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(encoder_inputs)\n",
    "    encoded = MaxPooling2D(pool_size=2, padding='same')(encoded)\n",
    "\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    # Conv + max_pool /2\n",
    "    encoded = Convolution2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    encoded = MaxPooling2D(pool_size=2, padding='same')(encoded)\n",
    "\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    \n",
    "    # 2 Conv\n",
    "    encoded = Convolution2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    encoded = Convolution2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    \n",
    "    # Pooling + Convnet + Pooling (Note pool_size)\n",
    "    encoded = MaxPooling2D(pool_size=(2,1))(encoded)\n",
    "    encoded = Convolution2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    encoded = MaxPooling2D(pool_size=(1,2))(encoded)\n",
    "    \n",
    "    # BatchNormalization, Convolution\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Convolution2D(filters=512, kernel_size=(3,3), padding='valid', activation='relu')(encoded)\n",
    "\n",
    "    #encoded = time_signal()(encoded)\n",
    "\n",
    "    encoded_shape = encoded.get_shape().as_list()\n",
    "    _, h, w, c = encoded_shape\n",
    "\n",
    "    #Unroll the encoding to a series of vectors (e1, e2, e3..... en)\n",
    "    encoded = Reshape((w*h, c), name=\"unroll_encoding\")(encoded)\n",
    "    \n",
    "    return encoder_inputs, encoded\n",
    "\n",
    "\n",
    "def get_encoded2(image_h, image_w):\n",
    "\n",
    "    encoder_inputs = Input(shape=(image_h, image_w,1), name=\"encoder_input_image\", dtype='float32')\n",
    "\n",
    "    \n",
    "    # Conv\n",
    "    encoded = Convolution2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(encoder_inputs)\n",
    "\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    # Conv\n",
    "    encoded = Convolution2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    \n",
    "    # 2 Conv\n",
    "    encoded = Convolution2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    encoded = Convolution2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    \n",
    "    # Pooling + Convnet + Pooling (Note pool_size)\n",
    "    encoded = MaxPooling2D(pool_size=(2,1))(encoded)\n",
    "    encoded = Convolution2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    encoded = MaxPooling2D(pool_size=(1,2))(encoded)\n",
    "    \n",
    "    # BatchNormalization, Convolution\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Convolution2D(filters=512, kernel_size=(3,3), padding='valid', activation='relu')(encoded)\n",
    "\n",
    "    #encoded = time_signal()(encoded)\n",
    "\n",
    "    encoded_shape = encoded.get_shape().as_list()\n",
    "    _, h, w, c = encoded_shape\n",
    "\n",
    "    #Unroll the encoding to a series of vectors (e1, e2, e3..... en)\n",
    "    encoded = Reshape((w*h, c), name=\"unroll_encoding\")(encoded)\n",
    "    \n",
    "    return encoder_inputs, encoded\n",
    "\n",
    "## Taken from paper: http://cs231n.stanford.edu/reports/2017/pdfs/815.pdf\n",
    "def get_encoded3(image_h, image_w):\n",
    "\n",
    "    encoder_inputs = Input(shape=(image_h, image_w,1), name=\"encoder_input_image\", dtype='float32')\n",
    "\n",
    "    \n",
    "    # Conv\n",
    "    encoded = Convolution2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(encoder_inputs)\n",
    "\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    # Conv\n",
    "    encoded = Convolution2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    \n",
    "    # 2 Conv\n",
    "    encoded = Convolution2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    encoded = Convolution2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "    \n",
    "    # \n",
    "    encoded = Convolution2D(filters=512, kernel_size=(4,2), strides=(2,2), activation='relu')(encoded)\n",
    "    \n",
    "    # BatchNormalization, Convolution\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Convolution2D(filters=512, kernel_size=(3,3), padding='valid', activation='relu')(encoded)\n",
    "\n",
    "    #encoded = time_signal()(encoded)\n",
    "\n",
    "    encoded_shape = encoded.get_shape().as_list()\n",
    "    _, h, w, c = encoded_shape\n",
    "\n",
    "    #Unroll the encoding to a series of vectors (e1, e2, e3..... en)\n",
    "    encoded = Reshape((w*h, c), name=\"unroll_encoding\")(encoded)\n",
    "    \n",
    "    return encoder_inputs, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder step II: transforming (e1, e2... en) to h0 and c0 \n",
    "# h0, and c0  will be the initial state of the decoder\n",
    "\n",
    "# Call convolutional encoder\n",
    "if hparams['encoder_model'] == \"CONVNET1\":\n",
    "    encoder_inputs, encoded = get_encoded1(image_h, image_w)\n",
    "elif hparams['encoder_model'] == \"CONVNET2\":\n",
    "    encoder_inputs, encoded = get_encoded2(image_h, image_w)\n",
    "elif hparams['encoder_model'] == \"CONVNET3\":\n",
    "    encoder_inputs, encoded = get_encoded3(image_h, image_w)\n",
    "    \n",
    "encoded_shape = encoded.get_shape().as_list()\n",
    "\n",
    "#Compute the average e from encoding.\n",
    "\n",
    "e_average = GlobalAveragePooling1D(name='average_e')(encoded)\n",
    "\n",
    "e_average = BatchNormalization()(e_average)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Compute h0 and c0, from e_average, following Genthial's suggestion\n",
    "h0 = Dense(hparams['lstm_dim'], activation='tanh', name=\"h0\")(e_average)\n",
    "c0 = Dense(hparams['lstm_dim'], activation='tanh', name=\"c0\")(e_average)\n",
    "\n",
    "h0 = BatchNormalization()(h0)\n",
    "c0 = BatchNormalization()(c0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decoder. LSTM + Softmax layer\n",
    "\n",
    "\n",
    "# Training decoder\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "#decoder_inputs = Input(shape=(max_decoder_seq_length, num_decoder_tokens), name='decoder_input_sequence')\n",
    "\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "\n",
    "    \n",
    "decoder_inputs = Input(shape=(max_decoder_seq_length, num_decoder_tokens), name='decoder_input_sequence')\n",
    "\n",
    "decoder_lstm = LSTM(hparams['lstm_dim'], return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=[h0, c0])\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Putting the training model together\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs],outputs=decoder_outputs)\n",
    "\n",
    "## Visualize the training model\n",
    "\n",
    "#plot_model(model, to_file='model_visualizations/training_model.png', show_shapes=False)\n",
    "\n",
    "#Image(filename='model_visualizations/training_model.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to get losses for each batch (and not each epoch)\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_loss = logs.get('loss')\n",
    "        val_loss = logs.get('val_loss')\n",
    "\n",
    "        file = open(\"/output/\" + \"metrics.txt\",\"a\") \n",
    "        \n",
    "        file.write(str(train_loss) + \"\\t\" + str(val_loss) + \"\\n\")\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "loss_history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "learning_rate = 0.003 # OBS: Learning rate is set with a callback instead (see next cell)\n",
    "beta_1 = 0.9 # Keras default\n",
    "beta_2 = 0.999 # Keras default\n",
    "epsilon=1e-08 # Keras default\n",
    "decay=0.0004 # \n",
    "clipvalue = 5 # \n",
    "\n",
    "adam_optimizer = optimizers.Adam(lr=learning_rate,\n",
    "                                       beta_1=beta_1,\n",
    "                                       beta_2=beta_2, \n",
    "                                       epsilon=epsilon,\n",
    "                                       decay=decay,\n",
    "                                        clipvalue=clipvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule (IN USE)\n",
    "#def step_decay(epoch):\n",
    "    \n",
    "    #base_rate = 0.002\n",
    "    #decay_rate = 0.4\n",
    "\n",
    "    #if epoch <= 2:\n",
    "        #lrate = 0.0005 #\n",
    "    #elif epoch <= 30:\n",
    "      #  lrate = base_rate * 1. / (1 + (epoch - 2) * decay_rate) \n",
    "    #else:\n",
    "     #   lrate = 0.0001\n",
    "       \n",
    "    # Epoch 1-2: 0.004\n",
    "    # Epoch 3: 0.0014\n",
    "    # Epoch 6: 0.0007\n",
    "    # Epoch 10: 0.0004\n",
    "    # Epoch 20: 0.00024\n",
    "    # Epoch 30: 0.00016 \n",
    "        \n",
    "    #print(\"Learning rate: \", lrate)\n",
    "    #return lrate\n",
    "\n",
    "#lrate = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9165 samples, validate on 483 samples\n",
      "Epoch 1/1\n",
      " 128/9165 [..............................] - ETA: 21:25 - loss: 3.2942"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-460f0e8c07ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m          callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Compile and train the model\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"checkpoints/weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=False, period=2)\n",
    "\n",
    "callbacks_list = [loss_history]\n",
    "\n",
    "model.compile(adam_optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "batch_size = hparams['batch_size']\n",
    "\n",
    "model_history = model.fit([images_train_val, decoder_input_data],\n",
    "          decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=hparams['epochs'],\n",
    "          validation_split=0.1,\n",
    "         callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.visualize_training_history(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 401 and 484 for 'Assign_40' (op: 'Assign') with input shapes: [401,2048], [484,2048].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-eb74993e82ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(\"Saving model\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model.save_weights('/output/my_model_weights.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_model_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   2620\u001b[0m             \u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2622\u001b[0;31m             \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m   3140\u001b[0m                              ' elements.')\n\u001b[1;32m   3141\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3142\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2240\u001b[0m                 assign_placeholder = tf.placeholder(tf_dtype,\n\u001b[1;32m   2241\u001b[0m                                                     shape=value.shape)\n\u001b[0;32m-> 2242\u001b[0;31m                 \u001b[0massign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking)\u001b[0m\n\u001b[1;32m    571\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0massignment\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \"\"\"\n\u001b[0;32m--> 573\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.pyc\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    274\u001b[0m     return gen_state_ops.assign(\n\u001b[1;32m    275\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    277\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.pyc\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m\"Assign\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 401 and 484 for 'Assign_40' (op: 'Assign') with input shapes: [401,2048], [484,2048]."
     ]
    }
   ],
   "source": [
    "## Save the model\n",
    "\n",
    "#print(\"Saving model\")\n",
    "model.save_weights('/output/my_model_weights.h5')\n",
    "#model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE:\n",
    "\n",
    "### Uncomment to use pretrained weights (though only for 5 epochs, \n",
    "## so not nearly enough to make meaningful predictions for this large dataset)\n",
    "\n",
    "# model.load_weights(\"checkpoints/weights.best.hdf5\")\n",
    "\n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and show summary\n",
    "\n",
    "#model.save('s2s.h5')\n",
    "\n",
    "print(\"Encoder / decoder model training: \")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Time for inference\n",
    "\n",
    "\n",
    "# Step 1. Set up the encoder as a separate model:\n",
    "\n",
    "encoder_model = Model(encoder_inputs, [h0, c0]) #encoded and e_average are included for debugging purposes\n",
    "encoder_model.save(\"/output/encoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Set up the decoder as a separate model.\n",
    "\n",
    "# The decoder takes three inputs: the input_state_h, input_state_c and a vector (last prediction)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(hparams['lstm_dim'],), name='decoder_state_input_h')\n",
    "decoder_state_input_c = Input(shape=(hparams['lstm_dim'],), name='decoder_state_input_c')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Will be a one-hot encoded vector\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_inputs')\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "decoder_model.save(\"/output/decoder.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the inference Encoder Model \n",
    "#plot_model(encoder_model, to_file='model_visualizations/inference_encoder_model.png', show_shapes=True) \n",
    "#Image(filename='inference_encoder_model.png') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the inference Decoder model\n",
    "#plot_model(decoder_model, to_file='model_visualizations/inference_decoder_model.png', show_shapes=True)\n",
    "#Image(filename='inference_decoder_model.png') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decode sequence using our two models\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    h0_ = states_value[0]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    #Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"**start**\"]] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    \n",
    "    decoded_sentence_list = []\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "        [target_seq] + states_value)\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token_prob = np.max(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_token_index[sampled_token_index]\n",
    "        \n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop token.\n",
    "        if (sampled_char == '**end**' or\n",
    "            len(decoded_sentence.split()) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        else: \n",
    "            decoded_sentence = decoded_sentence + ' ' + sampled_char\n",
    "            decoded_sentence_list.append(sampled_char)\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    #Return h0_ for debugging\n",
    "    return decoded_sentence, decoded_sentence_list, h0_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(images, seq_target_data, max_token_length = 100):\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_target_data = []\n",
    "    \n",
    "    for idx, sentence in enumerate(evaluation_target_data):\n",
    "        if len(sentence) <= max_token_length:\n",
    "            filtered_images.append(images[idx])\n",
    "            filtered_target_data.append(sentence)\n",
    "\n",
    "    return filtered_images, filtered_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(images):\n",
    "\n",
    "    #images_test, token_sequences_test\n",
    "    num_test = images.shape[0] #\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    print(\"Predicting \" + str(num_test) + \" images\")\n",
    "    \n",
    "    for seq_index in range(num_test):\n",
    "\n",
    "        input_seq = evaluation_images[seq_index: seq_index + 1]\n",
    "\n",
    "        decoded_sentence, decoded_sentence_list, h0_ = decode_sequence(input_seq)\n",
    "\n",
    "        predictions.append(decoded_sentence_list)\n",
    "\n",
    "        if seq_index % 200 == 0:\n",
    "            print(\"Finished \" + str(seq_index))\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_set = \"VAL\"\n",
    "# 4716\n",
    "if evaluation_set == \"TEST\":\n",
    "    evaluation_images = images_test\n",
    "    evaluation_target_data = test_target_data\n",
    "elif evaluation_set == \"VAL\":\n",
    "    evaluation_images = images_train_val[-4716:,:,:,:]\n",
    "    evaluation_images = np.reshape(evaluation_images, (evaluation_images.shape[0], evaluation_images.shape[1], evaluation_images.shape[2], 1))\n",
    "    evaluation_target_data = decoder_target_data[-4716:]\n",
    "elif evaluation_set == \"TRAIN\":\n",
    "    evaluation_images = images_train_val[:3000,:,:,:]\n",
    "    evaluation_images = np.reshape(evaluation_images, (evaluation_images.shape[0], evaluation_images.shape[1], evaluation_images.shape[2], 1))\n",
    "    evaluation_target_data = decoder_target_data[:3000]    \n",
    "\n",
    "# Filter data and only look at images with shorter sequences\n",
    "\n",
    "#evaluation_images, evaluation_target_data = filter_data(evaluation_images, evaluation_target_data, max_token_length = 40)\n",
    "\n",
    "eval_ground_truth = []\n",
    "\n",
    "for test_sequence in evaluation_target_data:\n",
    "    ground_truth = from_one_hot_to_latex_sequence(test_sequence)\n",
    "    eval_ground_truth.append(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(evaluation_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distance\n",
    "\n",
    "# CHECK\n",
    "def exact_match(labels, predictions, max_token_check=None):\n",
    "    #\"\"\"Compute exact match\"\"\"\n",
    "    match = 0.0\n",
    "    count = 0.0\n",
    "    if max_token_check == None:\n",
    "        max_token_check = 1000\n",
    "    for idx in range(len(labels)):\n",
    "        if np.all(labels[idx][:max_token_check] == predictions[idx][:max_token_check]):\n",
    "            match += 1\n",
    "\n",
    "        count += 1\n",
    "    return 100 * match / count\n",
    "\n",
    "\n",
    "def token_accuracy(labels, predictions, max_token_check=None):\n",
    "    #\"\"\"Compute accuracy on per word basis.\"\"\"\n",
    "    total_acc, total_count = 0., 0.\n",
    "    if max_token_check==None:\n",
    "        m_length = 1000\n",
    "    else:\n",
    "        m_length = max_token_check\n",
    "\n",
    "    for idx, target_sentence in enumerate(labels):\n",
    "        prediction = predictions[idx]\n",
    "\n",
    "        match = 0.0\n",
    "\n",
    "        total_count += 1 \n",
    "        for pos in range(min(len(target_sentence), len(prediction), m_length)):\n",
    "            label = target_sentence[pos]\n",
    "            pred = prediction[pos]\n",
    "            if label == pred:\n",
    "                match += 1\n",
    "        \n",
    "             \n",
    "        if max_token_check==None:\n",
    "            total_acc += 100 * match / max(len(target_sentence), len(prediction))\n",
    "        else:\n",
    "            total_acc += 100 * match / min(len(target_sentence), len(prediction), m_length)\n",
    "            \n",
    "    return total_acc / total_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lev_dist(labels, predictions, max_token_check=None):\n",
    "    \n",
    "    avg_distance = 0\n",
    "    count = 0.0\n",
    "  \n",
    "\n",
    "    for idx in range(len(labels)):\n",
    "\n",
    "\n",
    "        if max_token_check is not None:\n",
    "            lev_distance = distance.levenshtein(labels[idx][:max_token_check], predictions[idx][:max_token_check])\n",
    "        else:\n",
    "            lev_distance = distance.levenshtein(labels[idx], predictions[idx])\n",
    "\n",
    "        avg_distance = avg_distance + lev_distance\n",
    "        count += 1\n",
    "\n",
    "\n",
    "\n",
    "    avg_distance = float(avg_distance) / count\n",
    "    return avg_distance\n",
    "\n",
    "\n",
    "\n",
    "def get_metrics(labels, predictions, max_token_check=None):\n",
    "    exact_match_avg = exact_match(labels, predictions, max_token_check)\n",
    "    token_accuracy_avg = token_accuracy(labels, predictions, max_token_check)\n",
    "    edit_distance_avg = lev_dist(labels, predictions, max_token_check)\n",
    "    return exact_match_avg, token_accuracy_avg, edit_distance_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_ground_truth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ee679945a55b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_ground_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmetrics2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_ground_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmetrics3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_ground_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmetrics4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_ground_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_ground_truth' is not defined"
     ]
    }
   ],
   "source": [
    "metrics1 = get_metrics(eval_ground_truth, predictions)\n",
    "metrics2 = get_metrics(eval_ground_truth, predictions, 15)\n",
    "metrics3 = get_metrics(eval_ground_truth, predictions, 30)\n",
    "metrics4 = get_metrics(eval_ground_truth, predictions, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics1)\n",
    "print(metrics2)\n",
    "print(metrics3)\n",
    "print(metrics4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show some predictions with ground truth\n",
    "\n",
    "n = 20\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    plt.imshow(np.squeeze(evaluation_images[i]), cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Ground truth\")\n",
    "    print(*eval_ground_truth[i])\n",
    "    \n",
    "    print(\"Prediction\")\n",
    "    print(*predictions[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
